{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0f9132",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary libraries imported successfully.\n",
      "All configuration parameters and Bayesian optimization search spaces defined.\n",
      "\n",
      "Loading data from 'Band alignment-feature engineeringed.xlsx'...\n",
      "Data loaded successfully.\n",
      "\n",
      "Data split using specified method: Features from column 'Chemical potential (eV)' to 'Overall surface area (Angstrom^2)' and target 'Band alignment (eV) '.\n",
      "X shape=(7, 3), Y shape=(7,)\n",
      "\n",
      "Forcing data conversion to numeric types and handling non-numeric entries...\n",
      "\n",
      "Checking and filling NaN values in data...\n",
      "No NaN values detected in feature data X.\n",
      "No NaN values detected in target data Y.\n",
      "Final data shape for analysis: X=(7, 3), Y=(7,)\n",
      "Note: This full dataset will be used for cross-validation tuning, no separate test set is created here.\n",
      "X is a Pandas DataFrame, its column names will be used.\n",
      "\n",
      "Using Leave-1-Out Cross-Validation (7 folds) for hyperparameter tuning on the ENTIRE dataset.\n",
      "Each training set size is 6 samples, and test set size is 1 samples.\n",
      "Note: Leave-P-Out CV provides more evaluation points for small sample sizes, helping the Bayesian optimizer find more stable hyperparameters.\n",
      "\n",
      "Models to be tuned: ['XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM']\n",
      "\n",
      "Starting BayesSearchCV for each model on target 'Band alignment (eV) ' (n_iter = 50)...\n",
      "--- Tuning XGBR (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- XGBR (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 0.9995\n",
      "Best Parameters: {'colsample_bytree': 0.7356592978919161, 'gamma': 0.01416756447032746, 'learning_rate': 0.10220163356258628, 'max_depth': 2, 'n_estimators': 500, 'reg_alpha': 0.0016547246041473261, 'reg_lambda': 0.1, 'subsample': 0.7275767706631318}\n",
      "Total Tuning Duration: 58.94 seconds\n",
      "--- Tuning RF (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'sqrt', 1, 2, 500] before, using random point [1, 'log2', 2, 2, 299]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'sqrt', 1, 2, 500] before, using random point [1, 'sqrt', 2, 3, 88]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'sqrt', 1, 2, 500] before, using random point [1, 'log2', 1, 3, 496]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'log2', 1, 2, 500] before, using random point [2, 'log2', 2, 3, 66]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'log2', 1, 2, 500] before, using random point [2, 'log2', 2, 4, 297]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'log2', 1, 2, 500] before, using random point [1, 'sqrt', 2, 3, 221]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 'sqrt', 1, 2, 500] before, using random point [2, 'sqrt', 3, 4, 95]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- RF (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 1.4583\n",
      "Best Parameters: {'max_depth': 2, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Total Tuning Duration: 59.32 seconds\n",
      "--- Tuning GBRT (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- GBRT (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 0.7937\n",
      "Best Parameters: {'learning_rate': 0.17467019525567606, 'max_depth': 2, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500, 'subsample': 0.581404420185478}\n",
      "Total Tuning Duration: 72.88 seconds\n",
      "--- Tuning ETR (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- ETR (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 1.1443\n",
      "Best Parameters: {'max_depth': 2, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 497}\n",
      "Total Tuning Duration: 70.04 seconds\n",
      "--- Tuning HGBR (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- HGBR (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 1.0074\n",
      "Best Parameters: {'l2_regularization': 1e-06, 'learning_rate': 0.018373805980073103, 'max_depth': 1, 'max_iter': 344, 'min_samples_leaf': 2}\n",
      "Total Tuning Duration: 53.60 seconds\n",
      "--- Tuning CBR (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- CBR (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 1.0552\n",
      "Best Parameters: {'bootstrap_type': 'No', 'depth': 2, 'iterations': 474, 'l2_leaf_reg': 3.6564560817583924, 'learning_rate': 0.15381091498122448, 'rsm': 1.0}\n",
      "Total Tuning Duration: 55.65 seconds\n",
      "--- Tuning LGBM (Target: Band alignment (eV) ) ---\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "Fitting 7 folds for each of 1 candidates, totalling 7 fits\n",
      "--- LGBM (Target: Band alignment (eV) ) Tuning Complete ---\n",
      "Best Score (Leave-1-Out CV RMSE): 2.4003\n",
      "Best Parameters: {'colsample_bytree': 0.7654820824760737, 'learning_rate': 0.14081172089034283, 'max_depth': 2, 'n_estimators': 324, 'num_leaves': 4, 'reg_alpha': 0.44308999172093066, 'reg_lambda': 0.002216427152558403, 'subsample': 0.5008184803527521}\n",
      "Total Tuning Duration: 32.46 seconds\n",
      "\n",
      "All model hyperparameter tuning complete.\n",
      "\n",
      "--- Final Optimization Results Overview ---\n",
      "\n",
      "Target: Band alignment (eV) \n",
      "  Model: XGBR\n",
      "    Best Leave-1-Out CV RMSE: 0.9995\n",
      "    Best Parameters: OrderedDict([('colsample_bytree', 0.7356592978919161), ('gamma', 0.01416756447032746), ('learning_rate', 0.10220163356258628), ('max_depth', 2), ('n_estimators', 500), ('reg_alpha', 0.0016547246041473261), ('reg_lambda', 0.1), ('subsample', 0.7275767706631318)])\n",
      "  Model: RF\n",
      "    Best Leave-1-Out CV RMSE: 1.4583\n",
      "    Best Parameters: OrderedDict([('max_depth', 2), ('max_features', 'log2'), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 500)])\n",
      "  Model: GBRT\n",
      "    Best Leave-1-Out CV RMSE: 0.7937\n",
      "    Best Parameters: OrderedDict([('learning_rate', 0.17467019525567606), ('max_depth', 2), ('max_features', 'log2'), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 500), ('subsample', 0.581404420185478)])\n",
      "  Model: ETR\n",
      "    Best Leave-1-Out CV RMSE: 1.1443\n",
      "    Best Parameters: OrderedDict([('max_depth', 2), ('max_features', 1.0), ('min_samples_leaf', 1), ('min_samples_split', 2), ('n_estimators', 497)])\n",
      "  Model: HGBR\n",
      "    Best Leave-1-Out CV RMSE: 1.0074\n",
      "    Best Parameters: OrderedDict([('l2_regularization', 1e-06), ('learning_rate', 0.018373805980073103), ('max_depth', 1), ('max_iter', 344), ('min_samples_leaf', 2)])\n",
      "  Model: CBR\n",
      "    Best Leave-1-Out CV RMSE: 1.0552\n",
      "    Best Parameters: OrderedDict([('bootstrap_type', 'No'), ('depth', 2), ('iterations', 474), ('l2_leaf_reg', 3.6564560817583924), ('learning_rate', 0.15381091498122448), ('rsm', 1.0)])\n",
      "  Model: LGBM\n",
      "    Best Leave-1-Out CV RMSE: 2.4003\n",
      "    Best Parameters: OrderedDict([('colsample_bytree', 0.7654820824760737), ('learning_rate', 0.14081172089034283), ('max_depth', 2), ('n_estimators', 324), ('num_leaves', 4), ('reg_alpha', 0.44308999172093066), ('reg_lambda', 0.002216427152558403), ('subsample', 0.5008184803527521)])\n",
      "\n",
      "Training data X, Y, and sub-model optimization results (grid_searches) are retained in the current session's memory.\n",
      "Please continue to run stacking evaluation and prediction scripts in the same session.\n",
      "\n",
      "Script execution complete.\n"
     ]
    }
   ],
   "source": [
    "# optimize_base_learners.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Model imports\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: CatBoost is not installed. Skipping CBR model.\")\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: LightGBM is not installed. Skipping LGBM model.\")\n",
    "    lightgbm_available = False\n",
    "\n",
    "# Scikit-learn and Scikit-optimize tool imports\n",
    "from sklearn.model_selection import LeaveOneOut, LeavePOut\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.base import clone\n",
    "\n",
    "print(\"All necessary libraries imported successfully.\")\n",
    "\n",
    "### --- Adjustable Configuration Parameters --- ###\n",
    "\n",
    "# --- 1. Data Loading Settings ---\n",
    "EXCEL_FILE_PATH = 'Band alignment-feature engineeringed.xlsx' # <--- Please replace this path with your actual Excel file path.\n",
    "\n",
    "# --- 2. Bayesian Optimization Iterations ---\n",
    "N_ITER_BAYESIAN = 50 # Number of Bayesian optimization iterations for each model. Higher values lead to\n",
    "                     # a more thorough search but increase computation time.\n",
    "\n",
    "# --- 3. Default Random State (for all model initializations and Bayesian optimization itself) ---\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 # Ensures reproducibility of results across runs.\n",
    "\n",
    "# --- 4. P-value for Cross-Validation (Leave-P-Out) ---\n",
    "P_VALUE_FOR_BASE_TUNING = 1 # Set to 1 for Leave-One-Out Cross-Validation (LOOCV).\n",
    "                           # For small datasets, LOOCV provides a robust estimate of model performance\n",
    "                           # by training on N-1 samples and testing on 1, repeated N times.\n",
    "\n",
    "# --- 5. Sub-model Selection Interface ---\n",
    "# List the names of the sub-models for which you want to perform hyperparameter tuning.\n",
    "# If a model's name is not present in this list, it will be excluded from the tuning process\n",
    "# and subsequent stacking. You can easily disable a model by commenting out its entry in this list.\n",
    "# Available model names typically include: 'XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM'\n",
    "ENABLED_SUBMODELS_FOR_TUNING_LIST = [\n",
    "    'XGBR',\n",
    "    'RF',\n",
    "    'GBRT',\n",
    "    'ETR',\n",
    "    'HGBR',\n",
    "    'CBR',\n",
    "    'LGBM' \n",
    "]\n",
    "\n",
    "# Ensure models in ENABLED_SUBMODELS_FOR_TUNING_LIST are actually available\n",
    "if 'CBR' in ENABLED_SUBMODELS_FOR_TUNING_LIST and not catboost_available:\n",
    "    print(\"Warning: CatBoost is not installed. 'CBR' removed from ENABLED_SUBMODELS_FOR_TUNING_LIST.\")\n",
    "    ENABLED_SUBMODELS_FOR_TUNING_LIST.remove('CBR')\n",
    "if 'LGBM' in ENABLED_SUBMODELS_FOR_TUNING_LIST and not lightgbm_available:\n",
    "    print(\"Warning: LightGBM is not installed. 'LGBM' removed from ENABLED_SUBMODELS_FOR_TUNING_LIST.\")\n",
    "    ENABLED_SUBMODELS_FOR_TUNING_LIST.remove('LGBM')\n",
    "\n",
    "# Check if at least one model is enabled\n",
    "if not ENABLED_SUBMODELS_FOR_TUNING_LIST:\n",
    "    print(\"Error: No models enabled for hyperparameter tuning. Please check ENABLED_SUBMODELS_FOR_TUNING_LIST.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 6. Hyperparameter Search Space Definitions for Each Model ---\n",
    "# These parameter ranges are set to be generally suitable, with adjustments for potentially small sample sizes\n",
    "# when using Leave-P-Out Cross-Validation (e.g., P_VALUE_FOR_BASE_TUNING = 1).\n",
    "parameter_XGBR = {\n",
    "    'n_estimators': Integer(1, 500), # Number of boosting rounds (trees). Controls model complexity.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # Step size shrinkage used in update to prevent overfitting. Controls the contribution of each tree.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of a tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'subsample': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of the training instance. Prevents overfitting by sampling data.\n",
    "    'colsample_bytree': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of columns when constructing each tree. Prevents overfitting by sampling features.\n",
    "    'gamma': Real(0, 0.5, prior='uniform'), # Minimum loss reduction required to make a further partition on a leaf node. Higher gamma leads to more conservative models.\n",
    "    'reg_alpha': Real(1e-3, 0.5, prior='log-uniform'), # L1 regularization term on weights. Encourages sparsity.\n",
    "    'reg_lambda': Real(0.1, 5.0, prior='log-uniform') # L2 regularization term on weights. Prevents large weights.\n",
    "}\n",
    "parameter_RF = {\n",
    "    'n_estimators': Integer(1, 500), # Number of trees in the forest. More trees generally improve performance but increase computation.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'max_features': Categorical(['sqrt', 'log2']), # The number of features to consider when looking for the best split. 'sqrt' is common for regression.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node. For small datasets, a slightly higher minimum (e.g., 1-3) can help prevent overfitting.\n",
    "    'min_samples_split': Integer(2, 4) # The minimum number of samples required to split an internal node. For very small datasets, ensure this value is not too high to allow splits (e.g., 2 is the minimum).\n",
    "}\n",
    "parameter_CBR = {\n",
    "    'iterations': Integer(1, 500), # The maximum number of trees to build.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # The learning rate. Controls the step size of gradient descent.\n",
    "    'depth': Integer(1, 2), # Depth of the tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'l2_leaf_reg': Real(1, 5, prior='log-uniform'), # L2 regularization coefficient. Prevents overfitting by penalizing large weights.\n",
    "    'rsm': Real(0.5, 1.0, prior='uniform'), # The sampling rate for columns (feature subsampling).\n",
    "    'bootstrap_type': Categorical(['No']) # Defines the bootstrap type. 'No' disables internal sampling, which can be beneficial for very small datasets to ensure all samples are used.\n",
    "}\n",
    "parameter_LGBM = {\n",
    "    'n_estimators': Integer(1, 500), # Number of boosting rounds.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # Boosting learning rate.\n",
    "    'max_depth': Integer(1, 2), # Maximum tree depth for base learners. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'num_leaves': Integer(2, 4), # Maximum number of leaves in one tree. Should be related to `max_depth` (e.g., `num_leaves <= 2^max_depth`). Adjusted for small datasets.\n",
    "    'subsample': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of the training instance.\n",
    "    'colsample_bytree': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of columns when constructing each tree.\n",
    "    'reg_alpha': Real(1e-3, 0.5, prior='log-uniform'), # L1 regularization term on weights.\n",
    "    'reg_lambda': Real(1e-3, 0.5, prior='log-uniform') # L2 regularization term on weights.\n",
    "}\n",
    "parameter_GBRT = {\n",
    "    'n_estimators': Integer(1, 500), # The number of boosting stages to perform.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # Learning rate shrinks the contribution of each tree.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the individual regression estimators. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'max_features': Categorical(['sqrt', 'log2']), # The number of features to consider when looking for the best split.\n",
    "    'min_samples_split': Integer(2, 4), # The minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node.\n",
    "    'subsample': Real(0.5, 1.0, prior='uniform') # The fraction of samples to be used for fitting the individual base learners.\n",
    "}\n",
    "parameter_HGBR = {\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # The learning rate.\n",
    "    'max_iter': Integer(1, 500), # The maximum number of iterations (boosting rounds). Corresponds to n_estimators.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the individual regression estimators. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node.\n",
    "    'l2_regularization': Real(1e-6, 5.0, prior='log-uniform') # The L2 regularization term on weights.\n",
    "}\n",
    "parameter_ETR = {\n",
    "    'n_estimators': Integer(1, 500), # The number of trees in the forest.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'min_samples_split': Integer(2, 4), # The minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node.\n",
    "    'max_features': Categorical(['sqrt', 'log2', 1.0]) # The number of features to consider when looking for the best split. '1.0' means using all features.\n",
    "}\n",
    "\n",
    "### --- End Configuration Parameters --- ###\n",
    "\n",
    "print(\"All configuration parameters and Bayesian optimization search spaces defined.\")\n",
    "\n",
    "# --- 2. Data Loading and Inspection ---\n",
    "print(f\"\\nLoading data from '{EXCEL_FILE_PATH}'...\")\n",
    "try:\n",
    "    data = pd.read_excel(EXCEL_FILE_PATH)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{EXCEL_FILE_PATH}' not found. Please check the file path.\")\n",
    "    sys.exit(1) # Exit the program\n",
    "\n",
    "# --- User-defined Data Splitting ---\n",
    "# X: Features are from the second column (index 1) up to the second-to-last column (exclusive, index -1)\n",
    "X = data.iloc[:, 1:-1]\n",
    "# Y: Target variable is the last column (index -1)\n",
    "# Note: Adjust this index based on your specific target column.\n",
    "# For example:\n",
    "# If 'band alignment' is the second-to-last column: Y = data.iloc[:, -2]\n",
    "# If 'shift range' is the very last column: Y = data.iloc[:, -1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "target_column_name = Y.name if Y.name is not None else \"Unnamed_Target_Column\" # Get column name, assign default if none\n",
    "\n",
    "# Dynamically describe the extracted feature column range\n",
    "feature_col_names = X.columns.tolist()\n",
    "if len(feature_col_names) > 1:\n",
    "    feature_range_str = f\"from column '{feature_col_names[0]}' to '{feature_col_names[-1]}'\"\n",
    "elif len(feature_col_names) == 1:\n",
    "    feature_range_str = f\"column '{feature_col_names[0]}'\"\n",
    "else:\n",
    "    feature_range_str = \"with no features\" # Case where feature_col_names is empty\n",
    "\n",
    "print(f\"\\nData split using specified method: Features {feature_range_str} and target '{target_column_name}'.\")\n",
    "print(f\"X shape={X.shape}, Y shape={Y.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\nForcing data conversion to numeric types and handling non-numeric entries...\")\n",
    "initial_X_nan_count = X.isnull().sum().sum()\n",
    "initial_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "Y = Y.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "new_X_nan_count = X.isnull().sum().sum()\n",
    "new_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "if new_X_nan_count > initial_X_nan_count:\n",
    "    print(f\"Warning: {new_X_nan_count - initial_X_nan_count} non-numeric entries in feature data X were converted to NaN.\")\n",
    "if new_Y_nan_count > initial_Y_nan_count:\n",
    "    print(f\"Warning: {new_Y_nan_count - initial_Y_nan_count} non-numeric entries in target data Y were converted to NaN.\")\n",
    "\n",
    "print(\"\\nChecking and filling NaN values in data...\")\n",
    "if X.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in feature data X. Filling with median.\")\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"NaN values in X filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in feature data X.\")\n",
    "\n",
    "if Y.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in target data Y. Filling with median.\")\n",
    "    y_median_val = Y.median()\n",
    "    if pd.isna(y_median_val): # If the median itself is NaN, it means all Y values are NaN\n",
    "        print(\"Error: All values in target data Y column are NaN. Cannot perform effective filling or model training. Please check the raw data.\")\n",
    "        sys.exit(1) # Exit the script\n",
    "    Y = Y.fillna(y_median_val)\n",
    "    print(\"NaN values in Y filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in target data Y.\")\n",
    "\n",
    "# Check if the number of samples is compatible with Leave-P-Out\n",
    "if X.shape[0] <= P_VALUE_FOR_BASE_TUNING:\n",
    "    print(f\"Error: Dataset has too few samples ({X.shape[0]}) for Leave-{P_VALUE_FOR_BASE_TUNING}-Out CV. At least {P_VALUE_FOR_BASE_TUNING + 1} samples are required.\")\n",
    "    sys.exit(1) # Exit the program\n",
    "\n",
    "print(f\"Final data shape for analysis: X={X.shape}, Y={Y.shape}\")\n",
    "print(\"Note: This full dataset will be used for cross-validation tuning, no separate test set is created here.\")\n",
    "\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    feature_names_for_init = X.columns.tolist()\n",
    "    print(\"X is a Pandas DataFrame, its column names will be used.\")\n",
    "else:\n",
    "    print(\"Warning: Input X is not a Pandas DataFrame. Feature importances will use default names 'Feature_i'.\")\n",
    "    feature_names_for_init = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# --- 3. Cross-Validation Setup (using LeavePOut) ---\n",
    "cross_Valid = LeavePOut(p=P_VALUE_FOR_BASE_TUNING)\n",
    "n_splits_calculated = cross_Valid.get_n_splits(X)\n",
    "print(f\"\\nUsing Leave-{P_VALUE_FOR_BASE_TUNING}-Out Cross-Validation ({n_splits_calculated} folds) for hyperparameter tuning on the ENTIRE dataset.\")\n",
    "print(f\"Each training set size is {X.shape[0] - P_VALUE_FOR_BASE_TUNING} samples, and test set size is {P_VALUE_FOR_BASE_TUNING} samples.\")\n",
    "print(\"Note: Leave-P-Out CV provides more evaluation points for small sample sizes, helping the Bayesian optimizer find more stable hyperparameters.\")\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "def initialize_best_estimators(grid_searches_dict, enabled_models_list):\n",
    "    \"\"\"\n",
    "    Initializes best base learner instances from BayesSearchCV results, filtered by the enabled_models_list.\n",
    "    This function is primarily intended for use in evaluation or prediction scripts.\n",
    "\n",
    "    Args:\n",
    "        grid_searches_dict (dict): A dictionary containing BayesSearchCV results for each model.\n",
    "        enabled_models_list (list): A list of model names that are enabled for tuning.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of initialized best estimator instances.\n",
    "    \"\"\"\n",
    "    estimators_init = {}\n",
    "    # Define all possible models and their default parameters\n",
    "    all_possible_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "\n",
    "    if catboost_available:\n",
    "        all_possible_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available:\n",
    "        all_possible_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "    for name, (model_class, fixed_params) in all_possible_models.items():\n",
    "        if name not in enabled_models_list: # If the model is not in the enabled list, skip it\n",
    "            print(f\"  Model {name} is not in ENABLED_SUBMODELS_FOR_TUNING_LIST, skipping initialization.\")\n",
    "            continue\n",
    "\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} with optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "        else:\n",
    "            print(f\"  No optimized results found for {name}. Attempting to initialize with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} with default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (with default parameters): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v is not None}\n",
    "    if not initialized_estimators:\n",
    "         print(\"Warning: No models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "# --- 4. Dynamically Define Models and Parameter Mappings (based on ENABLED_SUBMODELS_FOR_TUNING_LIST) ---\n",
    "# Stores all possible base model instances with their default parameters\n",
    "all_possible_estimators_base = {\n",
    "    'XGBR': XGB.XGBRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, objective='reg:squarederror'),\n",
    "    'RF': RandomForestRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'GBRT': GradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'HGBR': HistGradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'ETR': ExtraTreesRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "}\n",
    "if catboost_available: all_possible_estimators_base['CBR'] = CatBoostRegressor(verbose=False, random_state=DEFAULT_MODEL_RANDOM_STATE, allow_writing_files=False)\n",
    "if lightgbm_available: all_possible_estimators_base['LGBM'] = LGBMRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, verbosity=-1, objective='regression')\n",
    "\n",
    "# Stores all possible parameter search spaces\n",
    "all_possible_params_base = {\n",
    "    'XGBR': parameter_XGBR,\n",
    "    'RF': parameter_RF,\n",
    "    'GBRT': parameter_GBRT,\n",
    "    'HGBR': parameter_HGBR,\n",
    "    'ETR': parameter_ETR,\n",
    "    'CBR': parameter_CBR,\n",
    "    'LGBM': parameter_LGBM\n",
    "}\n",
    "\n",
    "# Filter models and parameters based on ENABLED_SUBMODELS_FOR_TUNING_LIST\n",
    "estimators_for_bayes = {}\n",
    "params_mapping = {}\n",
    "\n",
    "for model_name in ENABLED_SUBMODELS_FOR_TUNING_LIST:\n",
    "    if model_name in all_possible_estimators_base and model_name in all_possible_params_base:\n",
    "        estimators_for_bayes[model_name] = all_possible_estimators_base[model_name]\n",
    "        params_mapping[model_name] = all_possible_params_base[model_name]\n",
    "    else:\n",
    "        print(f\"Warning: Model '{model_name}' is in ENABLED_SUBMODELS_FOR_TUNING_LIST, but its definition or parameter space is missing/unavailable. Skipping.\")\n",
    "\n",
    "if not estimators_for_bayes:\n",
    "    print(\"Error: No valid models available for tuning based on ENABLED_SUBMODELS_FOR_TUNING_LIST. Please check configuration.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\nModels to be tuned: {list(estimators_for_bayes.keys())}\")\n",
    "\n",
    "\n",
    "# --- 5. Set up and Run BayesSearchCV (currently for a single Y target) ---\n",
    "grid_searches = {} # Stores the tuning result object for each model\n",
    "\n",
    "print(f\"\\nStarting BayesSearchCV for each model on target '{target_column_name}' (n_iter = {N_ITER_BAYESIAN})...\")\n",
    "\n",
    "for name, estimator in estimators_for_bayes.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"--- Tuning {name} (Target: {target_column_name}) ---\")\n",
    "    \n",
    "    # Ensure the model has a corresponding parameter space\n",
    "    if name not in params_mapping:\n",
    "        print(f\"Warning: Parameter space not found for model {name}, skipping.\")\n",
    "        grid_searches[name] = None\n",
    "        continue\n",
    "\n",
    "    search_space = params_mapping[name]\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=clone(estimator), # Clone the estimator to ensure each tuning is independent\n",
    "        search_spaces=search_space,\n",
    "        scoring='neg_mean_squared_error', # Using negative MSE (for optimizing RMSE, as BayesSearchCV maximizes the score)\n",
    "        n_iter=N_ITER_BAYESIAN,\n",
    "        cv=cross_Valid, # Use LeavePOut (LOOCV)\n",
    "        n_jobs=-1, # Typically -1 to use all available cores for parallel processing\n",
    "        random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "        verbose=1 # Keep verbose=1 to output detailed information for each iteration\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        bayes_search.fit(X, Y) # Use the selected single target Y\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        grid_searches[name] = bayes_search\n",
    "\n",
    "        print(f\"--- {name} (Target: {target_column_name}) Tuning Complete ---\")\n",
    "        # Note: bayes_search.best_score_ is negative MSE, so take the negative and square root to get RMSE\n",
    "        print(f\"Best Score (Leave-{P_VALUE_FOR_BASE_TUNING}-Out CV RMSE): {np.sqrt(-bayes_search.best_score_):.4f}\")\n",
    "        print(f\"Best Parameters: {dict(bayes_search.best_params_)}\")\n",
    "        print(f\"Total Tuning Duration: {duration:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\n!!! Error occurred during tuning {name} (Target: {target_column_name}): {e}\")\n",
    "        print(f\"Tuning attempt duration: {duration:.2f} seconds\")\n",
    "        grid_searches[name] = None\n",
    "\n",
    "print(\"\\nAll model hyperparameter tuning complete.\")\n",
    "\n",
    "# --- 6. Final Results Overview ---\n",
    "print(\"\\n--- Final Optimization Results Overview ---\")\n",
    "print(f\"\\nTarget: {target_column_name}\")\n",
    "for name, result in grid_searches.items():\n",
    "    if result is not None:\n",
    "        print(f\"  Model: {name}\")\n",
    "        # Note: result.best_score_ is negative MSE, so take the negative and square root to get RMSE\n",
    "        print(f\"    Best Leave-{P_VALUE_FOR_BASE_TUNING}-Out CV RMSE: {np.sqrt(-result.best_score_):.4f}\")\n",
    "        print(f\"    Best Parameters: {result.best_params_}\")\n",
    "    else:\n",
    "        print(f\"  Model: {name} (Tuning Failed or Skipped)\")\n",
    "\n",
    "# X, Y, and grid_searches remain in memory for further use in the current session.\n",
    "print(\"\\nTraining data X, Y, and sub-model optimization results (grid_searches) are retained in the current session's memory.\")\n",
    "print(\"Please continue to run stacking evaluation and prediction scripts in the same session.\")\n",
    "\n",
    "print(\"\\nScript execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4156ff-118b-48fc-9573-76bb04eddf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "\n",
      "Forcing data conversion to numeric types and handling non-numeric entries...\n",
      "\n",
      "Checking and filling NaN values in data...\n",
      "No NaN values detected in feature data X.\n",
      "No NaN values detected in target data Y.\n",
      "Dataset sample size N = 7\n",
      "External evaluation will use Leave-3-Out Cross-Validation.\n",
      "Total combinations for C(7, 3) = 35 folds.\n",
      "Noise augmentation is disabled.\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "\n",
      "=== Starting Meta-Learner Hyperparameter Pre-tuning (One-time execution, using Leave-One-Out CV for OOF generation and internal CV) ===\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Generating OOF predictions for model XGBR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model RF (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model GBRT (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model ETR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model HGBR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model CBR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model LGBM (for meta-learner tuning)...\n",
      "  Starting meta-learner Bayesian optimization...\n",
      "  Meta-learner Best Score (RMSE): 0.0045\n",
      "  Meta-learner Best Parameters: {'elasticnet__alpha': 0.0011904551061321413, 'elasticnet__l1_ratio': 1.0}\n",
      "Global best meta-learner initialized with optimized parameters and fixed coefficients.\n",
      "Fixed Meta-Learner Coefficients: {'XGBR': 0.0, 'RF': 0.0, 'GBRT': 0.0, 'ETR': 0.0, 'HGBR': 0.0, 'CBR': 0.00015644783056536552, 'LGBM': -2.056091533970495}\n",
      "\n",
      "--- Meta-Learner Hyperparameter Tuning Complete ---\n",
      "\n",
      "=== Starting Repeated Evaluation on 35 Leave-3-Out Combinations ===\n",
      "\n",
      "--- Starting Run 0 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 0: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 0: Leave-3-Out evaluation complete.\n",
      "  Run 0: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 0: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 0 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 3.1949\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.8503\n",
      "    RF: MAE Mean = 1.8622\n",
      "    GBRT: MAE Mean = 0.9694\n",
      "    ETR: MAE Mean = 1.3430\n",
      "    HGBR: MAE Mean = 1.0560\n",
      "    CBR: MAE Mean = 1.5457\n",
      "    LGBM: MAE Mean = 2.4129\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 1 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 1: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 1: Leave-3-Out evaluation complete.\n",
      "  Run 1: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 1: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 1 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 3.6051\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.6315\n",
      "    RF: MAE Mean = 1.3777\n",
      "    GBRT: MAE Mean = 0.9283\n",
      "    ETR: MAE Mean = 1.0484\n",
      "    HGBR: MAE Mean = 1.4559\n",
      "    CBR: MAE Mean = 1.1912\n",
      "    LGBM: MAE Mean = 2.1043\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 2 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 2: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 2: Leave-3-Out evaluation complete.\n",
      "  Run 2: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 2: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 2 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.5688\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.0578\n",
      "    RF: MAE Mean = 1.0617\n",
      "    GBRT: MAE Mean = 1.0070\n",
      "    ETR: MAE Mean = 0.5957\n",
      "    HGBR: MAE Mean = 1.0000\n",
      "    CBR: MAE Mean = 0.3837\n",
      "    LGBM: MAE Mean = 1.8280\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 3 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 3: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 3: Leave-3-Out evaluation complete.\n",
      "  Run 3: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 3: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 3 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.4618\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.2579\n",
      "    RF: MAE Mean = 0.9057\n",
      "    GBRT: MAE Mean = 0.5634\n",
      "    ETR: MAE Mean = 0.6410\n",
      "    HGBR: MAE Mean = 1.1210\n",
      "    CBR: MAE Mean = 0.8416\n",
      "    LGBM: MAE Mean = 1.6956\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 4 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 4: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 4: Leave-3-Out evaluation complete.\n",
      "  Run 4: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 4: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 4 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.8703\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.9964\n",
      "    RF: MAE Mean = 0.9486\n",
      "    GBRT: MAE Mean = 1.0228\n",
      "    ETR: MAE Mean = 1.3791\n",
      "    HGBR: MAE Mean = 0.8324\n",
      "    CBR: MAE Mean = 1.0781\n",
      "    LGBM: MAE Mean = 1.4843\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 5 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 5: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 5: Leave-3-Out evaluation complete.\n",
      "  Run 5: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 5: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 5 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.1338\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.2173\n",
      "    RF: MAE Mean = 1.8642\n",
      "    GBRT: MAE Mean = 1.3875\n",
      "    ETR: MAE Mean = 1.5907\n",
      "    HGBR: MAE Mean = 1.1843\n",
      "    CBR: MAE Mean = 1.6666\n",
      "    LGBM: MAE Mean = 2.6543\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 6 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 6: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 6: Leave-3-Out evaluation complete.\n",
      "  Run 6: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 6: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 6 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 4.1232\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.3437\n",
      "    RF: MAE Mean = 2.0808\n",
      "    GBRT: MAE Mean = 1.4735\n",
      "    ETR: MAE Mean = 1.6435\n",
      "    HGBR: MAE Mean = 1.4982\n",
      "    CBR: MAE Mean = 1.4513\n",
      "    LGBM: MAE Mean = 2.7447\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 7 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 7: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 7: Leave-3-Out evaluation complete.\n",
      "  Run 7: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 7: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 7 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.2969\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.0931\n",
      "    RF: MAE Mean = 1.4397\n",
      "    GBRT: MAE Mean = 0.9783\n",
      "    ETR: MAE Mean = 1.2263\n",
      "    HGBR: MAE Mean = 0.9499\n",
      "    CBR: MAE Mean = 1.3894\n",
      "    LGBM: MAE Mean = 2.2456\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 8 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 8: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 8: Leave-3-Out evaluation complete.\n",
      "  Run 8: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 8: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 8 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.3813\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.5272\n",
      "    RF: MAE Mean = 1.4480\n",
      "    GBRT: MAE Mean = 0.9931\n",
      "    ETR: MAE Mean = 1.3970\n",
      "    HGBR: MAE Mean = 0.9365\n",
      "    CBR: MAE Mean = 1.2588\n",
      "    LGBM: MAE Mean = 2.0343\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 9 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 9: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 9: Leave-3-Out evaluation complete.\n",
      "  Run 9: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 9: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 9 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.6768\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.1609\n",
      "    RF: MAE Mean = 1.0442\n",
      "    GBRT: MAE Mean = 0.9143\n",
      "    ETR: MAE Mean = 1.0617\n",
      "    HGBR: MAE Mean = 0.9818\n",
      "    CBR: MAE Mean = 1.3961\n",
      "    LGBM: MAE Mean = 2.3034\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 10 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 10: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 10: Leave-3-Out evaluation complete.\n",
      "  Run 10: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 10: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 10 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 6.6972\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.5822\n",
      "    RF: MAE Mean = 2.0779\n",
      "    GBRT: MAE Mean = 1.3391\n",
      "    ETR: MAE Mean = 1.8595\n",
      "    HGBR: MAE Mean = 1.2667\n",
      "    CBR: MAE Mean = 1.3433\n",
      "    LGBM: MAE Mean = 3.3514\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 11 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 11: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 11: Leave-3-Out evaluation complete.\n",
      "  Run 11: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 11: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 11 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 6.1059\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.4874\n",
      "    RF: MAE Mean = 2.0247\n",
      "    GBRT: MAE Mean = 0.9539\n",
      "    ETR: MAE Mean = 1.4540\n",
      "    HGBR: MAE Mean = 0.7921\n",
      "    CBR: MAE Mean = 1.3494\n",
      "    LGBM: MAE Mean = 3.0555\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 12 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 12: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 12: Leave-3-Out evaluation complete.\n",
      "  Run 12: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 12: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 12 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.5334\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.9675\n",
      "    RF: MAE Mean = 0.7472\n",
      "    GBRT: MAE Mean = 0.9981\n",
      "    ETR: MAE Mean = 0.7449\n",
      "    HGBR: MAE Mean = 1.1078\n",
      "    CBR: MAE Mean = 1.3002\n",
      "    LGBM: MAE Mean = 1.8947\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 13 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 13: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 13: Leave-3-Out evaluation complete.\n",
      "  Run 13: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 13: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 13 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.4053\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.4354\n",
      "    RF: MAE Mean = 1.0403\n",
      "    GBRT: MAE Mean = 0.9511\n",
      "    ETR: MAE Mean = 0.7941\n",
      "    HGBR: MAE Mean = 0.4346\n",
      "    CBR: MAE Mean = 1.0580\n",
      "    LGBM: MAE Mean = 1.6833\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 14 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 14: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 14: Leave-3-Out evaluation complete.\n",
      "  Run 14: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 14: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 14 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 4.9625\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 2.4303\n",
      "    RF: MAE Mean = 1.6699\n",
      "    GBRT: MAE Mean = 0.8392\n",
      "    ETR: MAE Mean = 0.9981\n",
      "    HGBR: MAE Mean = 0.4297\n",
      "    CBR: MAE Mean = 1.1079\n",
      "    LGBM: MAE Mean = 2.4833\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 15 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 15: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 15: Leave-3-Out evaluation complete.\n",
      "  Run 15: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 15: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 15 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.6544\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.9599\n",
      "    RF: MAE Mean = 2.3047\n",
      "    GBRT: MAE Mean = 1.2691\n",
      "    ETR: MAE Mean = 1.5535\n",
      "    HGBR: MAE Mean = 0.8824\n",
      "    CBR: MAE Mean = 2.2004\n",
      "    LGBM: MAE Mean = 2.5388\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 16 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 16: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 16: Leave-3-Out evaluation complete.\n",
      "  Run 16: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 16: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 16 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 7.7715\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 3.3082\n",
      "    RF: MAE Mean = 3.6405\n",
      "    GBRT: MAE Mean = 3.2487\n",
      "    ETR: MAE Mean = 3.2683\n",
      "    HGBR: MAE Mean = 3.1740\n",
      "    CBR: MAE Mean = 3.4200\n",
      "    LGBM: MAE Mean = 3.8891\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 17 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 17: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 17: Leave-3-Out evaluation complete.\n",
      "  Run 17: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 17: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 17 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 3.7513\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.7472\n",
      "    RF: MAE Mean = 2.0333\n",
      "    GBRT: MAE Mean = 1.3953\n",
      "    ETR: MAE Mean = 1.0711\n",
      "    HGBR: MAE Mean = 1.1055\n",
      "    CBR: MAE Mean = 1.9433\n",
      "    LGBM: MAE Mean = 2.2936\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 18 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 18: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 18: Leave-3-Out evaluation complete.\n",
      "  Run 18: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 18: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 18 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 4.3426\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.4520\n",
      "    RF: MAE Mean = 1.8766\n",
      "    GBRT: MAE Mean = 1.0978\n",
      "    ETR: MAE Mean = 1.0164\n",
      "    HGBR: MAE Mean = 1.4587\n",
      "    CBR: MAE Mean = 1.6515\n",
      "    LGBM: MAE Mean = 2.1731\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 19 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 19: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 19: Leave-3-Out evaluation complete.\n",
      "  Run 19: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 19: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 19 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.4851\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.1080\n",
      "    RF: MAE Mean = 1.1200\n",
      "    GBRT: MAE Mean = 0.3320\n",
      "    ETR: MAE Mean = 0.7100\n",
      "    HGBR: MAE Mean = 0.3746\n",
      "    CBR: MAE Mean = 0.6467\n",
      "    LGBM: MAE Mean = 1.9539\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 20 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 20: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 20: Leave-3-Out evaluation complete.\n",
      "  Run 20: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 20: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 20 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 3.0488\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.2811\n",
      "    RF: MAE Mean = 0.8913\n",
      "    GBRT: MAE Mean = 0.4355\n",
      "    ETR: MAE Mean = 0.5757\n",
      "    HGBR: MAE Mean = 1.1711\n",
      "    CBR: MAE Mean = 1.4513\n",
      "    LGBM: MAE Mean = 1.9054\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 21 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 21: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 21: Leave-3-Out evaluation complete.\n",
      "  Run 21: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 21: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 21 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.4574\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.9648\n",
      "    RF: MAE Mean = 0.5734\n",
      "    GBRT: MAE Mean = 0.4982\n",
      "    ETR: MAE Mean = 0.6921\n",
      "    HGBR: MAE Mean = 1.0530\n",
      "    CBR: MAE Mean = 0.6986\n",
      "    LGBM: MAE Mean = 1.6941\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 22 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 22: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 22: Leave-3-Out evaluation complete.\n",
      "  Run 22: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 22: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 22 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.1151\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.8900\n",
      "    RF: MAE Mean = 0.7678\n",
      "    GBRT: MAE Mean = 0.6256\n",
      "    ETR: MAE Mean = 0.4149\n",
      "    HGBR: MAE Mean = 0.9711\n",
      "    CBR: MAE Mean = 0.4982\n",
      "    LGBM: MAE Mean = 1.7087\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 23 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 23: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 23: Leave-3-Out evaluation complete.\n",
      "  Run 23: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 23: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 23 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.7064\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 2.0725\n",
      "    RF: MAE Mean = 0.9212\n",
      "    GBRT: MAE Mean = 0.6409\n",
      "    ETR: MAE Mean = 0.4658\n",
      "    HGBR: MAE Mean = 1.4427\n",
      "    CBR: MAE Mean = 0.5577\n",
      "    LGBM: MAE Mean = 1.5819\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 24 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 24: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 24: Leave-3-Out evaluation complete.\n",
      "  Run 24: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 24: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 24 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.3141\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.4966\n",
      "    RF: MAE Mean = 0.2767\n",
      "    GBRT: MAE Mean = 1.2965\n",
      "    ETR: MAE Mean = 0.5440\n",
      "    HGBR: MAE Mean = 1.5425\n",
      "    CBR: MAE Mean = 0.9760\n",
      "    LGBM: MAE Mean = 1.2854\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 25 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 25: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 25: Leave-3-Out evaluation complete.\n",
      "  Run 25: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 25: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 25 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 3.5362\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.4334\n",
      "    RF: MAE Mean = 2.3299\n",
      "    GBRT: MAE Mean = 1.5773\n",
      "    ETR: MAE Mean = 1.6467\n",
      "    HGBR: MAE Mean = 2.2751\n",
      "    CBR: MAE Mean = 1.9053\n",
      "    LGBM: MAE Mean = 2.8705\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 26 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 26: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 26: Leave-3-Out evaluation complete.\n",
      "  Run 26: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 26: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 26 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.2131\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.2228\n",
      "    RF: MAE Mean = 1.8720\n",
      "    GBRT: MAE Mean = 0.8144\n",
      "    ETR: MAE Mean = 1.1120\n",
      "    HGBR: MAE Mean = 0.8017\n",
      "    CBR: MAE Mean = 1.1062\n",
      "    LGBM: MAE Mean = 2.4554\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 27 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 27: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 27: Leave-3-Out evaluation complete.\n",
      "  Run 27: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 27: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 27 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.2975\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.3881\n",
      "    RF: MAE Mean = 1.5113\n",
      "    GBRT: MAE Mean = 0.9202\n",
      "    ETR: MAE Mean = 0.9948\n",
      "    HGBR: MAE Mean = 1.1646\n",
      "    CBR: MAE Mean = 1.2132\n",
      "    LGBM: MAE Mean = 2.2441\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 28 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 28: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 28: Leave-3-Out evaluation complete.\n",
      "  Run 28: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 28: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 28 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 4.6796\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.2266\n",
      "    RF: MAE Mean = 1.9501\n",
      "    GBRT: MAE Mean = 1.4677\n",
      "    ETR: MAE Mean = 1.5353\n",
      "    HGBR: MAE Mean = 2.0171\n",
      "    CBR: MAE Mean = 1.8697\n",
      "    LGBM: MAE Mean = 2.6253\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 29 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 29: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 29: Leave-3-Out evaluation complete.\n",
      "  Run 29: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 29: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 29 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 5.2709\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.7385\n",
      "    RF: MAE Mean = 2.0077\n",
      "    GBRT: MAE Mean = 1.4572\n",
      "    ETR: MAE Mean = 1.3451\n",
      "    HGBR: MAE Mean = 1.9177\n",
      "    CBR: MAE Mean = 1.6549\n",
      "    LGBM: MAE Mean = 2.6376\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 30 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 30: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 30: Leave-3-Out evaluation complete.\n",
      "  Run 30: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 30: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 30 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.4607\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.8541\n",
      "    RF: MAE Mean = 1.1855\n",
      "    GBRT: MAE Mean = 1.0949\n",
      "    ETR: MAE Mean = 1.0623\n",
      "    HGBR: MAE Mean = 1.7818\n",
      "    CBR: MAE Mean = 1.5388\n",
      "    LGBM: MAE Mean = 1.8354\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 31 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 31: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 31: Leave-3-Out evaluation complete.\n",
      "  Run 31: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 31: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 31 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 2.1205\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.6258\n",
      "    RF: MAE Mean = 0.9656\n",
      "    GBRT: MAE Mean = 0.4660\n",
      "    ETR: MAE Mean = 0.7632\n",
      "    HGBR: MAE Mean = 0.6601\n",
      "    CBR: MAE Mean = 0.5909\n",
      "    LGBM: MAE Mean = 2.1045\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 32 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 32: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 32: Leave-3-Out evaluation complete.\n",
      "  Run 32: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 32: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 32 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.8247\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 0.9927\n",
      "    RF: MAE Mean = 0.7065\n",
      "    GBRT: MAE Mean = 0.4652\n",
      "    ETR: MAE Mean = 0.6645\n",
      "    HGBR: MAE Mean = 0.4536\n",
      "    CBR: MAE Mean = 0.9254\n",
      "    LGBM: MAE Mean = 1.8931\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 33 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 33: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 33: Leave-3-Out evaluation complete.\n",
      "  Run 33: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 33: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 33 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 5.5495\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.7773\n",
      "    RF: MAE Mean = 1.5681\n",
      "    GBRT: MAE Mean = 0.4241\n",
      "    ETR: MAE Mean = 0.8841\n",
      "    HGBR: MAE Mean = 0.5169\n",
      "    CBR: MAE Mean = 1.0011\n",
      "    LGBM: MAE Mean = 2.7770\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "--- Starting Run 34 ---\n",
      "Initializing models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "  Run 34: Starting Leave-3-Out evaluation...\n",
      "\n",
      "  Run 34: Leave-3-Out evaluation complete.\n",
      "  Run 34: Calculating feature importances based on current run (SHAP)...\n",
      "  Run 34: Feature importance calculation complete (SHAP).\n",
      "\n",
      "--- Run 34 Results Summary ---\n",
      "  Stacking Regressor MAE Mean: 1.2914\n",
      "  Submodel MAE Means:\n",
      "    XGBR: MAE Mean = 1.3535\n",
      "    RF: MAE Mean = 0.4773\n",
      "    GBRT: MAE Mean = 0.6728\n",
      "    ETR: MAE Mean = 0.4225\n",
      "    HGBR: MAE Mean = 2.0005\n",
      "    CBR: MAE Mean = 0.9554\n",
      "    LGBM: MAE Mean = 1.4844\n",
      "  Meta-Learner Coefficients (Fixed Global):\n",
      "    XGBR: 0.0000\n",
      "    RF: 0.0000\n",
      "    GBRT: 0.0000\n",
      "    ETR: 0.0000\n",
      "    HGBR: 0.0000\n",
      "    CBR: 0.0002\n",
      "    LGBM: -2.0561\n",
      "\n",
      "=== Average Results Across All 35 Valid Runs ===\n",
      "Average Stacking Regressor RMSE Mean: 3.5433\n",
      "Average Stacking Regressor MAE Mean: 3.1402\n",
      "\n",
      "Average Submodel R2, RMSE and MAE Means:\n",
      "  XGBR: R2 Mean = -1.9565, RMSE Mean = 1.4235, MAE Mean = 1.2838\n",
      "  RF: R2 Mean = -1.9984, RMSE Mean = 1.6272, MAE Mean = 1.4449\n",
      "  GBRT: R2 Mean = -0.2997, RMSE Mean = 1.1420, MAE Mean = 1.0148\n",
      "  ETR: R2 Mean = -0.9198, RMSE Mean = 1.2579, MAE Mean = 1.1005\n",
      "  HGBR: R2 Mean = -0.4492, RMSE Mean = 1.3722, MAE Mean = 1.1946\n",
      "  CBR: R2 Mean = -0.8790, RMSE Mean = 1.4483, MAE Mean = 1.2905\n",
      "  LGBM: R2 Mean = -5.3450, RMSE Mean = 2.4149, MAE Mean = 2.2265\n",
      "\n",
      "Fixed Global Meta-Learner Coefficients:\n",
      "  XGBR: 0.0000\n",
      "  RF: 0.0000\n",
      "  GBRT: 0.0000\n",
      "  ETR: 0.0000\n",
      "  HGBR: 0.0000\n",
      "  CBR: 0.0002\n",
      "  LGBM: -2.0561\n",
      "\n",
      "Average Weighted Feature Importances (Sorted by SHAP):\n",
      "  Chemical potential (eV): 0.5896\n",
      "  Overall surface area (Angstrom^2): 0.1994\n",
      "  Minimal value (eV): 0.1554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuyi\\AppData\\Local\\Temp\\ipykernel_10680\\3328024168.py:925: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=importances_to_plot, y=features_to_plot, palette=\"viridis\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuyi\\AppData\\Local\\Temp\\ipykernel_10680\\3328024168.py:1139: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(shap_explanation, final_X_data_for_plot, plot_type=\"dot\", show=False) # plot_type=\"dot\" is the swarm plot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregating raw SHAP data and plotting weighted average combined SHAP swarm plot (exp(-RMSE) weighted) ---\n",
      "  exp(-RMSE) weighting coefficients (normalized): {'XGBR': 0.148802389373504, 'RF': 0.1213864269116487, 'GBRT': 0.19718465195378448, 'ETR': 0.17560977584141715, 'HGBR': 0.1566420973948126, 'CBR': 0.14515723143469256, 'LGBM': 0.055217427090140464}\n",
      "\n",
      "--- Exporting aggregated SHAP swarm plot data to Excel/CSV ---\n",
      "Successfully exported aggregated SHAP swarm plot data to: shap_swarm_data_20250710_231931.xlsx\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x270 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exporting prediction results data to Excel/CSV (pivoted by model) ---\n",
      "    Pivoting data to have models as columns...\n",
      "    Data pivoting and column name cleaning complete.\n",
      "Successfully exported pivoted data by model to: cv_predictions_pivoted_20250710_231931.xlsx\n",
      "\n",
      "--- Exporting MAE and RMSE data per run to Excel/CSV (pivoted by model) ---\n",
      "Successfully exported MAE and RMSE data per run to: cv_model_scores_per_run_pivoted_20250710_231931.xlsx\n",
      "\n",
      "Script execution complete.\n"
     ]
    }
   ],
   "source": [
    "# evaluate_stacking_model.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import itertools # Used for generating combinations for LeavePOut\n",
    "\n",
    "# --- SHAP library import ---\n",
    "import shap\n",
    "# ------------------------\n",
    "\n",
    "# Model imports\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# Import StackingRegressor and meta-learner\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import CatBoost and LightGBM (if available)\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: CatBoost is not installed. Skipping CBR model.\")\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: LightGBM is not installed. Skipping LGBM model.\")\n",
    "    lightgbm_available = False\n",
    "\n",
    "# Scikit-learn Utilities\n",
    "from sklearn.model_selection import LeaveOneOut, LeavePOut, KFold # KFold added for meta-learner internal CV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.base import clone\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "### --- Adjustable Configuration Parameters --- ###\n",
    "\n",
    "# --- 1. Data Loading Settings (Note: Data X, Y, and grid_searches are expected to be available in the environment,\n",
    "# typically by running 'optimize_base_learners.py' first.) ---\n",
    "\n",
    "# --- 2. Bayesian Optimization Iterations ---\n",
    "# META_LEARNER_N_ITER_BAYESIAN controls the number of Bayesian optimization iterations for the meta-learner.\n",
    "META_LEARNER_N_ITER_BAYESIAN =  50  # Number of Bayesian optimization iterations for the meta-learner.\n",
    "                                 # Higher values lead to a more thorough search but increase computation time.\n",
    "\n",
    "# --- 3. Default Random State (for all model initializations and Bayesian optimization itself) ---\n",
    "# Note: For LeavePOut and LeaveOneOut (shuffle=False), the splits are deterministic.\n",
    "# The random_state primarily affects the internal randomness of the models.\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 # Ensures reproducibility of results across runs.\n",
    "\n",
    "# --- 4. External Cross-Validation and Overall Evaluation Parameters ---\n",
    "# Adjust OUTER_CV_P_VALUE here to select the external evaluation method:\n",
    "# P=1: Leave-One-Out (LOO) Cross-Validation. Each sample is a test set once.\n",
    "# P=2: Leave-Two-Out (LTO) Cross-Validation. Each pair of samples is a test set once.\n",
    "# P=3: Leave-Three-Out (L3O) Cross-Validation. Each triplet of samples is a test set once.\n",
    "#\n",
    "# Implications for dataset partitioning:\n",
    "# - A higher P value means smaller training sets (N-P samples) and a larger number of total folds (combinations).\n",
    "# - For N=7 samples:\n",
    "#   - P=1 (LOO) results in C(7,1) = 7 combinations (folds).\n",
    "#   - P=2 (LTO) results in C(7,2) = 21 combinations (folds).\n",
    "#   - P=3 (L3O) results in C(7,3) = 35 combinations (folds).\n",
    "# Choose P based on your dataset size and computational resources. For very small N, P=1 or P=2 is common.\n",
    "OUTER_CV_P_VALUE = 3        # <-- P-value for external evaluation.\n",
    "N_SEEDS_FOR_EVALUATION = None # <-- This value will be dynamically calculated based on OUTER_CV_P_VALUE.\n",
    "\n",
    "# --- 5. Noise Intensity Control Interface ---\n",
    "NOISE_SCALE_FACTOR = 0     # Factor for data augmentation (Gaussian noise). Set to 0 to disable.\n",
    "                           # Adding noise to the training data can help regularize the model and\n",
    "                           # prevent overfitting, especially beneficial for small datasets.\n",
    "                           # By tuning this factor, one can potentially mitigate overfitting.\n",
    "                           # In this specific work, noise augmentation is not utilized (factor is 0).\n",
    "\n",
    "# --- 6. SHAP Feature Importance Calculation Parameters ---\n",
    "# For KernelExplainer (used for non-tree models), the background data for SHAP calculation\n",
    "# will directly use the current training set. No separate background sample limit is needed.\n",
    "\n",
    "# --- 7. Plotting Parameters ---\n",
    "N_FEATURES_TO_PLOT = 20 # Number of features to display in the feature importance bar plot.\n",
    "PLOT_SHAP_SWARM_PLOT = True # Whether to generate the SHAP swarm plot.\n",
    "SHAP_SWARM_SAMPLES_LIMIT = 1000 # Maximum number of samples to use for the SHAP swarm plot to prevent memory issues.\n",
    "                                # Set to None to use all available samples.\n",
    "\n",
    "# --- 8. Sub-model Selection Interface for Stacking ---\n",
    "# List the names of the sub-models you wish to include in the Stacking model.\n",
    "# If a model's name is not present in this list, it will be excluded from the Stacking ensemble.\n",
    "# You can easily disable a model by commenting out its entry in this list.\n",
    "# Available model names typically include: 'XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM'\n",
    "ENABLED_SUBMODELS_LIST = [\n",
    "    'XGBR',\n",
    "    'RF',\n",
    "    'GBRT',\n",
    "    'ETR',\n",
    "    'HGBR',\n",
    "    'CBR',\n",
    "    'LGBM'\n",
    "]\n",
    "# Ensure models in ENABLED_SUBMODELS_LIST are actually available\n",
    "if 'CBR' in ENABLED_SUBMODELS_LIST and not catboost_available:\n",
    "    print(\"Warning: CatBoost is not installed. 'CBR' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('CBR')\n",
    "if 'LGBM' in ENABLED_SUBMODELS_LIST and not lightgbm_available:\n",
    "    print(\"Warning: LightGBM is not installed. 'LGBM' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('LGBM')\n",
    "\n",
    "### --- End Configuration Parameters --- ###\n",
    "\n",
    "# --- IMPORTANT: X, Y, and grid_searches variables must be defined in the environment before running this script. ---\n",
    "# For example, by first executing the 'optimize_base_learners.py' script to provide these variables.\n",
    "# If these variables are not defined, this script will fail with a NameError.\n",
    "\n",
    "try:\n",
    "    # Check if X is a DataFrame, which is important for getting indices\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        print(\"Warning: Input X is not a Pandas DataFrame. Sample indices will use RangeIndex.\")\n",
    "        X_index = pd.RangeIndex(start=0, stop=len(X), step=1)\n",
    "    else:\n",
    "        X_index = X.index\n",
    "\n",
    "    # Check Y's type and length\n",
    "    if len(Y) != len(X):\n",
    "         raise ValueError(\"Length of X and Y do not match!\")\n",
    "    if isinstance(Y, pd.DataFrame):\n",
    "        if Y.shape[1] != 1:\n",
    "            raise ValueError(\"If Y is a DataFrame, it should contain only one target column.\")\n",
    "        if not Y.index.equals(X_index):\n",
    "            print(\"Warning: Y's index does not match X's index, which might cause issues. It is recommended to align indices beforehand.\")\n",
    "        Y = Y.iloc[:, 0] # Ensure Y is a Series\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}. Variables 'X', 'Y', or 'grid_searches' are not defined in the current environment. Please ensure 'optimize_base_learners.py' is run first or load the required variables manually.\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Data Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Robust NaN Filling Check ---\n",
    "print(\"\\nForcing data conversion to numeric types and handling non-numeric entries...\")\n",
    "initial_X_nan_count = X.isnull().sum().sum()\n",
    "initial_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "Y = Y.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "new_X_nan_count = X.isnull().sum().sum()\n",
    "new_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "if new_X_nan_count > initial_X_nan_count:\n",
    "    print(f\"Warning: {new_X_nan_count - initial_X_nan_count} non-numeric entries in feature data X were converted to NaN.\")\n",
    "if new_Y_nan_count > initial_Y_nan_count:\n",
    "    print(f\"Warning: {new_Y_nan_count - initial_Y_nan_count} non-numeric entries in target data Y were converted to NaN.\")\n",
    "\n",
    "print(\"\\nChecking and filling NaN values in data...\")\n",
    "if X.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in feature data X. Filling with median.\")\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"NaN values in X filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in feature data X.\")\n",
    "\n",
    "if Y.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in target data Y. Filling with median.\")\n",
    "    y_median_val = Y.median()\n",
    "    if pd.isna(y_median_val): # If the median itself is NaN, it means all Y values are NaN\n",
    "        print(\"Error: All values in target data Y column are NaN. Cannot perform effective filling or model training. Please check the raw data.\")\n",
    "        exit() # Exit the script\n",
    "    Y = Y.fillna(y_median_val)\n",
    "    print(\"NaN values in Y filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in target data Y.\")\n",
    "# --- End Robust NaN Filling Check ---\n",
    "\n",
    "\n",
    "# Check if the P-value for LeavePOut is valid\n",
    "if OUTER_CV_P_VALUE >= X.shape[0]:\n",
    "    print(f\"Error: The P-value for Leave-P-Out ({OUTER_CV_P_VALUE}) must be less than the total number of samples ({X.shape[0]}). Please adjust OUTER_CV_P_VALUE.\")\n",
    "    exit()\n",
    "if OUTER_CV_P_VALUE < 1:\n",
    "    print(f\"Error: The P-value for Leave-P-Out ({OUTER_CV_P_VALUE}) must be greater than or equal to 1. Please adjust OUTER_CV_P_VALUE.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Dataset sample size N = {X.shape[0]}\")\n",
    "print(f\"External evaluation will use Leave-{OUTER_CV_P_VALUE}-Out Cross-Validation.\")\n",
    "# Generate all possible test set combinations (e.g., N=7, P=2 will generate C(7,2) = 21 combinations)\n",
    "all_sample_indices = list(range(len(X)))\n",
    "lpo_combinations = list(itertools.combinations(all_sample_indices, OUTER_CV_P_VALUE))\n",
    "N_SEEDS_FOR_EVALUATION = len(lpo_combinations)\n",
    "print(f\"Total combinations for C({X.shape[0]}, {OUTER_CV_P_VALUE}) = {N_SEEDS_FOR_EVALUATION} folds.\")\n",
    "\n",
    "\n",
    "# --- Noise Intensity Control Interface ---\n",
    "if NOISE_SCALE_FACTOR > 0:\n",
    "    print(f\"Gaussian noise augmentation is enabled with intensity factor: {NOISE_SCALE_FACTOR}\")\n",
    "else:\n",
    "    print(\"Noise augmentation is disabled.\")\n",
    "np.random.seed(DEFAULT_MODEL_RANDOM_STATE) # Seed for noise generation\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def initialize_best_estimators(grid_searches_dict, enabled_models_list):\n",
    "    \"\"\"\n",
    "    Initializes best base learner instances from BayesSearchCV results, filtered by the enabled_models_list.\n",
    "    This function is used to retrieve the optimized base models for stacking.\n",
    "\n",
    "    Args:\n",
    "        grid_searches_dict (dict): A dictionary containing BayesSearchCV results for each model.\n",
    "        enabled_models_list (list): A list of model names that are enabled for stacking.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of initialized best estimator instances.\n",
    "    \"\"\"\n",
    "    estimators_init = {}\n",
    "    # Define all possible models and their default parameters\n",
    "    all_possible_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "\n",
    "    if catboost_available:\n",
    "        all_possible_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available:\n",
    "        all_possible_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "    for name, (model_class, fixed_params) in all_possible_models.items():\n",
    "        if name not in enabled_models_list: # If the model is not in the enabled list, skip it\n",
    "            print(f\"  Model {name} is not in ENABLED_SUBMODELS_LIST, skipping initialization.\")\n",
    "            continue\n",
    "\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} with optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "        else:\n",
    "            print(f\"  No optimized results found for {name}. Attempting to initialize with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} with default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (with default parameters): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v is not None}\n",
    "    if not initialized_estimators:\n",
    "         print(\"Warning: No models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "\n",
    "# --- Define Meta-Learner Hyperparameter Search Space ---\n",
    "meta_learner_params = {\n",
    "    'elasticnet__alpha': Real(1e-5, 10.0, prior='log-uniform', name='elasticnet__alpha'),\n",
    "    'elasticnet__l1_ratio': Real(0.0, 1.0, prior='uniform', name='elasticnet__l1_ratio')\n",
    "}\n",
    "\n",
    "# Global variables to store the best meta-learner instance (with fixed hyperparameters and coefficients)\n",
    "# after a one-time pre-tuning.\n",
    "global_fixed_meta_learner = None\n",
    "global_fixed_meta_coeffs_dict = None # Stores the final fixed coefficients for printing and inspection.\n",
    "\n",
    "# Define function to evaluate model performance and feature importance\n",
    "def evaluate_models_manual_cv(run_idx, X_full, Y_full, X_full_index, grid_searches_dict, noise_factor,\n",
    "                              fixed_meta_learner_instance, current_test_indices):\n",
    "    \"\"\"\n",
    "    Performs a manual Leave-P-Out CV iteration to generate Out-Of-Fold (OOF) predictions,\n",
    "    evaluate the Stacking model's performance, and calculate feature importances using SHAP values.\n",
    "\n",
    "    Args:\n",
    "        run_idx (int): Current run index for logging.\n",
    "        X_full (pd.DataFrame or np.array): Full feature dataset.\n",
    "        Y_full (pd.Series or np.array): Full target dataset.\n",
    "        X_full_index (pd.Index or np.array): Original indices of X_full.\n",
    "        grid_searches_dict (dict): Dictionary of pre-tuned base model results.\n",
    "        noise_factor (float): Factor for Gaussian noise augmentation.\n",
    "        fixed_meta_learner_instance (sklearn.pipeline.Pipeline): The pre-tuned and fixed meta-learner.\n",
    "        current_test_indices (list): Indices of samples for the current test fold.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics, feature importances, plot data,\n",
    "              meta-learner coefficients, and raw SHAP values for the current run.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Run {run_idx} ---\")\n",
    "  \n",
    "    # Initialize base models, filtered by ENABLED_SUBMODELS_LIST\n",
    "    best_estimators_dict = initialize_best_estimators(grid_searches_dict, ENABLED_SUBMODELS_LIST)\n",
    "\n",
    "    if not best_estimators_dict:\n",
    "        print(f\"Run {run_idx}: No base models available for evaluation.\")\n",
    "        return {\n",
    "            'submodel_r2_means': {}, 'submodel_rmse_means': {}, 'submodel_mae_means': {},\n",
    "            'stacking_regressor_rmse_mean': np.nan, 'stacking_regressor_mae_mean': np.nan,\n",
    "            'submodel_feature_importances': {}, 'weighted_feature_importances': {},\n",
    "            'plot_data': [], 'meta_learner_coefficients': {},\n",
    "            'raw_shap_values': {}, 'shap_expected_values': {}, 'X_val_data_for_shap': [],\n",
    "            'all_scores_data_for_seed_fold': []\n",
    "        }\n",
    "\n",
    "    submodels = list(best_estimators_dict.items())\n",
    "    submodel_names = [name for name, _ in submodels]\n",
    "\n",
    "    # --- Initialize storage for Fold results (now for a single run's results) ---\n",
    "    fold_r2_scores = {name: [] for name in submodel_names}\n",
    "    fold_rmse_scores = {name: [] for name in submodel_names}\n",
    "    fold_mae_scores = {name: [] for name in submodel_names}\n",
    "    fold_rmse_scores['Stacking'] = []\n",
    "    fold_mae_scores['Stacking'] = []\n",
    "    all_plot_data_for_seed = []\n",
    "    all_scores_data_for_seed_fold = []\n",
    "\n",
    "    # --- Initialize accumulators for feature importances (sub-models) ---\n",
    "    n_features = X_full.shape[1]\n",
    "    fold_feature_importances_sums = {name: np.zeros(n_features) for name in submodel_names}\n",
    "    fold_feature_importances_counts = {name: 0 for name in submodel_names}\n",
    "\n",
    "    # --- New: Lists to collect raw SHAP values ---\n",
    "    all_raw_shap_values_per_model = {name: [] for name in submodel_names}\n",
    "    all_shap_expected_values_per_model = {name: [] for name in submodel_names}\n",
    "    all_X_val_data_for_shap_folds = [] # Stores X_val_fold (DataFrame) for the current run\n",
    "\n",
    "    print(f\"  Run {run_idx}: Starting Leave-{OUTER_CV_P_VALUE}-Out evaluation...\")\n",
    "    is_X_dataframe = isinstance(X_full, pd.DataFrame)\n",
    "    original_columns = X_full.columns if is_X_dataframe else [f'Feature_{i}' for i in range(n_features)]\n",
    "    feature_names = list(original_columns)\n",
    "\n",
    "    # --- External Leave-P-Out loop (now a single iteration, with train/val indices provided by the main loop) ---\n",
    "    train_loc_idx = np.array(list(set(range(len(X_full))) - set(current_test_indices)))\n",
    "    val_loc_idx = np.array(current_test_indices)\n",
    "\n",
    "    train_original_indices = X_full_index[train_loc_idx]\n",
    "    val_original_indices = X_full_index[val_loc_idx]\n",
    "\n",
    "    if is_X_dataframe:\n",
    "        X_train_fold, X_val_fold = X_full.loc[train_original_indices], X_full.loc[val_original_indices]\n",
    "        if isinstance(Y_full, (pd.Series, pd.DataFrame)):\n",
    "            Y_train_fold, Y_val_fold = Y_full.loc[train_original_indices], Y_full.loc[val_original_indices]\n",
    "        else:\n",
    "            Y_train_fold, Y_val_fold = Y_full[train_loc_idx], Y_full[val_loc_idx]\n",
    "    else:\n",
    "        X_train_fold, X_val_fold = X_full[train_loc_idx], X_full[val_loc_idx]\n",
    "        Y_train_fold, Y_val_fold = Y_full[train_loc_idx], Y_full[val_loc_idx]\n",
    "\n",
    "    # --- Crucial: Add the current X_val_fold to the list for SHAP aggregation ---\n",
    "    all_X_val_data_for_shap_folds.append(X_val_fold)\n",
    "\n",
    "    y_val_true_np = Y_val_fold.values if isinstance(Y_val_fold, (pd.Series, pd.DataFrame)) else np.array(Y_val_fold)\n",
    "    y_train_true_np = Y_train_fold.values if isinstance(Y_train_fold, (pd.Series, pd.DataFrame)) else np.array(Y_train_fold)\n",
    "\n",
    "    X_train_augmented = X_train_fold\n",
    "    if noise_factor > 0:\n",
    "        # Apply Gaussian noise to the training features for data augmentation.\n",
    "        # This helps in regularizing the model and improving its generalization\n",
    "        # by making it robust to small perturbations in the input data.\n",
    "        X_train_fold_np = X_train_fold.values if is_X_dataframe else X_train_fold\n",
    "        feature_std_devs = np.std(X_train_fold_np, axis=0)\n",
    "        noise_std_devs = feature_std_devs * noise_factor + 1e-9\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_std_devs, size=X_train_fold_np.shape)\n",
    "        if is_X_dataframe:\n",
    "            X_train_augmented = pd.DataFrame(X_train_fold_np + noise, index=train_original_indices, columns=original_columns)\n",
    "        else:\n",
    "            X_train_augmented = X_train_fold_np + noise\n",
    "\n",
    "    # --- Store current run's OOF predictions (from base learners) ---\n",
    "    # Strict OOF prediction generation for meta-learner training on current external training set\n",
    "    oof_preds_train = np.zeros((len(Y_train_fold), len(submodels)))\n",
    "    oof_preds_val = np.zeros((len(Y_val_fold), len(submodels)))\n",
    "\n",
    "    # Use LeaveOneOut on the current external training set to generate OOF predictions for meta-learner training.\n",
    "    # For example, if external evaluation is Leave-Two-Out (P=2), the training set size is N-2 = 5 samples.\n",
    "    # LeaveOneOut will generate 5 folds for this internal OOF generation.\n",
    "    kf_for_oof_train = LeaveOneOut() # Base learners always use LeaveOneOut for OOF prediction generation for the meta-learner.\n",
    "  \n",
    "    for i, (name, estimator_template) in enumerate(submodels):\n",
    "        current_model_oof_preds = np.zeros(len(Y_train_fold))\n",
    "      \n",
    "        try:\n",
    "            # Generate OOF predictions for the current external training fold\n",
    "            for inner_train_idx, inner_val_idx in kf_for_oof_train.split(X_train_augmented, Y_train_fold):\n",
    "                X_inner_train = X_train_augmented.iloc[inner_train_idx] if is_X_dataframe else X_train_augmented[inner_train_idx]\n",
    "                Y_inner_train = Y_train_fold.iloc[inner_train_idx] if isinstance(Y_train_fold, pd.Series) else Y_train_fold[inner_train_idx]\n",
    "                X_inner_val = X_train_augmented.iloc[inner_val_idx] if is_X_dataframe else X_train_augmented[inner_val_idx]\n",
    "\n",
    "                estimator_clone = clone(estimator_template)\n",
    "                # Apply noise to X_inner_train if noise_factor > 0\n",
    "                X_inner_train_augmented_for_oof = X_inner_train\n",
    "                if noise_factor > 0:\n",
    "                    # Apply Gaussian noise to the training features for data augmentation.\n",
    "                    # This helps in regularizing the model and improving its generalization\n",
    "                    # by making it robust to small perturbations in the input data.\n",
    "                    X_inner_train_np_for_oof = X_inner_train.values if isinstance(X_inner_train, pd.DataFrame) else X_inner_train\n",
    "                    feature_std_devs_for_oof = np.std(X_inner_train_np_for_oof, axis=0)\n",
    "                    noise_std_devs_for_oof = feature_std_devs_for_oof * noise_factor + 1e-9\n",
    "                    noise_for_oof = np.random.normal(loc=0.0, scale=noise_std_devs_for_oof, size=X_inner_train_np_for_oof.shape)\n",
    "                    if isinstance(X_inner_train, pd.DataFrame):\n",
    "                        X_inner_train_augmented_for_oof = pd.DataFrame(X_inner_train_np_for_oof + noise_for_oof, index=X_inner_train.index, columns=X_inner_train.columns)\n",
    "                    else:\n",
    "                        X_inner_train_augmented_for_oof = X_inner_train_np_for_oof + noise_for_oof\n",
    "\n",
    "                estimator_clone.fit(X_inner_train_augmented_for_oof, Y_inner_train) # Use augmented data for training\n",
    "                current_model_oof_preds[inner_val_idx] = estimator_clone.predict(X_inner_val) # Predict on original (non-augmented) inner_val_set\n",
    "          \n",
    "            oof_preds_train[:, i] = current_model_oof_preds\n",
    "          \n",
    "            # Train a final model on the full X_train_augmented for prediction on X_val_fold\n",
    "            final_estimator_for_val_pred = clone(estimator_template)\n",
    "            final_estimator_for_val_pred.fit(X_train_augmented, Y_train_fold)\n",
    "            oof_preds_val[:, i] = final_estimator_for_val_pred.predict(X_val_fold)\n",
    "\n",
    "\n",
    "        except Exception as fit_e:\n",
    "             print(f\"\\nWarning: Run {run_idx}, Model {name} training or OOF generation failed: {fit_e}\")\n",
    "             oof_preds_train[:, i] = 0\n",
    "             oof_preds_val[:, i] = 0\n",
    "             # Record NaN scores for failed models\n",
    "             all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'MAE', 'Value': np.nan})\n",
    "             all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'RMSE', 'Value': np.nan})\n",
    "             continue\n",
    "\n",
    "        # Calculate metrics for base models on the current validation fold\n",
    "        r2 = r2_score(Y_val_fold, oof_preds_val[:, i])\n",
    "        rmse = np.sqrt(mean_squared_error(Y_val_fold, oof_preds_val[:, i]))\n",
    "        mae = mean_absolute_error(Y_val_fold, oof_preds_val[:, i])\n",
    "\n",
    "        fold_r2_scores[name].append(r2)\n",
    "        fold_rmse_scores[name].append(rmse)\n",
    "        fold_mae_scores[name].append(mae)\n",
    "      \n",
    "        # --- New: Record MAE and RMSE for each sub-model in the current run ---\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'MAE', 'Value': mae})\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'RMSE', 'Value': rmse})\n",
    "\n",
    "        for idx, (actual, pred) in enumerate(zip(y_val_true_np, oof_preds_val[:, i])):\n",
    "            original_index = val_original_indices[idx]\n",
    "            all_plot_data_for_seed.append({\n",
    "                'Seed': run_idx, 'Fold': 0, 'Model': name, # Fold unified to 0 for external loop\n",
    "                'Set': 'Validation',\n",
    "                'Actual': actual, 'Predicted': pred,\n",
    "                'Sample_Index': original_index\n",
    "            })\n",
    "        for idx, (actual, pred) in enumerate(zip(y_train_true_np, oof_preds_train[:, i])):\n",
    "            original_index = train_original_indices[idx]\n",
    "            all_plot_data_for_seed.append({\n",
    "                'Seed': run_idx, 'Fold': 0, 'Model': name, # Fold unified to 0 for external loop\n",
    "                'Set': 'Train',\n",
    "                'Actual': actual, 'Predicted': pred,\n",
    "                'Sample_Index': original_index\n",
    "            })\n",
    "\n",
    "        # --- SHAP Feature Importance Calculation ---\n",
    "        try:\n",
    "            # Convert to NumPy array for SHAP compatibility\n",
    "            X_val_fold_np = X_val_fold.values if isinstance(X_val_fold, pd.DataFrame) else X_val_fold\n",
    "            X_train_fold_np = X_train_fold.values if isinstance(X_train_fold, pd.DataFrame) else X_train_fold\n",
    "\n",
    "            # Ensure validation set is not empty, otherwise SHAP will error\n",
    "            if X_val_fold_np.shape[0] == 0:\n",
    "                print(f\"Warning: Run {run_idx}, Model {name}: Validation set is empty, skipping SHAP calculation.\")\n",
    "                continue\n",
    "\n",
    "            # Check model type to select appropriate Explainer\n",
    "            if isinstance(final_estimator_for_val_pred, (XGB.XGBRegressor, RandomForestRegressor,\n",
    "                                           GradientBoostingRegressor, CatBoostRegressor, LGBMRegressor,\n",
    "                                           ExtraTreesRegressor, HistGradientBoostingRegressor)):\n",
    "                explainer = shap.TreeExplainer(final_estimator_for_val_pred)\n",
    "                shap_values = explainer.shap_values(X_val_fold_np)\n",
    "                expected_value = explainer.expected_value\n",
    "            else: # For non-tree models, use KernelExplainer\n",
    "                # For N=7, P=2, training set has 5 samples, so background data is the training set itself.\n",
    "                # Ensure background data is not empty.\n",
    "                if X_train_fold_np.shape[0] == 0:\n",
    "                    print(f\"Warning: Run {run_idx}, Model {name}: Training set is empty, cannot generate background data for KernelExplainer, skipping SHAP calculation.\")\n",
    "                    continue\n",
    "\n",
    "                background_data = X_train_fold_np\n",
    "              \n",
    "                print(f\"  Run {run_idx}, Model {name}: Calculating SHAP using KernelExplainer (may be slow)...\")\n",
    "                explainer = shap.KernelExplainer(final_estimator_for_val_pred.predict, background_data)\n",
    "                shap_values = explainer.shap_values(X_val_fold_np)\n",
    "                expected_value = explainer.expected_value\n",
    "          \n",
    "            # --- Store raw SHAP values and expected values ---\n",
    "            all_raw_shap_values_per_model[name].append(shap_values)\n",
    "            all_shap_expected_values_per_model[name].append(expected_value)\n",
    "\n",
    "            # Global feature importance from SHAP is typically the mean absolute SHAP value (for bar plots)\n",
    "            importances = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "            if importances.shape[0] == n_features:\n",
    "                 fold_feature_importances_sums[name] += importances\n",
    "                 fold_feature_importances_counts[name] += 1\n",
    "            else:\n",
    "                print(f\"Warning: Run {run_idx}, Model {name} SHAP importance dimension mismatch.\")\n",
    "\n",
    "        except Exception as imp_e:\n",
    "             print(f\"\\nWarning: Run {run_idx}, Model {name} error getting SHAP importance: {imp_e}\")\n",
    "        # --- END SHAP Calculation ---\n",
    "\n",
    "    # --- Train and Evaluate Meta-Learner (ElasticNet) ---\n",
    "    if np.all(oof_preds_train == 0) or np.any(np.isnan(oof_preds_train)):\n",
    "        print(f\"Warning: Run {run_idx}: OOF predictions are all zeros or contain NaN, skipping meta-learner training.\")\n",
    "        fold_rmse_scores['Stacking'].append(np.nan)\n",
    "        fold_mae_scores['Stacking'].append(np.nan)\n",
    "        # Record NaN scores for failed models\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': np.nan})\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': np.nan})\n",
    "        return {\n",
    "            'submodel_r2_means': {}, 'submodel_rmse_means': {}, 'submodel_mae_means': {},\n",
    "            'stacking_regressor_rmse_mean': np.nan,\n",
    "            'stacking_regressor_mae_mean': np.nan,\n",
    "            'submodel_feature_importances': {}, 'weighted_feature_importances': {},\n",
    "            'plot_data': all_plot_data_for_seed, 'meta_learner_coefficients': {},\n",
    "            'raw_shap_values': {}, 'shap_expected_values': {}, 'X_val_data_for_shap': [],\n",
    "            'all_scores_data_for_seed_fold': []\n",
    "        }\n",
    "\n",
    "    # Use the globally fixed meta-learner instance for prediction, no re-training here.\n",
    "    try:\n",
    "        y_pred_val_stacking = fixed_meta_learner_instance.predict(oof_preds_val)\n",
    "        y_pred_train_stacking = fixed_meta_learner_instance.predict(oof_preds_train)\n",
    "    except Exception as meta_pred_e:\n",
    "        print(f\"\\nWarning: Run {run_idx}, prediction with fixed meta-learner failed: {meta_pred_e}\")\n",
    "        fold_rmse_scores['Stacking'].append(np.nan)\n",
    "        fold_mae_scores['Stacking'].append(np.nan)\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': np.nan})\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': np.nan})\n",
    "        return {\n",
    "            'submodel_r2_means': {name: np.nan for name in submodel_names},\n",
    "            'submodel_rmse_means': {name: np.nan for name in submodel_names},\n",
    "            'submodel_mae_means': {name: np.nan for name in submodel_names},\n",
    "            'stacking_regressor_rmse_mean': np.nan,\n",
    "            'stacking_regressor_mae_mean': np.nan,\n",
    "            'submodel_feature_importances': {}, 'weighted_feature_importances': {},\n",
    "            'plot_data': all_plot_data_for_seed, 'meta_learner_coefficients': {},\n",
    "            'raw_shap_values': {},\n",
    "            'shap_expected_values': {},\n",
    "            'X_val_data_for_shap': [],\n",
    "            'all_scores_data_for_seed_fold': all_scores_data_for_seed_fold\n",
    "        }\n",
    "\n",
    "    rmse_stacking = np.sqrt(mean_squared_error(Y_val_fold, y_pred_val_stacking))\n",
    "    mae_stacking = mean_absolute_error(Y_val_fold, y_pred_val_stacking)\n",
    "\n",
    "    fold_rmse_scores['Stacking'].append(rmse_stacking)\n",
    "    fold_mae_scores['Stacking'].append(mae_stacking)\n",
    "  \n",
    "    # --- New: Record MAE and RMSE for the Stacking model in the current run ---\n",
    "    all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': mae_stacking})\n",
    "    all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': rmse_stacking})\n",
    "\n",
    "    for idx, (actual, pred) in enumerate(zip(y_val_true_np, y_pred_val_stacking)):\n",
    "        original_index = val_original_indices[idx]\n",
    "        all_plot_data_for_seed.append({\n",
    "            'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking',\n",
    "            'Set': 'Validation',\n",
    "            'Actual': actual, 'Predicted': pred,\n",
    "            'Sample_Index': original_index\n",
    "        })\n",
    "    for idx, (actual, pred) in enumerate(zip(y_train_true_np, y_pred_train_stacking)):\n",
    "        original_index = train_original_indices[idx]\n",
    "        all_plot_data_for_seed.append({\n",
    "            'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking',\n",
    "            'Set': 'Train',\n",
    "            'Actual': actual, 'Predicted': pred,\n",
    "            'Sample_Index': original_index\n",
    "        })\n",
    "\n",
    "\n",
    "    # --- End of Run ---\n",
    "    print(f\"\\n  Run {run_idx}: Leave-{OUTER_CV_P_VALUE}-Out evaluation complete.\")\n",
    "\n",
    "    # --- Calculate average scores (now just taking the values from the list for a single run) ---\n",
    "    submodel_r2_means = {name: scores[0] if scores else np.nan for name, scores in fold_r2_scores.items() if scores}\n",
    "    submodel_rmse_means = {name: scores[0] if scores else np.nan for name, scores in fold_rmse_scores.items() if scores}\n",
    "    submodel_mae_means = {name: scores[0] if scores else np.nan for name, scores in fold_mae_scores.items() if scores}\n",
    "    stacking_regressor_rmse_mean = fold_rmse_scores['Stacking'][0] if fold_rmse_scores['Stacking'] else np.nan\n",
    "    stacking_regressor_mae_mean = fold_mae_scores['Stacking'][0] if fold_mae_scores['Stacking'] else np.nan\n",
    "\n",
    "    # --- Calculate Feature Importances (Sub-models) ---\n",
    "    print(f\"  Run {run_idx}: Calculating feature importances based on current run (SHAP)...\")\n",
    "    submodel_avg_seed_importances = {}\n",
    "    feature_importances_weighted_sum = np.zeros(n_features)\n",
    "    total_exp_rmse_weighting = 0 # Sum of exp(-RMSE) for weighting\n",
    "    for name in submodel_names:\n",
    "        count = fold_feature_importances_counts[name]\n",
    "        if count > 0:\n",
    "            avg_importances = fold_feature_importances_sums[name] / count\n",
    "            sum_avg_importances = np.sum(avg_importances)\n",
    "            if sum_avg_importances > 1e-9:\n",
    "                importances_normalized = avg_importances / sum_avg_importances\n",
    "            else:\n",
    "                importances_normalized = np.zeros_like(avg_importances)\n",
    "            submodel_avg_seed_importances[name] = dict(zip(feature_names, importances_normalized))\n",
    "          \n",
    "            # Use exp(-RMSE) for weighting\n",
    "            model_rmse_val = submodel_rmse_means.get(name, np.nan)\n",
    "            if not np.isnan(model_rmse_val): # RMSE will not be negative, only check for NaN\n",
    "                 weight = np.exp(-model_rmse_val) # exp(-RMSE)\n",
    "                 feature_importances_weighted_sum += importances_normalized * weight\n",
    "                 total_exp_rmse_weighting += weight\n",
    "        else:\n",
    "            submodel_avg_seed_importances[name] = {}\n",
    "  \n",
    "    if total_exp_rmse_weighting > 1e-9:\n",
    "        weighted_feature_importances_vector = feature_importances_weighted_sum / total_exp_rmse_weighting\n",
    "    else:\n",
    "        weighted_feature_importances_vector = np.zeros(n_features)\n",
    "    weighted_feature_importances_dict = dict(zip(feature_names, weighted_feature_importances_vector))\n",
    "    print(f\"  Run {run_idx}: Feature importance calculation complete (SHAP).\")\n",
    "\n",
    "    # Return the globally fixed meta-learner coefficients\n",
    "    return {\n",
    "        'submodel_r2_means': submodel_r2_means,\n",
    "        'submodel_rmse_means': submodel_rmse_means,\n",
    "        'submodel_mae_means': submodel_mae_means,\n",
    "        'stacking_regressor_rmse_mean': stacking_regressor_rmse_mean,\n",
    "        'stacking_regressor_mae_mean': stacking_regressor_mae_mean,\n",
    "        'submodel_feature_importances': submodel_avg_seed_importances,\n",
    "        'weighted_feature_importances': weighted_feature_importances_dict,\n",
    "        'plot_data': all_plot_data_for_seed,\n",
    "        'meta_learner_coefficients': global_fixed_meta_coeffs_dict, # Always return the fixed global coefficients\n",
    "        'raw_shap_values': all_raw_shap_values_per_model, # New return\n",
    "        'shap_expected_values': all_shap_expected_values_per_model, # New return\n",
    "        'X_val_data_for_shap': all_X_val_data_for_shap_folds, # New return\n",
    "        'all_scores_data_for_seed_fold': all_scores_data_for_seed_fold # New return\n",
    "    }\n",
    "\n",
    "# === Main Execution Flow ===\n",
    "# Get X's index and feature names\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    X_index = X.index\n",
    "    feature_names_list = X.columns.tolist()\n",
    "else:\n",
    "    print(\"Warning: Input X is not a Pandas DataFrame. Feature importances will use default names 'Feature_i', and sample indices will use RangeIndex.\")\n",
    "    X_index = pd.RangeIndex(start=0, stop=len(X), step=1)\n",
    "    feature_names_list = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# Get available model names (filtered by ENABLED_SUBMODELS_LIST)\n",
    "try:\n",
    "    _initial_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "    model_names_available = list(_initial_estimators.keys())\n",
    "    if not model_names_available:\n",
    "        print(\"Error: No models successfully initialized, cannot proceed. Please check ENABLED_SUBMODELS_LIST.\")\n",
    "        exit()\n",
    "    del _initial_estimators\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while trying to initialize models to get names: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Meta-Learner Hyperparameter Pre-tuning Stage (One-time execution) ---\n",
    "# The meta-learner tuning uses Leave-One-Out CV for OOF generation and internal CV, then fixes coefficients.\n",
    "print(f\"\\n=== Starting Meta-Learner Hyperparameter Pre-tuning (One-time execution, using Leave-One-Out CV for OOF generation and internal CV) ===\")\n",
    "\n",
    "# Generate OOF predictions for ALL N samples for this meta-tuning iteration\n",
    "oof_preds_full_current_iter = np.zeros((len(X), len(model_names_available)))\n",
    "\n",
    "# Initialize base estimators, filtered by ENABLED_SUBMODELS_LIST\n",
    "temp_base_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "if not temp_base_estimators:\n",
    "    print(\"Warning: Failed to initialize base learners, skipping meta-learner tuning.\")\n",
    "    # Fallback to a default ElasticNet if base learners fail\n",
    "    global_fixed_meta_learner = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "    ])\n",
    "    # Also set default coefficients if no tuning happens\n",
    "    global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "else:\n",
    "    # Use LeaveOneOut to generate OOF predictions on the entire dataset (e.g., N=7)\n",
    "    loo_for_full_oof = LeaveOneOut() # Meta-learner OOF generation always uses LeaveOneOut.\n",
    "  \n",
    "    # This check is mostly for general robustness; for N=7, LOOCV is fine.\n",
    "    if loo_for_full_oof.get_n_splits(X) < 2:\n",
    "        print(f\"Warning: LeaveOneOut cannot generate at least 2 folds with {len(X)} samples. Skipping meta-learner tuning.\")\n",
    "        global_fixed_meta_learner = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "        ])\n",
    "        global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "    else:\n",
    "        for i, (name, estimator_template) in enumerate(temp_base_estimators.items()):\n",
    "            print(f\"  Generating OOF predictions for model {name} (for meta-learner tuning)...\")\n",
    "            fold_preds = np.zeros(len(X))\n",
    "          \n",
    "            # Generate OOF predictions using LeaveOneOut\n",
    "            for train_idx, val_idx in loo_for_full_oof.split(X, Y):\n",
    "                X_inner_train = X.iloc[train_idx] if isinstance(X, pd.DataFrame) else X[train_idx]\n",
    "                Y_inner_train = Y.iloc[train_idx] if isinstance(Y, pd.Series) else Y[train_idx]\n",
    "                X_inner_val = X.iloc[val_idx] if isinstance(X, pd.DataFrame) else X[val_idx]\n",
    "\n",
    "                estimator_clone = clone(estimator_template)\n",
    "                # Apply noise to X_inner_train if NOISE_SCALE_FACTOR > 0\n",
    "                X_inner_train_augmented_for_oof_meta_tune = X_inner_train\n",
    "                if NOISE_SCALE_FACTOR > 0:\n",
    "                    # Apply Gaussian noise to the training features for data augmentation.\n",
    "                    # This helps in regularizing the model and improving its generalization\n",
    "                    # by making it robust to small perturbations in the input data.\n",
    "                    X_inner_train_np_for_oof_meta_tune = X_inner_train.values if isinstance(X_inner_train, pd.DataFrame) else X_inner_train\n",
    "                    feature_std_devs_for_oof_meta_tune = np.std(X_inner_train_np_for_oof_meta_tune, axis=0)\n",
    "                    noise_std_devs_for_oof_meta_tune = feature_std_devs_for_oof_meta_tune * NOISE_SCALE_FACTOR + 1e-9\n",
    "                    noise_for_oof_meta_tune = np.random.normal(loc=0.0, scale=noise_std_devs_for_oof_meta_tune, size=X_inner_train_np_for_oof_meta_tune.shape)\n",
    "                    if isinstance(X_inner_train, pd.DataFrame):\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = pd.DataFrame(X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune, index=X_inner_train.index, columns=X_inner_train.columns)\n",
    "                    else:\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune\n",
    "\n",
    "                estimator_clone.fit(X_inner_train_augmented_for_oof_meta_tune, Y_inner_train) # Use augmented data for training\n",
    "                fold_preds[val_idx] = estimator_clone.predict(X_inner_val) # Predict on original (non-augmented) inner_val_set\n",
    "            oof_preds_full_current_iter[:, i] = fold_preds\n",
    "\n",
    "        if np.all(oof_preds_full_current_iter == 0) or np.any(np.isnan(oof_preds_full_current_iter)):\n",
    "            print(\"Warning: Generated OOF predictions are all zeros or contain NaN, skipping meta-learner tuning.\")\n",
    "            global_fixed_meta_learner = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "            ])\n",
    "            global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "        else:\n",
    "            print(\"  Starting meta-learner Bayesian optimization...\")\n",
    "            meta_bayes_search = BayesSearchCV(\n",
    "                estimator=Pipeline([('scaler', StandardScaler()), ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))]),\n",
    "                search_spaces=meta_learner_params,\n",
    "                n_iter=META_LEARNER_N_ITER_BAYESIAN,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                cv=LeaveOneOut(), # Meta-learner internal CV always uses LeaveOneOut.\n",
    "                n_jobs=-1,\n",
    "                random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                verbose=0\n",
    "            )\n",
    "            try:\n",
    "                meta_bayes_search.fit(oof_preds_full_current_iter, Y) # Use full OOF and full Y\n",
    "                print(f\"  Meta-learner Best Score (RMSE): {np.sqrt(-meta_bayes_search.best_score_):.4f}\")\n",
    "                print(f\"  Meta-learner Best Parameters: {dict(meta_bayes_search.best_params_)}\")\n",
    "              \n",
    "                # Initialize and train the final meta-learner with optimized parameters, then fix its coefficients.\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(\n",
    "                        alpha=meta_bayes_search.best_params_.get('elasticnet__alpha', 1.0),\n",
    "                        l1_ratio=meta_bayes_search.best_params_.get('elasticnet__l1_ratio', 0.5),\n",
    "                        random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                        max_iter=2000\n",
    "                    ))\n",
    "                ])\n",
    "                # Train once on the entire OOF dataset to fix coefficients\n",
    "                global_fixed_meta_learner.fit(oof_preds_full_current_iter, Y)\n",
    "              \n",
    "                # Extract and store fixed coefficients\n",
    "                elastic_net_model = global_fixed_meta_learner.named_steps['elasticnet']\n",
    "                global_fixed_meta_coeffs_dict = dict(zip(model_names_available, elastic_net_model.coef_))\n",
    "\n",
    "                print(\"Global best meta-learner initialized with optimized parameters and fixed coefficients.\")\n",
    "                print(f\"Fixed Meta-Learner Coefficients: {global_fixed_meta_coeffs_dict}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Meta-learner tuning or final training failed: {e}\")\n",
    "                print(\"Using default meta-learner parameters, and coefficients cannot be fixed.\")\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "                ])\n",
    "                global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available} # Indicate failure for coeffs\n",
    "\n",
    "print(\"\\n--- Meta-Learner Hyperparameter Tuning Complete ---\")\n",
    "\n",
    "\n",
    "# Define storage for results\n",
    "all_results = []\n",
    "all_plot_data_accumulated = []\n",
    "all_scores_accumulated = [] # New: Stores MAE/RMSE data for all runs\n",
    "\n",
    "# Initialize accumulators for cross-run averages\n",
    "submodel_r2_sums = {name: 0.0 for name in model_names_available}\n",
    "submodel_rmse_sums = {name: 0.0 for name in model_names_available}\n",
    "submodel_mae_sums = {name: 0.0 for name in model_names_available}\n",
    "stacking_rmse_sum = 0.0\n",
    "stacking_mae_sum = 0.0\n",
    "weighted_feature_importances_sums = pd.Series(0.0, index=feature_names_list)\n",
    "# meta_coefficients_sums_across_seeds is redundant as coefficients are now fixed globally\n",
    "valid_runs_count = 0\n",
    "\n",
    "# --- New: List to aggregate all raw SHAP values ---\n",
    "all_aggregated_shap_values_for_swarm_raw_data = [] # Stores raw SHAP data for each model in each run\n",
    "\n",
    "print(f\"\\n=== Starting Repeated Evaluation on {N_SEEDS_FOR_EVALUATION} Leave-{OUTER_CV_P_VALUE}-Out Combinations ===\")\n",
    "for run_idx, test_indices_tuple in enumerate(lpo_combinations): # Iterate through all combinations\n",
    "    test_indices = list(test_indices_tuple) # Convert tuple to list\n",
    "  \n",
    "    # Call the evaluation function, passing the globally fixed meta-learner\n",
    "    result = evaluate_models_manual_cv(run_idx, X, Y, X_index, grid_searches, NOISE_SCALE_FACTOR,\n",
    "                                       fixed_meta_learner_instance=global_fixed_meta_learner, current_test_indices=test_indices)\n",
    "\n",
    "    all_results.append(result)\n",
    "    plot_data = result.get('plot_data', [])\n",
    "    if plot_data:\n",
    "        all_plot_data_accumulated.extend(plot_data)\n",
    "  \n",
    "    # --- New: Accumulate MAE/RMSE data for each run ---\n",
    "    scores_data_for_current_seed = result.get('all_scores_data_for_seed_fold', [])\n",
    "    if scores_data_for_current_seed:\n",
    "        all_scores_accumulated.extend(scores_data_for_current_seed)\n",
    "\n",
    "    # --- Accumulate raw SHAP values and related data (for subsequent swarm plot aggregation) ---\n",
    "    if PLOT_SHAP_SWARM_PLOT and result.get('raw_shap_values') and result.get('X_val_data_for_shap'):\n",
    "        current_seed_raw_shaps = result['raw_shap_values']\n",
    "        current_seed_expected_vals = result['shap_expected_values']\n",
    "        # Note: X_val_data_for_shap is a list of X_val_fold DataFrames for each inner fold within this run\n",
    "        current_X_val_data_folds = result['X_val_data_for_shap']\n",
    "\n",
    "        for model_name in model_names_available:\n",
    "            if model_name in current_seed_raw_shaps:\n",
    "                for fold_idx_inner in range(len(current_seed_raw_shaps[model_name])):\n",
    "                    # Ensure we have corresponding X_val_data for the SHAP values\n",
    "                    if fold_idx_inner < len(current_X_val_data_folds):\n",
    "                        all_aggregated_shap_values_for_swarm_raw_data.append({\n",
    "                            'seed': run_idx,\n",
    "                            'fold_idx': fold_idx_inner, # This fold_idx_inner is always 0 in this script's current structure for outer loop\n",
    "                            'model_name': model_name,\n",
    "                            'shap_values': current_seed_raw_shaps[model_name][fold_idx_inner],\n",
    "                            'expected_value': current_seed_expected_vals[model_name][fold_idx_inner],\n",
    "                            'X_val_data': current_X_val_data_folds[fold_idx_inner]\n",
    "                        })\n",
    "\n",
    "\n",
    "    # --- Accumulate overall results (for final average printing) ---\n",
    "    current_stacking_rmse = result.get('stacking_regressor_rmse_mean', np.nan)\n",
    "    current_stacking_mae = result.get('stacking_regressor_mae_mean', np.nan)\n",
    "  \n",
    "    # Check if all key metrics are valid to decide whether to accumulate\n",
    "    if not np.isnan(current_stacking_rmse) and not np.isnan(current_stacking_mae):\n",
    "        stacking_rmse_sum += current_stacking_rmse\n",
    "        stacking_mae_sum += current_stacking_mae\n",
    "      \n",
    "        for name in model_names_available:\n",
    "             submodel_r2_sums[name] += result.get('submodel_r2_means', {}).get(name, np.nan)\n",
    "             submodel_rmse_sums[name] += result.get('submodel_rmse_means', {}).get(name, np.nan)\n",
    "             submodel_mae_sums[name] += result.get('submodel_mae_means', {}).get(name, np.nan)\n",
    "      \n",
    "        current_weighted_dict = result.get('weighted_feature_importances', {})\n",
    "        if current_weighted_dict:\n",
    "             current_weighted_series = pd.Series(current_weighted_dict)\n",
    "             aligned_importances = current_weighted_series.reindex(feature_names_list).fillna(0.0)\n",
    "             weighted_feature_importances_sums += aligned_importances\n",
    "\n",
    "        valid_runs_count += 1\n",
    "    else:\n",
    "         print(f\"Warning: Stacking Regressor results for Run {run_idx} are NaN, skipping accumulation.\")\n",
    "\n",
    "\n",
    "    # --- Print results for the current run (only MAE) ---\n",
    "    print(f\"\\n--- Run {run_idx} Results Summary ---\")\n",
    "    sr_mae_str = f\"{current_stacking_mae:.4f}\" if not np.isnan(current_stacking_mae) else \"N/A\"\n",
    "    print(f\"  Stacking Regressor MAE Mean: {sr_mae_str}\")\n",
    "    print(\"  Submodel MAE Means:\")\n",
    "    current_sub_mae = result.get('submodel_mae_means', {})\n",
    "    for name in model_names_available:\n",
    "        mae_val = current_sub_mae.get(name, np.nan)\n",
    "        mae_str = f\"{mae_val:.4f}\" if not np.isnan(mae_val) else \"N/A\"\n",
    "        print(f\"    {name}: MAE Mean = {mae_str}\")\n",
    "\n",
    "    # Print fixed meta-learner coefficients\n",
    "    if global_fixed_meta_coeffs_dict:\n",
    "        print(\"  Meta-Learner Coefficients (Fixed Global):\")\n",
    "        for name, coeff in global_fixed_meta_coeffs_dict.items():\n",
    "            print(f\"    {name}: {coeff:.4f}\")\n",
    "\n",
    "\n",
    "# --- Calculate Average Results Across All Runs ---\n",
    "if valid_runs_count > 0:\n",
    "    avg_stacking_regressor_rmse_mean = stacking_rmse_sum / valid_runs_count\n",
    "    avg_stacking_regressor_mae_mean = stacking_mae_sum / valid_runs_count\n",
    "    avg_submodel_r2_means = {name: total / valid_runs_count for name, total in submodel_r2_sums.items()}\n",
    "    avg_submodel_rmse_means = {name: total / valid_runs_count for name, total in submodel_rmse_sums.items()}\n",
    "    avg_submodel_mae_means = {name: total / valid_runs_count for name, total in submodel_mae_sums.items()}\n",
    "    avg_weighted_feature_importances_series = weighted_feature_importances_sums / valid_runs_count\n",
    "  \n",
    "    # Average meta-learner coefficients are now the fixed global coefficients\n",
    "    avg_meta_coefficients_final = global_fixed_meta_coeffs_dict\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Average Results Across All {valid_runs_count} Valid Runs ===\")\n",
    "    print(f\"Average Stacking Regressor RMSE Mean: {avg_stacking_regressor_rmse_mean:.4f}\")\n",
    "    print(f\"Average Stacking Regressor MAE Mean: {avg_stacking_regressor_mae_mean:.4f}\")\n",
    "    print(\"\\nAverage Submodel R2, RMSE and MAE Means:\")\n",
    "    for name in model_names_available:\n",
    "        r2_avg = avg_submodel_r2_means.get(name, np.nan)\n",
    "        rmse_avg = avg_submodel_rmse_means.get(name, np.nan)\n",
    "        mae_avg = avg_submodel_mae_means.get(name, np.nan)\n",
    "        r2_avg_str = f\"{r2_avg:.4f}\" if not np.isnan(r2_avg) else \"N/A\"\n",
    "        rmse_avg_str = f\"{rmse_avg:.4f}\" if not np.isnan(rmse_avg) else \"N/A\"\n",
    "        mae_avg_str = f\"{mae_avg:.4f}\" if not np.isnan(mae_avg) else \"N/A\"\n",
    "        print(f\"  {name}: R2 Mean = {r2_avg_str}, RMSE Mean = {rmse_avg_str}, MAE Mean = {mae_avg_str}\")\n",
    "\n",
    "    print(\"\\nFixed Global Meta-Learner Coefficients:\")\n",
    "    if avg_meta_coefficients_final:\n",
    "        for name, coeff in avg_meta_coefficients_final.items():\n",
    "            print(f\"  {name}: {coeff:.4f}\")\n",
    "    else:\n",
    "        print(\"  (Meta-learner coefficients not successfully fixed)\")\n",
    "\n",
    "    # --- Process and Output Feature Importances (Bar Plot) ---\n",
    "    avg_weighted_feature_importances_dict = avg_weighted_feature_importances_series.to_dict()\n",
    "    sorted_feature_importances = sorted(avg_weighted_feature_importances_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(\"\\nAverage Weighted Feature Importances (Sorted by SHAP):\")\n",
    "    for feature, importance in sorted_feature_importances:\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "    try:\n",
    "        features_to_plot = [item[0] for item in sorted_feature_importances[:N_FEATURES_TO_PLOT]]\n",
    "        importances_to_plot = [item[1] for item in sorted_feature_importances[:N_FEATURES_TO_PLOT]]\n",
    "        plt.figure(figsize=(10, max(6, N_FEATURES_TO_PLOT * 0.4)))\n",
    "        sns.barplot(x=importances_to_plot, y=features_to_plot, palette=\"viridis\")\n",
    "        plt.xlabel('Average Weighted Feature Importance (exp(-RMSE) Weight, From CV Runs, SHAP)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Top {N_FEATURES_TO_PLOT} Feature Importances Averaged Over {valid_runs_count} Runs (SHAP)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError plotting feature importance: {e}\")\n",
    "\n",
    "    # --- Plot Weighted Average Combined SHAP Swarm Plot ---\n",
    "    if PLOT_SHAP_SWARM_PLOT:\n",
    "        print(\"\\n--- Aggregating raw SHAP data and plotting weighted average combined SHAP swarm plot (exp(-RMSE) weighted) ---\")\n",
    "      \n",
    "        # 1. Calculate exp(-RMSE) weights\n",
    "        shap_exp_rmse_weights = {}\n",
    "        total_exp_rmse_sum_for_shap_weights = 0.0\n",
    "        for name in model_names_available:\n",
    "            rmse_val = avg_submodel_rmse_means.get(name, np.nan)\n",
    "            if not np.isnan(rmse_val):\n",
    "                weight = np.exp(-rmse_val)\n",
    "                shap_exp_rmse_weights[name] = weight\n",
    "                total_exp_rmse_sum_for_shap_weights += weight\n",
    "            else:\n",
    "                shap_exp_rmse_weights[name] = 0.0\n",
    "\n",
    "        # Normalize weights\n",
    "        if total_exp_rmse_sum_for_shap_weights > 1e-9:\n",
    "            for name in shap_exp_rmse_weights:\n",
    "                shap_exp_rmse_weights[name] /= total_exp_rmse_sum_for_shap_weights\n",
    "            print(f\"  exp(-RMSE) weighting coefficients (normalized): {shap_exp_rmse_weights}\")\n",
    "        else:\n",
    "            print(\"Warning: Average RMSE for all submodels resulted in invalid exp(-RMSE) weights. Skipping swarm plot.\")\n",
    "            PLOT_SHAP_SWARM_PLOT = False # Disable swarm plot\n",
    "          \n",
    "        if PLOT_SHAP_SWARM_PLOT: # Re-check if disabled\n",
    "            # 2. Aggregate weighted SHAP values and X data from all runs\n",
    "            final_shap_values_for_plot_list = []\n",
    "            final_X_data_for_plot_list = []\n",
    "            final_expected_values_for_plot_list = []\n",
    "\n",
    "            # Iterate through all collected raw SHAP data\n",
    "            # all_aggregated_shap_values_for_swarm_raw_data contains raw SHAP data for each model in each run.\n",
    "            # We need to group by (run_idx, fold_idx) and then weight models within each group.\n",
    "          \n",
    "            # First, group by (run_idx, fold_idx)\n",
    "            grouped_shap_data = {}\n",
    "            for item in all_aggregated_shap_values_for_swarm_raw_data:\n",
    "                key = (item['seed'], item['fold_idx'])\n",
    "                if key not in grouped_shap_data:\n",
    "                    grouped_shap_data[key] = {}\n",
    "                grouped_shap_data[key][item['model_name']] = {\n",
    "                    'shap_values': item['shap_values'],\n",
    "                    'expected_value': item['expected_value'],\n",
    "                    'X_val_data': item['X_val_data']\n",
    "                }\n",
    "\n",
    "            for (run_key, fold_key), models_data in grouped_shap_data.items():\n",
    "                # Ensure at least one model successfully calculated SHAP values, otherwise skip\n",
    "                first_valid_model_shap_found = False\n",
    "                for name in model_names_available:\n",
    "                    if name in models_data and models_data[name]['shap_values'].shape[0] > 0:\n",
    "                        weighted_shap_for_fold = np.zeros_like(models_data[name]['shap_values'])\n",
    "                        weighted_expected_value_for_fold = 0.0\n",
    "                        current_X_val_fold = models_data[name]['X_val_data']\n",
    "                        first_valid_model_shap_found = True\n",
    "                        break\n",
    "                if not first_valid_model_shap_found:\n",
    "                    print(f\"  Warning: Run/Fold {run_key}/{fold_key+1} has no valid SHAP data, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                valid_model_shaps_in_fold = 0\n",
    "                for name in model_names_available:\n",
    "                    if name in models_data:\n",
    "                        weight = shap_exp_rmse_weights.get(name, 0.0)\n",
    "                        shap_vals = models_data[name]['shap_values']\n",
    "                        exp_val = models_data[name]['expected_value']\n",
    "\n",
    "                        # Ensure shap_vals and weighted_shap_for_fold dimensions match\n",
    "                        if shap_vals.shape == weighted_shap_for_fold.shape:\n",
    "                            weighted_shap_for_fold += weight * shap_vals\n",
    "                            weighted_expected_value_for_fold += weight * exp_val\n",
    "                            valid_model_shaps_in_fold += 1\n",
    "                        else:\n",
    "                            print(f\"  Warning: Run/Fold {run_key}/{fold_key+1}, Model {name}'s SHAP value dimension mismatch, skipping weighting.\")\n",
    "              \n",
    "                if valid_model_shaps_in_fold > 0:\n",
    "                    final_shap_values_for_plot_list.append(weighted_shap_for_fold)\n",
    "                    final_X_data_for_plot_list.append(current_X_val_fold)\n",
    "                    # Each sample corresponds to this weighted expected value\n",
    "                    final_expected_values_for_plot_list.extend([weighted_expected_value_for_fold] * current_X_val_fold.shape[0])\n",
    "          \n",
    "            if not final_shap_values_for_plot_list:\n",
    "                print(\"Warning: No valid weighted SHAP data collected, cannot plot swarm plot.\")\n",
    "            else:\n",
    "                try:\n",
    "                    final_shap_values_for_plot = np.vstack(final_shap_values_for_plot_list)\n",
    "                    final_X_data_for_plot = pd.concat(final_X_data_for_plot_list)\n",
    "                  \n",
    "                    # Limit sample count to prevent memory issues or slow plotting\n",
    "                    if SHAP_SWARM_SAMPLES_LIMIT is not None and final_shap_values_for_plot.shape[0] > SHAP_SWARM_SAMPLES_LIMIT:\n",
    "                        print(f\"  Warning: Too many samples for SHAP swarm plot ({final_shap_values_for_plot.shape[0]}), randomly sampling {SHAP_SWARM_SAMPLES_LIMIT} samples.\")\n",
    "                        rng = np.random.default_rng(DEFAULT_MODEL_RANDOM_STATE)\n",
    "                        sample_indices = rng.choice(final_shap_values_for_plot.shape[0], SHAP_SWARM_SAMPLES_LIMIT, replace=False)\n",
    "                        final_shap_values_for_plot = final_shap_values_for_plot[sample_indices]\n",
    "                        final_X_data_for_plot = final_X_data_for_plot.iloc[sample_indices]\n",
    "                        final_expected_values_for_plot_sampled = np.array(final_expected_values_for_plot_list)[sample_indices]\n",
    "                        final_base_value = np.mean(final_expected_values_for_plot_sampled)\n",
    "                    else:\n",
    "                        final_base_value = np.mean(final_expected_values_for_plot_list)\n",
    "\n",
    "                    # --- New: Export aggregated swarm plot data (fourth Excel) ---\n",
    "                    print(\"\\n--- Exporting aggregated SHAP swarm plot data to Excel/CSV ---\")\n",
    "                    try:\n",
    "                        # Create DataFrame for SHAP values\n",
    "                        shap_values_df = pd.DataFrame(final_shap_values_for_plot, columns=[f'SHAP_{col}' for col in feature_names_list])\n",
    "\n",
    "                        # Combine original feature data and SHAP values\n",
    "                        # Ensure indices are aligned if X_data was a DataFrame\n",
    "                        combined_shap_data_df = pd.concat([final_X_data_for_plot.reset_index(drop=True), shap_values_df], axis=1)\n",
    "\n",
    "                        # Add the base value as a constant column\n",
    "                        combined_shap_data_df['SHAP_Base_Value'] = final_base_value\n",
    "\n",
    "                        # --- START MODIFICATION FOR SHAP EXCEL/CSV SORTING ---\n",
    "                        # Calculate mean absolute value for each SHAP feature\n",
    "                        shap_feature_columns = [col for col in combined_shap_data_df.columns if col.startswith('SHAP_')]\n",
    "                        if shap_feature_columns: # Ensure there are SHAP columns to sort\n",
    "                            mean_abs_shap_values = combined_shap_data_df[shap_feature_columns].abs().mean().sort_values(ascending=False)\n",
    "                            # Get sorted SHAP feature column names\n",
    "                            sorted_shap_columns = mean_abs_shap_values.index.tolist()\n",
    "\n",
    "                            # Get non-SHAP feature column names (e.g., Seed, Fold, Sample_Index, Actual, SHAP_Base_Value etc.)\n",
    "                            non_shap_columns = [col for col in combined_shap_data_df.columns if not col.startswith('SHAP_')]\n",
    "                            # Ensure SHAP_Base_Value is at the end, or any desired position\n",
    "                            if 'SHAP_Base_Value' in non_shap_columns:\n",
    "                                non_shap_columns.remove('SHAP_Base_Value')\n",
    "                                non_shap_columns.append('SHAP_Base_Value')\n",
    "\n",
    "                            # Construct final column order: non-SHAP columns + sorted SHAP columns\n",
    "                            final_column_order = non_shap_columns + sorted_shap_columns\n",
    "                            # Reorder DataFrame columns\n",
    "                            combined_shap_data_df = combined_shap_data_df[final_column_order]\n",
    "                        # --- END MODIFICATION FOR SHAP EXCEL/CSV SORTING ---\n",
    "\n",
    "                        excel_filename_shap_data = f'shap_swarm_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "                        combined_shap_data_df.to_excel(excel_filename_shap_data, index=False, engine='openpyxl')\n",
    "                        print(f\"Successfully exported aggregated SHAP swarm plot data to: {excel_filename_shap_data}\")\n",
    "\n",
    "                    except ImportError:\n",
    "                        print(\"!!! Exporting to Excel requires 'openpyxl' library. Attempting to export as CSV file...\")\n",
    "                        try:\n",
    "                            # Re-create combined_shap_data_df if needed for CSV fallback\n",
    "                            # This block is executed if openpyxl is not found, so combined_shap_data_df might not be fully processed yet\n",
    "                            if 'combined_shap_data_df' not in locals() or combined_shap_data_df.empty:\n",
    "                                shap_values_df = pd.DataFrame(final_shap_values_for_plot, columns=[f'SHAP_{col}' for col in feature_names_list])\n",
    "                                combined_shap_data_df = pd.concat([final_X_data_for_plot.reset_index(drop=True), shap_values_df], axis=1)\n",
    "                                combined_shap_data_df['SHAP_Base_Value'] = final_base_value\n",
    "\n",
    "                            # --- START MODIFICATION FOR SHAP CSV SORTING (DUPLICATE LOGIC) ---\n",
    "                            shap_feature_columns = [col for col in combined_shap_data_df.columns if col.startswith('SHAP_')]\n",
    "                            if shap_feature_columns: # Ensure there are SHAP columns to sort\n",
    "                                mean_abs_shap_values = combined_shap_data_df[shap_feature_columns].abs().mean().sort_values(ascending=False)\n",
    "                                sorted_shap_columns = mean_abs_shap_values.index.tolist()\n",
    "                                non_shap_columns = [col for col in combined_shap_data_df.columns if not col.startswith('SHAP_')]\n",
    "                                if 'SHAP_Base_Value' in non_shap_columns:\n",
    "                                    non_shap_columns.remove('SHAP_Base_Value')\n",
    "                                    non_shap_columns.append('SHAP_Base_Value')\n",
    "                                final_column_order = non_shap_columns + sorted_shap_columns\n",
    "                                combined_shap_data_df = combined_shap_data_df[final_column_order]\n",
    "                            # --- END MODIFICATION FOR SHAP CSV SORTING ---\n",
    "\n",
    "                            csv_filename_shap_data = f'shap_swarm_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "                            combined_shap_data_df.to_csv(csv_filename_shap_data, index=False)\n",
    "                            print(f\"Successfully exported SHAP swarm plot data as CSV: {csv_filename_shap_data}\")\n",
    "                        except Exception as e_csv:\n",
    "                            print(f\"!!! Error exporting SHAP swarm plot data to CSV: {e_csv}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"!!! Other error occurred while exporting aggregated SHAP swarm plot data to Excel: {e}\")\n",
    "                        print(\"Attempting to export SHAP swarm plot data as CSV file...\")\n",
    "                        try:\n",
    "                            if 'combined_shap_data_df' not in locals() or combined_shap_data_df.empty:\n",
    "                                shap_values_df = pd.DataFrame(final_shap_values_for_plot, columns=[f'SHAP_{col}' for col in feature_names_list])\n",
    "                                combined_shap_data_df = pd.concat([final_X_data_for_plot.reset_index(drop=True), shap_values_df], axis=1)\n",
    "                                combined_shap_data_df['SHAP_Base_Value'] = final_base_value\n",
    "\n",
    "                            # --- START MODIFICATION FOR SHAP CSV SORTING (DUPLICATE LOGIC) ---\n",
    "                            shap_feature_columns = [col for col in combined_shap_data_df.columns if col.startswith('SHAP_')]\n",
    "                            if shap_feature_columns: # Ensure there are SHAP columns to sort\n",
    "                                mean_abs_shap_values = combined_shap_data_df[shap_feature_columns].abs().mean().sort_values(ascending=False)\n",
    "                                sorted_shap_columns = mean_abs_shap_values.index.tolist()\n",
    "                                non_shap_columns = [col for col in combined_shap_data_df.columns if not col.startswith('SHAP_')]\n",
    "                                if 'SHAP_Base_Value' in non_shap_columns:\n",
    "                                    non_shap_columns.remove('SHAP_Base_Value')\n",
    "                                    non_shap_columns.append('SHAP_Base_Value')\n",
    "                                final_column_order = non_shap_columns + sorted_shap_columns\n",
    "                                combined_shap_data_df = combined_shap_data_df[final_column_order]\n",
    "                            # --- END MODIFICATION FOR SHAP CSV SORTING ---\n",
    "\n",
    "                            csv_filename_shap_data = f'shap_swarm_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "                            combined_shap_data_df.to_csv(csv_filename_shap_data, index=False)\n",
    "                            print(f\"Successfully exported SHAP swarm plot data as CSV: {csv_filename_shap_data}\")\n",
    "                        except Exception as e_csv:\n",
    "                            print(f\"!!! Error exporting SHAP swarm plot data to CSV: {e_csv}\")\n",
    "                    # --- End Export ---\n",
    "\n",
    "                    # Create shap.Explanation object\n",
    "                    shap_explanation = shap.Explanation(\n",
    "                        values=final_shap_values_for_plot,\n",
    "                        base_values=final_base_value,\n",
    "                        data=final_X_data_for_plot.values, # shap.Explanation expects numpy array\n",
    "                        feature_names=feature_names_list\n",
    "                    )\n",
    "\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    shap.summary_plot(shap_explanation, final_X_data_for_plot, plot_type=\"dot\", show=False) # plot_type=\"dot\" is the swarm plot\n",
    "                    plt.title(f'exp(-RMSE) Weighted Average SHAP Values for Stacking Model (Aggregated over {valid_runs_count} Runs)')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError plotting SHAP swarm plot: {e}\")\n",
    "        else:\n",
    "            print(\"\\nWarning: Swarm plot not generated due to invalid exp(-RMSE) weights.\")\n",
    "    else:\n",
    "        print(\"\\nWarning: PLOT_SHAP_SWARM_PLOT is set to False, skipping swarm plot generation.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nError: No runs completed successfully, cannot calculate average results or generate plots.\")\n",
    "\n",
    "\n",
    "# --- Export predictions in wide format (first Excel) ---\n",
    "print(\"\\n--- Exporting prediction results data to Excel/CSV (pivoted by model) ---\")\n",
    "if all_plot_data_accumulated:\n",
    "    try:\n",
    "        plot_data_df_for_pivot = pd.DataFrame(all_plot_data_accumulated)\n",
    "\n",
    "        if 'Sample_Index' not in plot_data_df_for_pivot.columns:\n",
    "            print(\"Error: 'Sample_Index' column is missing from plot data, cannot perform pivoted export. Please check evaluate_models_manual_cv function.\")\n",
    "            raise KeyError(\"'Sample_Index' not found in plot data\")\n",
    "\n",
    "        print(\"    Pivoting data to have models as columns...\")\n",
    "        # Note: The 'Fold' column is now unified to 0, so pivot only by Seed, Sample_Index, Actual\n",
    "        export_df_pivoted = pd.pivot_table(\n",
    "            plot_data_df_for_pivot,\n",
    "            index=['Seed', 'Sample_Index', 'Actual'], # <-- Removed Fold\n",
    "            columns=['Model', 'Set'],\n",
    "            values='Predicted'\n",
    "        )\n",
    "\n",
    "        export_df_pivoted = export_df_pivoted.reset_index()\n",
    "\n",
    "        new_columns = []\n",
    "        for col in export_df_pivoted.columns.values:\n",
    "            if isinstance(col, tuple):\n",
    "                flat_name = '_'.join(filter(None, col)).strip('_')\n",
    "                new_columns.append(flat_name + '_Pred')\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "        export_df_pivoted.columns = new_columns\n",
    "        print(\"    Data pivoting and column name cleaning complete.\")\n",
    "\n",
    "\n",
    "        excel_filename = f'cv_predictions_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "        export_df_pivoted.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Successfully exported pivoted data by model to: {excel_filename}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"!!! Exporting to Excel requires 'openpyxl' library. Please run 'pip install openpyxl'.\")\n",
    "        print(\"Attempting to export pivoted data by model as CSV file...\")\n",
    "        try:\n",
    "            if 'export_df_pivoted' not in locals():\n",
    "                 if 'plot_data_df_for_pivot' not in locals(): plot_data_df_for_pivot = pd.DataFrame(all_plot_data_accumulated)\n",
    "                 if 'Sample_Index' not in plot_data_df_for_pivot.columns: raise KeyError(\"Sample_Index missing\")\n",
    "                 export_df_pivoted = pd.pivot_table(plot_data_df_for_pivot, index=['Seed', 'Sample_Index', 'Actual'], columns=['Model', 'Set'], values='Predicted') # <-- Removed Fold\n",
    "                 export_df_pivoted = export_df_pivoted.reset_index()\n",
    "                 new_columns = ['_'.join(filter(None, col)).strip('_')+'_Pred' if isinstance(col, tuple) else col for col in export_df_pivoted.columns.values]\n",
    "                 export_df_pivoted.columns = new_columns\n",
    "\n",
    "            csv_filename = f'cv_predictions_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            export_df_pivoted.to_csv(csv_filename, index=False)\n",
    "            print(f\"Successfully exported data as CSV: {csv_filename}\")\n",
    "        except KeyError:\n",
    "             print(\"!!! Pivoting and CSV export failed due to missing Sample_Index.\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "\n",
    "    except KeyError as e_key:\n",
    "        print(f\"!!! Data pivoting failed due to missing required column: {e_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Other error occurred while exporting pivoted data by model to Excel: {e}\")\n",
    "        print(\"Attempting to export pivoted data by model as CSV file...\")\n",
    "        try:\n",
    "            if 'export_df_pivoted' not in locals():\n",
    "                 if 'plot_data_df_for_pivot' not in locals(): plot_data_df_for_pivot = pd.DataFrame(all_plot_data_accumulated)\n",
    "                 if 'Sample_Index' not in plot_data_df_for_pivot.columns: raise KeyError(\"Sample_Index missing\")\n",
    "                 export_df_pivoted = pd.pivot_table(plot_data_df_for_pivot, index=['Seed', 'Sample_Index', 'Actual'], columns=['Model', 'Set'], values='Predicted') # <-- Removed Fold\n",
    "                 export_df_pivoted = export_df_pivoted.reset_index()\n",
    "                 new_columns = ['_'.join(filter(None, col)).strip('_')+'_Pred' if isinstance(col, tuple) else col for col in export_df_pivoted.columns.values]\n",
    "                 export_df_pivoted.columns = new_columns\n",
    "\n",
    "            csv_filename = f'cv_predictions_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            export_df_pivoted.to_csv(csv_filename, index=False)\n",
    "            print(f\"Successfully exported data as CSV: {csv_filename}\")\n",
    "        except KeyError:\n",
    "             print(\"!!! Pivoting and CSV export failed due to missing Sample_Index.\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "else:\n",
    "    print(\"No data collected for plotting or export.\")\n",
    "\n",
    "# --- Export MAE/RMSE data per run (second Excel) ---\n",
    "print(\"\\n--- Exporting MAE and RMSE data per run to Excel/CSV (pivoted by model) ---\")\n",
    "if all_scores_accumulated:\n",
    "    try:\n",
    "        scores_df = pd.DataFrame(all_scores_accumulated)\n",
    "      \n",
    "        # First pivot: Convert 'Metric' column to columns\n",
    "        scores_df_temp = scores_df.pivot_table(\n",
    "            index=['Seed', 'Model'], # <-- Removed Fold\n",
    "            columns='Metric',\n",
    "            values='Value'\n",
    "        ).reset_index()\n",
    "      \n",
    "        # Second pivot: Convert 'Model' column to columns, and create new column names for MAE and RMSE\n",
    "        scores_df_pivoted = scores_df_temp.pivot_table(\n",
    "            index=['Seed'], # <-- Removed Fold\n",
    "            columns='Model',\n",
    "            values=['MAE', 'RMSE'] # <-- Select only MAE and RMSE\n",
    "        )\n",
    "      \n",
    "        # Clean multi-level column names, e.g., ('MAE', 'XGBR') becomes 'XGBR_MAE'\n",
    "        scores_df_pivoted.columns = [f'{col[1]}_{col[0]}' for col in scores_df_pivoted.columns]\n",
    "        scores_df_pivoted = scores_df_pivoted.reset_index()\n",
    "      \n",
    "        excel_filename_scores = f'cv_model_scores_per_run_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "        scores_df_pivoted.to_excel(excel_filename_scores, index=False, engine='openpyxl')\n",
    "        print(f\"Successfully exported MAE and RMSE data per run to: {excel_filename_scores}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"!!! Exporting to Excel requires 'openpyxl' library. Please run 'pip install openpyxl'。\")\n",
    "        print(\"Attempting to export MAE and RMSE data as CSV file...\")\n",
    "        try:\n",
    "            if 'scores_df_pivoted' not in locals():\n",
    "                scores_df = pd.DataFrame(all_scores_accumulated)\n",
    "                scores_df_temp = scores_df.pivot_table(\n",
    "                    index=['Seed', 'Model'], # <-- Removed Fold\n",
    "                    columns='Metric',\n",
    "                    values='Value'\n",
    "                ).reset_index()\n",
    "                scores_df_pivoted = scores_df_temp.pivot_table(\n",
    "                    index=['Seed'], # <-- Removed Fold\n",
    "                    columns='Model',\n",
    "                    values=['MAE', 'RMSE'] # <-- Select only MAE and RMSE\n",
    "                )\n",
    "                scores_df_pivoted.columns = [f'{col[1]}_{col[0]}' for col in scores_df_pivoted.columns]\n",
    "                scores_df_pivoted = scores_df_pivoted.reset_index()\n",
    "\n",
    "            csv_filename_scores = f'cv_model_scores_per_run_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            scores_df_pivoted.to_csv(csv_filename_scores, index=False)\n",
    "            print(f\"Successfully exported MAE and RMSE data as CSV: {csv_filename_scores}\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Other error occurred while exporting MAE and RMSE data per run to Excel: {e}\")\n",
    "        print(\"Attempting to export MAE and RMSE data as CSV file...\")\n",
    "        try:\n",
    "            if 'scores_df_pivoted' not in locals():\n",
    "                scores_df = pd.DataFrame(all_scores_accumulated)\n",
    "                scores_df_temp = scores_df.pivot_table(\n",
    "                    index=['Seed', 'Model'], # <-- Removed Fold\n",
    "                    columns='Metric',\n",
    "                    values='Value'\n",
    "                ).reset_index()\n",
    "                scores_df_pivoted = scores_df_temp.pivot_table(\n",
    "                    index=['Seed'], # <-- Removed Fold\n",
    "                    columns='Model',\n",
    "                    values=['MAE', 'RMSE'] # <-- Select only MAE and RMSE\n",
    "                )\n",
    "                scores_df_pivoted.columns = [f'{col[1]}_{col[0]}' for col in scores_df_pivoted.columns]\n",
    "                scores_df_pivoted = scores_df_pivoted.reset_index()\n",
    "\n",
    "            csv_filename_scores = f'cv_model_scores_per_run_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            scores_df_pivoted.to_csv(csv_filename_scores, index=False)\n",
    "            print(f\"Successfully exported MAE and RMSE data as CSV: {csv_filename_scores}\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "else:\n",
    "    print(\"No data collected for MAE and RMSE export.\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d89dd1-a04d-4225-b914-8b38daa03d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "\n",
      "--- Starting Prediction on Unknown Data ---\n",
      "All required training data and base model optimization results loaded successfully.\n",
      "\n",
      "--- Pre-trained meta-learner not found, starting meta-learner hyperparameter tuning and training. ---\n",
      "Initializing base models...\n",
      "  Successfully initialized XGBR with optimized parameters.\n",
      "  Successfully initialized RF with optimized parameters.\n",
      "  Successfully initialized GBRT with optimized parameters.\n",
      "  Successfully initialized ETR with optimized parameters.\n",
      "  Successfully initialized HGBR with optimized parameters.\n",
      "  Successfully initialized CBR with optimized parameters.\n",
      "  Successfully initialized LGBM with optimized parameters.\n",
      "\n",
      "=== Starting Meta-Learner Hyperparameter Tuning (using Leave-One-Out CV for OOF generation and internal CV) ===\n",
      "  Generating OOF predictions for model XGBR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model RF (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model GBRT (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model ETR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model HGBR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model CBR (for meta-learner tuning)...\n",
      "  Generating OOF predictions for model LGBM (for meta-learner tuning)...\n",
      "  Starting meta-learner Bayesian optimization...\n"
     ]
    }
   ],
   "source": [
    "# predict_unknown_data.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys # Used for sys.exit()\n",
    "import warnings\n",
    "\n",
    "# Model imports\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# Import StackingRegressor and meta-learner components\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import CatBoost and LightGBM (if available)\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: CatBoost is not installed. Skipping CBR model.\")\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: LightGBM is not installed. Skipping LGBM model.\")\n",
    "    lightgbm_available = False\n",
    "\n",
    "# Scikit-learn Utilities\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import LeaveOneOut # Used for meta-learner internal CV during re-training\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "### --- Adjustable Configuration Parameters --- ###\n",
    "\n",
    "# --- 1. Unknown Data File Path ---\n",
    "UNKNOWN_DATA_FILE = 'SR-yuce.xlsx' # Path to your Excel file containing unknown data for prediction.\n",
    "\n",
    "# --- 2. Prediction Results Output File Path ---\n",
    "OUTPUT_PREDICTIONS_FILE = f'predictions_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx' # Output path for the prediction results file.\n",
    "\n",
    "# --- 3. Default Random State (for model initialization, ensuring reproducibility) ---\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 # Ensures reproducibility of results across runs.\n",
    "\n",
    "# --- 4. Sub-model Selection Interface ---\n",
    "# List the names of the sub-models you wish to include in the Stacking model for prediction.\n",
    "# These models must have been successfully tuned in the previous optimization script.\n",
    "# Available model names typically include: 'XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM'\n",
    "# You can easily exclude a model from the stacking prediction by commenting out its entry in this list.\n",
    "ENABLED_SUBMODELS_LIST = [\n",
    "    'XGBR',\n",
    "    'RF',\n",
    "    'GBRT',\n",
    "    'ETR',\n",
    "    'HGBR',\n",
    "    'CBR',\n",
    "    'LGBM'\n",
    "]\n",
    "# Ensure models in ENABLED_SUBMODELS_LIST are actually available\n",
    "if 'CBR' in ENABLED_SUBMODELS_LIST and not catboost_available:\n",
    "    print(\"Warning: CatBoost is not installed. 'CBR' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('CBR')\n",
    "if 'LGBM' in ENABLED_SUBMODELS_LIST and not lightgbm_available:\n",
    "    print(\"Warning: LightGBM is not installed. 'LGBM' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('LGBM')\n",
    "\n",
    "# --- 5. Meta-Learner Bayesian Optimization Iterations (if re-training is needed) ---\n",
    "META_LEARNER_N_ITER_BAYESIAN = 50 # Number of Bayesian optimization iterations for the meta-learner.\n",
    "                                 # This is only used if the meta-learner is not found in memory and needs re-training.\n",
    "\n",
    "# --- 6. Noise Intensity Control Interface (if re-training is needed) ---\n",
    "NOISE_SCALE_FACTOR = 0     # Factor for data augmentation (Gaussian noise). Set to 0 to disable.\n",
    "                           # This parameter is used if the meta-learner needs to be re-trained within this script.\n",
    "                           # Adding noise to the training data can help regularize the model and\n",
    "                           # prevent overfitting, especially beneficial for small datasets.\n",
    "                           # By tuning this factor, one can potentially mitigate overfitting.\n",
    "                           # In this specific work, noise augmentation is not utilized (factor is 0).\n",
    "\n",
    "### --- End Configuration Parameters --- ###\n",
    "\n",
    "print(\"\\n--- Starting Prediction on Unknown Data ---\")\n",
    "\n",
    "# --- 0. Mandatory Check: Ensure training data X, Y and base model optimization results grid_searches are in memory ---\n",
    "# These variables are expected to be generated by the previous model tuning script in the same Python session.\n",
    "\n",
    "# Check if X (training features) exists\n",
    "if 'X' not in globals():\n",
    "    print(\"\\nFatal Error: Variable 'X' (training feature set) is not defined in the current environment.\")\n",
    "    print(\"Please ensure you have run the model tuning script in the same Python session.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    X = globals()['X'] # Retrieve X from the global scope\n",
    "\n",
    "# Check if Y (training target) exists\n",
    "if 'Y' not in globals():\n",
    "    print(\"\\nFatal Error: Variable 'Y' (training target set) is not defined in the current environment.\")\n",
    "    print(\"Please ensure you have run the model tuning script in the same Python session.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    Y = globals()['Y'] # Retrieve Y from the global scope\n",
    "\n",
    "# Check if grid_searches (base model optimization results) exists\n",
    "if 'grid_searches' not in globals():\n",
    "    print(\"\\nFatal Error: Variable 'grid_searches' (base model optimization results) is not defined in the current environment.\")\n",
    "    print(\"Please ensure you have run the model tuning script in the same Python session.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    grid_searches = globals()['grid_searches'] # Retrieve grid_searches from the global scope\n",
    "    if not grid_searches: # Check if grid_searches is an empty dictionary\n",
    "        print(\"\\nFatal Error: 'grid_searches' is empty. No base models were successfully tuned.\")\n",
    "        print(\"Please check the execution results of the model tuning script.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"All required training data and base model optimization results loaded successfully.\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def initialize_best_estimators(grid_searches_dict, enabled_models_list):\n",
    "    \"\"\"\n",
    "    Initializes best base learner instances from BayesSearchCV results, filtered by the enabled_models_list.\n",
    "    This function is used to retrieve the optimized base models for stacking.\n",
    "\n",
    "    Args:\n",
    "        grid_searches_dict (dict): A dictionary containing BayesSearchCV results for each model.\n",
    "        enabled_models_list (list): A list of model names that are enabled for stacking.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of initialized best estimator instances.\n",
    "    \"\"\"\n",
    "    estimators_init = {}\n",
    "    # Define all possible models and their default parameters\n",
    "    all_possible_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "\n",
    "    if catboost_available:\n",
    "        all_possible_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available:\n",
    "        all_possible_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing base models...\")\n",
    "    for name, (model_class, fixed_params) in all_possible_models.items():\n",
    "        if name not in enabled_models_list: # If the model is not in the enabled list, skip it\n",
    "            print(f\"  Model {name} is not in ENABLED_SUBMODELS_LIST, skipping initialization.\")\n",
    "            continue\n",
    "\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} with optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "        else:\n",
    "            print(f\"  No optimized results found for {name}. Attempting to initialize with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} with default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (with default parameters): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v is not None}\n",
    "    if not initialized_estimators:\n",
    "         print(\"Warning: No base models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "# --- Define Meta-Learner Hyperparameter Search Space (copied from evaluate_stacking_model.py) ---\n",
    "meta_learner_params = {\n",
    "    'elasticnet__alpha': Real(1e-5, 10.0, prior='log-uniform', name='elasticnet__alpha'),\n",
    "    'elasticnet__l1_ratio': Real(0.0, 1.0, prior='uniform', name='elasticnet__l1_ratio')\n",
    "}\n",
    "\n",
    "# --- Check and Retrieve or Train global_fixed_meta_learner ---\n",
    "global_fixed_meta_learner = None\n",
    "global_fixed_meta_coeffs_dict = None # Stores the final fixed coefficients for printing and inspection.\n",
    "\n",
    "# Attempt to load the pre-trained meta-learner from memory (if it was passed from the evaluation script)\n",
    "if 'global_fixed_meta_learner' in globals() and isinstance(globals()['global_fixed_meta_learner'], Pipeline):\n",
    "    global_fixed_meta_learner = globals()['global_fixed_meta_learner']\n",
    "    print(\"\\n--- Successfully loaded pre-trained meta-learner from memory (from evaluation script). ---\")\n",
    "    # Attempt to retrieve coefficients, if passed by the evaluation script\n",
    "    if 'global_fixed_meta_coeffs_dict' in globals():\n",
    "        global_fixed_meta_coeffs_dict = globals()['global_fixed_meta_coeffs_dict']\n",
    "        print(f\"  Loaded meta-learner coefficients: {global_fixed_meta_coeffs_dict}\")\n",
    "    else:\n",
    "        print(\"  Meta-learner coefficients not found, but the meta-learner itself was loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Pre-trained meta-learner not found, starting meta-learner hyperparameter tuning and training. ---\")\n",
    "    # Get available model names (filtered by ENABLED_SUBMODELS_LIST)\n",
    "    try:\n",
    "        temp_base_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "        model_names_available = list(temp_base_estimators.keys())\n",
    "        if not model_names_available:\n",
    "            print(\"Error: No models successfully initialized, cannot proceed with meta-learner training. Please check ENABLED_SUBMODELS_LIST.\")\n",
    "            sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred while trying to initialize models to get names: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Meta-Learner Hyperparameter Pre-tuning Stage (copied from evaluate_stacking_model.py) ---\n",
    "    print(f\"\\n=== Starting Meta-Learner Hyperparameter Tuning (using Leave-One-Out CV for OOF generation and internal CV) ===\")\n",
    "\n",
    "    # Generate OOF predictions for ALL N samples for this meta-tuning iteration\n",
    "    oof_preds_full_current_iter = np.zeros((len(X), len(model_names_available)))\n",
    "\n",
    "    # Use LeaveOneOut to generate OOF predictions on the entire dataset\n",
    "    loo_for_full_oof = LeaveOneOut()\n",
    "\n",
    "    is_X_dataframe = isinstance(X, pd.DataFrame)\n",
    "    original_columns = X.columns if is_X_dataframe else [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    if loo_for_full_oof.get_n_splits(X) < 2:\n",
    "        print(f\"Warning: LeaveOneOut cannot generate at least 2 folds with {len(X)} samples. Skipping meta-learner tuning.\")\n",
    "        global_fixed_meta_learner = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "        ])\n",
    "        global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "    else:\n",
    "        for i, (name, estimator_template) in enumerate(temp_base_estimators.items()):\n",
    "            print(f\"  Generating OOF predictions for model {name} (for meta-learner tuning)...\")\n",
    "            fold_preds = np.zeros(len(X))\n",
    "\n",
    "            # Generate OOF predictions using LeaveOneOut\n",
    "            for train_idx, val_idx in loo_for_full_oof.split(X, Y):\n",
    "                X_inner_train = X.iloc[train_idx] if is_X_dataframe else X[train_idx]\n",
    "                Y_inner_train = Y.iloc[train_idx] if isinstance(Y, pd.Series) else Y[train_idx]\n",
    "                X_inner_val = X.iloc[val_idx] if is_X_dataframe else X[val_idx]\n",
    "\n",
    "                estimator_clone = clone(estimator_template)\n",
    "                # Apply noise to X_inner_train if NOISE_SCALE_FACTOR > 0\n",
    "                X_inner_train_augmented_for_oof_meta_tune = X_inner_train\n",
    "                if NOISE_SCALE_FACTOR > 0:\n",
    "                    # Apply Gaussian noise to the training features for data augmentation.\n",
    "                    # This helps in regularizing the model and improving its generalization\n",
    "                    # by making it robust to small perturbations in the input data.\n",
    "                    X_inner_train_np_for_oof_meta_tune = X_inner_train.values if isinstance(X_inner_train, pd.DataFrame) else X_inner_train\n",
    "                    feature_std_devs_for_oof_meta_tune = np.std(X_inner_train_np_for_oof_meta_tune, axis=0)\n",
    "                    noise_std_devs_for_oof_meta_tune = feature_std_devs_for_oof_meta_tune * NOISE_SCALE_FACTOR + 1e-9\n",
    "                    noise_for_oof_meta_tune = np.random.normal(loc=0.0, scale=noise_std_devs_for_oof_meta_tune, size=X_inner_train_np_for_oof_meta_tune.shape)\n",
    "                    if isinstance(X_inner_train, pd.DataFrame):\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = pd.DataFrame(X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune, index=X_inner_train.index, columns=X_inner_train.columns)\n",
    "                    else:\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune\n",
    "\n",
    "                estimator_clone.fit(X_inner_train_augmented_for_oof_meta_tune, Y_inner_train) # Use augmented data for training\n",
    "                fold_preds[val_idx] = estimator_clone.predict(X_inner_val) # Predict on original (non-augmented) inner_val_set\n",
    "            oof_preds_full_current_iter[:, i] = fold_preds\n",
    "\n",
    "        if np.all(oof_preds_full_current_iter == 0) or np.any(np.isnan(oof_preds_full_current_iter)):\n",
    "            print(\"Warning: Generated OOF predictions are all zeros or contain NaN, skipping meta-learner tuning.\")\n",
    "            global_fixed_meta_learner = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "            ])\n",
    "            global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "        else:\n",
    "            print(\"  Starting meta-learner Bayesian optimization...\")\n",
    "            meta_bayes_search = BayesSearchCV(\n",
    "                estimator=Pipeline([('scaler', StandardScaler()), ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))]),\n",
    "                search_spaces=meta_learner_params,\n",
    "                n_iter=META_LEARNER_N_ITER_BAYESIAN,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                cv=LeaveOneOut(), # Meta-learner internal CV always uses LeaveOneOut.\n",
    "                n_jobs=-1,\n",
    "                random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                verbose=0\n",
    "            )\n",
    "            try:\n",
    "                meta_bayes_search.fit(oof_preds_full_current_iter, Y) # Use full OOF and full Y\n",
    "                print(f\"  Meta-learner Best Score (RMSE): {np.sqrt(-meta_bayes_search.best_score_):.4f}\")\n",
    "                print(f\"  Meta-learner Best Parameters: {dict(meta_bayes_search.best_params_)}\")\n",
    "\n",
    "                # Initialize and train the final meta-learner with optimized parameters, then fix its coefficients.\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(\n",
    "                        alpha=meta_bayes_search.best_params_.get('elasticnet__alpha', 1.0),\n",
    "                        l1_ratio=meta_bayes_search.best_params_.get('elasticnet__l1_ratio', 0.5),\n",
    "                        random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                        max_iter=2000\n",
    "                    ))\n",
    "                ])\n",
    "                # Train once on the entire OOF dataset to fix coefficients\n",
    "                global_fixed_meta_learner.fit(oof_preds_full_current_iter, Y)\n",
    "\n",
    "                # Extract and store fixed coefficients\n",
    "                elastic_net_model = global_fixed_meta_learner.named_steps['elasticnet']\n",
    "                global_fixed_meta_coeffs_dict = dict(zip(model_names_available, elastic_net_model.coef_))\n",
    "\n",
    "                print(\"Global best meta-learner initialized with optimized parameters and fixed coefficients.\")\n",
    "                print(f\"Fixed Meta-Learner Coefficients: {global_fixed_meta_coeffs_dict}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Meta-learner tuning or final training failed: {e}\")\n",
    "                print(\"Using default meta-learner parameters, and coefficients cannot be fixed.\")\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "                ])\n",
    "                global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available} # Indicate failure for coeffs\n",
    "\n",
    "print(\"\\n--- Meta-learner preparation complete ---\")\n",
    "\n",
    "\n",
    "# --- Main Prediction Flow ---\n",
    "print(f\"\\n--- Loading unknown data file: {UNKNOWN_DATA_FILE} ---\")\n",
    "try:\n",
    "    # Load the entire Excel file\n",
    "    full_unknown_data = pd.read_excel(UNKNOWN_DATA_FILE)\n",
    "\n",
    "    # Assume the first column contains sample names and save it\n",
    "    # Check column count to ensure a first column exists\n",
    "    if full_unknown_data.shape[1] > 0:\n",
    "        sample_names = full_unknown_data.iloc[:, 0]\n",
    "        # Feature data for model prediction, starting from the second column\n",
    "        unknown_data_features = full_unknown_data.iloc[:, 1:]\n",
    "        print(f\"Identified sample name column: '{full_unknown_data.columns[0]}'\")\n",
    "    else:\n",
    "        print(\"Warning: Unknown data file appears to have no columns. Sample names cannot be extracted.\")\n",
    "        sample_names = pd.Series(range(len(full_unknown_data)), name='Sample_Index')\n",
    "        unknown_data_features = full_unknown_data.copy() # If no columns, use an empty DataFrame directly\n",
    "\n",
    "    # Ensure all feature columns are numeric, coercing non-numeric to NaN\n",
    "    unknown_data_features = unknown_data_features.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill NaN values using the median from the training set (if training set had NaN filling)\n",
    "    # This assumes training set X has already been NaN-filled and its column medians are available.\n",
    "    if isinstance(X, pd.DataFrame) and X.isnull().any().any(): # Check if training set X has NaN\n",
    "        for col in X.columns:\n",
    "            if col in unknown_data_features.columns and unknown_data_features[col].isnull().any().any():\n",
    "                median_val = X[col].median()\n",
    "                if pd.isna(median_val): # If the median for that training feature is also NaN, fill with 0\n",
    "                    warnings.warn(f\"Warning: Median for training feature '{col}' is NaN, NaN values in unknown data will be filled with 0.\")\n",
    "                    unknown_data_features[col] = unknown_data_features[col].fillna(0)\n",
    "                else:\n",
    "                    unknown_data_features[col] = unknown_data_features[col].fillna(median_val)\n",
    "    print(f\"Successfully loaded {unknown_data_features.shape[0]} unknown data samples.\")\n",
    "    print(\"NaN values in unknown data filled using the median of corresponding training features (or 0).\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fatal Error: Unknown data file '{UNKNOWN_DATA_FILE}' not found. Please check the file path.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error: An error occurred while loading or processing unknown data: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Ensure unknown data feature columns are consistent with training data\n",
    "# Prioritizing training data column order and names\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    training_features = X.columns.tolist()\n",
    "    # Check if unknown data contains all training features\n",
    "    missing_features = [f for f in training_features if f not in unknown_data_features.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: Unknown data is missing the following features from the training set: {missing_features}. They will be filled with 0.\")\n",
    "        for mf in missing_features:\n",
    "            unknown_data_features[mf] = 0 # Fill missing features with 0\n",
    "\n",
    "    # Ensure unknown data column order matches training data\n",
    "    unknown_data_aligned = unknown_data_features[training_features]\n",
    "else:\n",
    "    # If X is not a DataFrame, assume unknown data column order is identical.\n",
    "    print(\"Warning: Training features X is not a Pandas DataFrame, cannot align unknown data columns by name. Assuming column order is identical.\")\n",
    "    unknown_data_aligned = unknown_data_features.values # Convert to numpy array\n",
    "\n",
    "# Get available model names (filtered by ENABLED_SUBMODELS_LIST)\n",
    "try:\n",
    "    base_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "    model_names_available_for_prediction = list(base_estimators.keys()) # Using a different variable name to avoid confusion\n",
    "    if not model_names_available_for_prediction:\n",
    "        print(\"Error: No base models successfully initialized, cannot proceed with prediction. Please check ENABLED_SUBMODELS_LIST.\")\n",
    "        sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while trying to initialize base models: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Generate Base Model Predictions ---\n",
    "print(\"\\n--- Generating Base Model Predictions ---\")\n",
    "base_model_predictions = pd.DataFrame(index=unknown_data_aligned.index)\n",
    "\n",
    "for name, estimator in base_estimators.items():\n",
    "    print(f\"  Using model {name} for prediction...\")\n",
    "    try:\n",
    "        # For default models that haven't been trained, training is required here.\n",
    "        # A simple check if already trained (e.g., has 'n_features_in_' or 'coef_')\n",
    "        if not hasattr(estimator, 'n_features_in_') and not hasattr(estimator, 'coef_'):\n",
    "            print(f\"  Warning: Model {name} appears untrained, training with training data X, Y...\")\n",
    "            try:\n",
    "                estimator.fit(X, Y) # Train with the original training data\n",
    "                print(f\"  Model {name} training complete.\")\n",
    "            except Exception as fit_e:\n",
    "                print(f\"  Error: Model {name} training failed: {fit_e}. This model will be skipped.\")\n",
    "                base_model_predictions[name] = np.nan\n",
    "                continue\n",
    "\n",
    "        preds = estimator.predict(unknown_data_aligned)\n",
    "        base_model_predictions[name] = preds\n",
    "        print(f\"  Model {name} prediction complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: An error occurred during prediction with model {name}: {e}. Predictions for this model will be NaN.\")\n",
    "        base_model_predictions[name] = np.nan\n",
    "\n",
    "if base_model_predictions.isnull().all().all():\n",
    "    print(\"Fatal Error: All base model predictions failed or are empty. Cannot proceed with Stacking prediction.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Generate Stacking Model Predictions ---\n",
    "print(\"\\n--- Generating Stacking Model Predictions ---\")\n",
    "\n",
    "# Ensure global_fixed_meta_learner exists and is valid\n",
    "if global_fixed_meta_learner is None:\n",
    "    print(\"Fatal Error: Meta-learner not successfully loaded or trained. Cannot proceed with Stacking prediction.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    stacking_predictions = global_fixed_meta_learner.predict(base_model_predictions.values)\n",
    "    base_model_predictions['Stacking_Prediction'] = stacking_predictions\n",
    "    print(\"Stacking model prediction complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error: An error occurred during Stacking model prediction: {e}.\")\n",
    "    print(\"Please check if global_fixed_meta_learner has been correctly trained and passed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Output Prediction Results ---\n",
    "print(f\"\\n--- Saving prediction results to file: {OUTPUT_PREDICTIONS_FILE} ---\")\n",
    "try:\n",
    "    # Merge original unknown data (including sample names) with prediction results\n",
    "    # Using full_unknown_data instead of unknown_data_features, as it contains the original first column.\n",
    "    final_output_df = full_unknown_data.copy()\n",
    "    final_output_df['Stacking_Prediction'] = base_model_predictions['Stacking_Prediction']\n",
    "    # Base model predictions can also be added.\n",
    "    for col in base_model_predictions.columns:\n",
    "        if col != 'Stacking_Prediction':\n",
    "            final_output_df[f'{col}_Prediction'] = base_model_predictions[col]\n",
    "\n",
    "    final_output_df.to_excel(OUTPUT_PREDICTIONS_FILE, index=False, engine='openpyxl')\n",
    "    print(f\"Prediction results successfully saved to: {OUTPUT_PREDICTIONS_FILE}\")\n",
    "except ImportError:\n",
    "    print(\"!!! Exporting to Excel requires 'openpyxl' library. Please run 'pip install openpyxl'.\")\n",
    "    print(\"Attempting to export as CSV file...\")\n",
    "    try:\n",
    "        final_output_df.to_csv(OUTPUT_PREDICTIONS_FILE.replace('.xlsx', '.csv'), index=False)\n",
    "        print(f\"Successfully exported prediction results as CSV: {OUTPUT_PREDICTIONS_FILE.replace('.xlsx', '.csv')}\")\n",
    "    except Exception as e_csv:\n",
    "        print(f\"!!! Error exporting to CSV as well: {e_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! Other error occurred while exporting prediction results to file: {e}\")\n",
    "\n",
    "print(\"\\nPrediction script execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a477a-01a9-4883-be43-de3c78adc6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matsci-ai)",
   "language": "python",
   "name": "matsci-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
