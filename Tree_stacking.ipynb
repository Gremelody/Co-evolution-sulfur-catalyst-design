{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f9132",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimize_base_learners.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Model imports\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: CatBoost is not installed. Skipping CBR model.\")\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: LightGBM is not installed. Skipping LGBM model.\")\n",
    "    lightgbm_available = False\n",
    "\n",
    "# Scikit-learn and Scikit-optimize tool imports\n",
    "from sklearn.model_selection import LeaveOneOut, LeavePOut\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.base import clone\n",
    "\n",
    "print(\"All necessary libraries imported successfully.\")\n",
    "\n",
    "### --- Adjustable Configuration Parameters --- ###\n",
    "\n",
    "# --- 1. Data Loading Settings ---\n",
    "EXCEL_FILE_PATH = 'Band alignment-feature engineeringed.xlsx' # <--- Please replace this path with your actual Excel file path.\n",
    "\n",
    "# --- 2. Bayesian Optimization Iterations ---\n",
    "N_ITER_BAYESIAN = 50 # Number of Bayesian optimization iterations for each model. Higher values lead to\n",
    "                     # a more thorough search but increase computation time.\n",
    "\n",
    "# --- 3. Default Random State (for all model initializations and Bayesian optimization itself) ---\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 # Ensures reproducibility of results across runs.\n",
    "\n",
    "# --- 4. P-value for Cross-Validation (Leave-P-Out) ---\n",
    "P_VALUE_FOR_BASE_TUNING = 1 # Set to 1 for Leave-One-Out Cross-Validation (LOOCV).\n",
    "                           # For small datasets, LOOCV provides a robust estimate of model performance\n",
    "                           # by training on N-1 samples and testing on 1, repeated N times.\n",
    "\n",
    "# --- 5. Sub-model Selection Interface ---\n",
    "# List the names of the sub-models for which you want to perform hyperparameter tuning.\n",
    "# If a model's name is not present in this list, it will be excluded from the tuning process\n",
    "# and subsequent stacking. You can easily disable a model by commenting out its entry in this list.\n",
    "# Available model names typically include: 'XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM'\n",
    "ENABLED_SUBMODELS_FOR_TUNING_LIST = [\n",
    "    'XGBR',\n",
    "    'RF',\n",
    "    'GBRT',\n",
    "    'ETR',\n",
    "    'HGBR',\n",
    "    'CBR',\n",
    "    'LGBM' \n",
    "]\n",
    "\n",
    "# Ensure models in ENABLED_SUBMODELS_FOR_TUNING_LIST are actually available\n",
    "if 'CBR' in ENABLED_SUBMODELS_FOR_TUNING_LIST and not catboost_available:\n",
    "    print(\"Warning: CatBoost is not installed. 'CBR' removed from ENABLED_SUBMODELS_FOR_TUNING_LIST.\")\n",
    "    ENABLED_SUBMODELS_FOR_TUNING_LIST.remove('CBR')\n",
    "if 'LGBM' in ENABLED_SUBMODELS_FOR_TUNING_LIST and not lightgbm_available:\n",
    "    print(\"Warning: LightGBM is not installed. 'LGBM' removed from ENABLED_SUBMODELS_FOR_TUNING_LIST.\")\n",
    "    ENABLED_SUBMODELS_FOR_TUNING_LIST.remove('LGBM')\n",
    "\n",
    "# Check if at least one model is enabled\n",
    "if not ENABLED_SUBMODELS_FOR_TUNING_LIST:\n",
    "    print(\"Error: No models enabled for hyperparameter tuning. Please check ENABLED_SUBMODELS_FOR_TUNING_LIST.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 6. Hyperparameter Search Space Definitions for Each Model ---\n",
    "# These parameter ranges are set to be generally suitable, with adjustments for potentially small sample sizes\n",
    "# when using Leave-P-Out Cross-Validation (e.g., P_VALUE_FOR_BASE_TUNING = 1).\n",
    "parameter_XGBR = {\n",
    "    'n_estimators': Integer(1, 500), # Number of boosting rounds (trees). Controls model complexity.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # Step size shrinkage used in update to prevent overfitting. Controls the contribution of each tree.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of a tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'subsample': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of the training instance. Prevents overfitting by sampling data.\n",
    "    'colsample_bytree': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of columns when constructing each tree. Prevents overfitting by sampling features.\n",
    "    'gamma': Real(0, 0.5, prior='uniform'), # Minimum loss reduction required to make a further partition on a leaf node. Higher gamma leads to more conservative models.\n",
    "    'reg_alpha': Real(1e-3, 0.5, prior='log-uniform'), # L1 regularization term on weights. Encourages sparsity.\n",
    "    'reg_lambda': Real(0.1, 5.0, prior='log-uniform') # L2 regularization term on weights. Prevents large weights.\n",
    "}\n",
    "parameter_RF = {\n",
    "    'n_estimators': Integer(1, 500), # Number of trees in the forest. More trees generally improve performance but increase computation.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'max_features': Categorical(['sqrt', 'log2']), # The number of features to consider when looking for the best split. 'sqrt' is common for regression.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node. For small datasets, a slightly higher minimum (e.g., 1-3) can help prevent overfitting.\n",
    "    'min_samples_split': Integer(2, 4) # The minimum number of samples required to split an internal node. For very small datasets, ensure this value is not too high to allow splits (e.g., 2 is the minimum).\n",
    "}\n",
    "parameter_CBR = {\n",
    "    'iterations': Integer(1, 500), # The maximum number of trees to build.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # The learning rate. Controls the step size of gradient descent.\n",
    "    'depth': Integer(1, 2), # Depth of the tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'l2_leaf_reg': Real(1, 5, prior='log-uniform'), # L2 regularization coefficient. Prevents overfitting by penalizing large weights.\n",
    "    'rsm': Real(0.5, 1.0, prior='uniform'), # The sampling rate for columns (feature subsampling).\n",
    "    'bootstrap_type': Categorical(['No']) # Defines the bootstrap type. 'No' disables internal sampling, which can be beneficial for very small datasets to ensure all samples are used.\n",
    "}\n",
    "parameter_LGBM = {\n",
    "    'n_estimators': Integer(1, 500), # Number of boosting rounds.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # Boosting learning rate.\n",
    "    'max_depth': Integer(1, 2), # Maximum tree depth for base learners. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'num_leaves': Integer(2, 4), # Maximum number of leaves in one tree. Should be related to `max_depth` (e.g., `num_leaves <= 2^max_depth`). Adjusted for small datasets.\n",
    "    'subsample': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of the training instance.\n",
    "    'colsample_bytree': Real(0.5, 1.0, prior='uniform'), # Subsample ratio of columns when constructing each tree.\n",
    "    'reg_alpha': Real(1e-3, 0.5, prior='log-uniform'), # L1 regularization term on weights.\n",
    "    'reg_lambda': Real(1e-3, 0.5, prior='log-uniform') # L2 regularization term on weights.\n",
    "}\n",
    "parameter_GBRT = {\n",
    "    'n_estimators': Integer(1, 500), # The number of boosting stages to perform.\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # Learning rate shrinks the contribution of each tree.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the individual regression estimators. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'max_features': Categorical(['sqrt', 'log2']), # The number of features to consider when looking for the best split.\n",
    "    'min_samples_split': Integer(2, 4), # The minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node.\n",
    "    'subsample': Real(0.5, 1.0, prior='uniform') # The fraction of samples to be used for fitting the individual base learners.\n",
    "}\n",
    "parameter_HGBR = {\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), # The learning rate.\n",
    "    'max_iter': Integer(1, 500), # The maximum number of iterations (boosting rounds). Corresponds to n_estimators.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the individual regression estimators. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node.\n",
    "    'l2_regularization': Real(1e-6, 5.0, prior='log-uniform') # The L2 regularization term on weights.\n",
    "}\n",
    "parameter_ETR = {\n",
    "    'n_estimators': Integer(1, 500), # The number of trees in the forest.\n",
    "    'max_depth': Integer(1, 2), # Maximum depth of the tree. A smaller depth (e.g., 1-2) is often preferred for small datasets to prevent overfitting.\n",
    "    'min_samples_split': Integer(2, 4), # The minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf': Integer(1, 3), # The minimum number of samples required to be at a leaf node.\n",
    "    'max_features': Categorical(['sqrt', 'log2', 1.0]) # The number of features to consider when looking for the best split. '1.0' means using all features.\n",
    "}\n",
    "\n",
    "### --- End Configuration Parameters --- ###\n",
    "\n",
    "print(\"All configuration parameters and Bayesian optimization search spaces defined.\")\n",
    "\n",
    "# --- 2. Data Loading and Inspection ---\n",
    "print(f\"\\nLoading data from '{EXCEL_FILE_PATH}'...\")\n",
    "try:\n",
    "    data = pd.read_excel(EXCEL_FILE_PATH)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{EXCEL_FILE_PATH}' not found. Please check the file path.\")\n",
    "    sys.exit(1) # Exit the program\n",
    "\n",
    "# --- User-defined Data Splitting ---\n",
    "# X: Features are from the second column (index 1) up to the second-to-last column (exclusive, index -1)\n",
    "X = data.iloc[:, 1:-1]\n",
    "# Y: Target variable is the last column (index -1)\n",
    "# Note: Adjust this index based on your specific target column.\n",
    "# For example:\n",
    "# If 'band alignment' is the second-to-last column: Y = data.iloc[:, -2]\n",
    "# If 'shift range' is the very last column: Y = data.iloc[:, -1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "target_column_name = Y.name if Y.name is not None else \"Unnamed_Target_Column\" # Get column name, assign default if none\n",
    "\n",
    "# Dynamically describe the extracted feature column range\n",
    "feature_col_names = X.columns.tolist()\n",
    "if len(feature_col_names) > 1:\n",
    "    feature_range_str = f\"from column '{feature_col_names[0]}' to '{feature_col_names[-1]}'\"\n",
    "elif len(feature_col_names) == 1:\n",
    "    feature_range_str = f\"column '{feature_col_names[0]}'\"\n",
    "else:\n",
    "    feature_range_str = \"with no features\" # Case where feature_col_names is empty\n",
    "\n",
    "print(f\"\\nData split using specified method: Features {feature_range_str} and target '{target_column_name}'.\")\n",
    "print(f\"X shape={X.shape}, Y shape={Y.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\nForcing data conversion to numeric types and handling non-numeric entries...\")\n",
    "initial_X_nan_count = X.isnull().sum().sum()\n",
    "initial_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "Y = Y.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "new_X_nan_count = X.isnull().sum().sum()\n",
    "new_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "if new_X_nan_count > initial_X_nan_count:\n",
    "    print(f\"Warning: {new_X_nan_count - initial_X_nan_count} non-numeric entries in feature data X were converted to NaN.\")\n",
    "if new_Y_nan_count > initial_Y_nan_count:\n",
    "    print(f\"Warning: {new_Y_nan_count - initial_Y_nan_count} non-numeric entries in target data Y were converted to NaN.\")\n",
    "\n",
    "print(\"\\nChecking and filling NaN values in data...\")\n",
    "if X.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in feature data X. Filling with median.\")\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"NaN values in X filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in feature data X.\")\n",
    "\n",
    "if Y.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in target data Y. Filling with median.\")\n",
    "    y_median_val = Y.median()\n",
    "    if pd.isna(y_median_val): # If the median itself is NaN, it means all Y values are NaN\n",
    "        print(\"Error: All values in target data Y column are NaN. Cannot perform effective filling or model training. Please check the raw data.\")\n",
    "        sys.exit(1) # Exit the script\n",
    "    Y = Y.fillna(y_median_val)\n",
    "    print(\"NaN values in Y filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in target data Y.\")\n",
    "\n",
    "# Check if the number of samples is compatible with Leave-P-Out\n",
    "if X.shape[0] <= P_VALUE_FOR_BASE_TUNING:\n",
    "    print(f\"Error: Dataset has too few samples ({X.shape[0]}) for Leave-{P_VALUE_FOR_BASE_TUNING}-Out CV. At least {P_VALUE_FOR_BASE_TUNING + 1} samples are required.\")\n",
    "    sys.exit(1) # Exit the program\n",
    "\n",
    "print(f\"Final data shape for analysis: X={X.shape}, Y={Y.shape}\")\n",
    "print(\"Note: This full dataset will be used for cross-validation tuning, no separate test set is created here.\")\n",
    "\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    feature_names_for_init = X.columns.tolist()\n",
    "    print(\"X is a Pandas DataFrame, its column names will be used.\")\n",
    "else:\n",
    "    print(\"Warning: Input X is not a Pandas DataFrame. Feature importances will use default names 'Feature_i'.\")\n",
    "    feature_names_for_init = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# --- 3. Cross-Validation Setup (using LeavePOut) ---\n",
    "cross_Valid = LeavePOut(p=P_VALUE_FOR_BASE_TUNING)\n",
    "n_splits_calculated = cross_Valid.get_n_splits(X)\n",
    "print(f\"\\nUsing Leave-{P_VALUE_FOR_BASE_TUNING}-Out Cross-Validation ({n_splits_calculated} folds) for hyperparameter tuning on the ENTIRE dataset.\")\n",
    "print(f\"Each training set size is {X.shape[0] - P_VALUE_FOR_BASE_TUNING} samples, and test set size is {P_VALUE_FOR_BASE_TUNING} samples.\")\n",
    "print(\"Note: Leave-P-Out CV provides more evaluation points for small sample sizes, helping the Bayesian optimizer find more stable hyperparameters.\")\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "def initialize_best_estimators(grid_searches_dict, enabled_models_list):\n",
    "    \"\"\"\n",
    "    Initializes best base learner instances from BayesSearchCV results, filtered by the enabled_models_list.\n",
    "    This function is primarily intended for use in evaluation or prediction scripts.\n",
    "\n",
    "    Args:\n",
    "        grid_searches_dict (dict): A dictionary containing BayesSearchCV results for each model.\n",
    "        enabled_models_list (list): A list of model names that are enabled for tuning.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of initialized best estimator instances.\n",
    "    \"\"\"\n",
    "    estimators_init = {}\n",
    "    # Define all possible models and their default parameters\n",
    "    all_possible_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "\n",
    "    if catboost_available:\n",
    "        all_possible_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available:\n",
    "        all_possible_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "    for name, (model_class, fixed_params) in all_possible_models.items():\n",
    "        if name not in enabled_models_list: # If the model is not in the enabled list, skip it\n",
    "            print(f\"  Model {name} is not in ENABLED_SUBMODELS_FOR_TUNING_LIST, skipping initialization.\")\n",
    "            continue\n",
    "\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} with optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "        else:\n",
    "            print(f\"  No optimized results found for {name}. Attempting to initialize with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} with default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (with default parameters): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v is not None}\n",
    "    if not initialized_estimators:\n",
    "         print(\"Warning: No models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "# --- 4. Dynamically Define Models and Parameter Mappings (based on ENABLED_SUBMODELS_FOR_TUNING_LIST) ---\n",
    "# Stores all possible base model instances with their default parameters\n",
    "all_possible_estimators_base = {\n",
    "    'XGBR': XGB.XGBRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, objective='reg:squarederror'),\n",
    "    'RF': RandomForestRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'GBRT': GradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'HGBR': HistGradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'ETR': ExtraTreesRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "}\n",
    "if catboost_available: all_possible_estimators_base['CBR'] = CatBoostRegressor(verbose=False, random_state=DEFAULT_MODEL_RANDOM_STATE, allow_writing_files=False)\n",
    "if lightgbm_available: all_possible_estimators_base['LGBM'] = LGBMRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, verbosity=-1, objective='regression')\n",
    "\n",
    "# Stores all possible parameter search spaces\n",
    "all_possible_params_base = {\n",
    "    'XGBR': parameter_XGBR,\n",
    "    'RF': parameter_RF,\n",
    "    'GBRT': parameter_GBRT,\n",
    "    'HGBR': parameter_HGBR,\n",
    "    'ETR': parameter_ETR,\n",
    "    'CBR': parameter_CBR,\n",
    "    'LGBM': parameter_LGBM\n",
    "}\n",
    "\n",
    "# Filter models and parameters based on ENABLED_SUBMODELS_FOR_TUNING_LIST\n",
    "estimators_for_bayes = {}\n",
    "params_mapping = {}\n",
    "\n",
    "for model_name in ENABLED_SUBMODELS_FOR_TUNING_LIST:\n",
    "    if model_name in all_possible_estimators_base and model_name in all_possible_params_base:\n",
    "        estimators_for_bayes[model_name] = all_possible_estimators_base[model_name]\n",
    "        params_mapping[model_name] = all_possible_params_base[model_name]\n",
    "    else:\n",
    "        print(f\"Warning: Model '{model_name}' is in ENABLED_SUBMODELS_FOR_TUNING_LIST, but its definition or parameter space is missing/unavailable. Skipping.\")\n",
    "\n",
    "if not estimators_for_bayes:\n",
    "    print(\"Error: No valid models available for tuning based on ENABLED_SUBMODELS_FOR_TUNING_LIST. Please check configuration.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\nModels to be tuned: {list(estimators_for_bayes.keys())}\")\n",
    "\n",
    "\n",
    "# --- 5. Set up and Run BayesSearchCV (currently for a single Y target) ---\n",
    "grid_searches = {} # Stores the tuning result object for each model\n",
    "\n",
    "print(f\"\\nStarting BayesSearchCV for each model on target '{target_column_name}' (n_iter = {N_ITER_BAYESIAN})...\")\n",
    "\n",
    "for name, estimator in estimators_for_bayes.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"--- Tuning {name} (Target: {target_column_name}) ---\")\n",
    "    \n",
    "    # Ensure the model has a corresponding parameter space\n",
    "    if name not in params_mapping:\n",
    "        print(f\"Warning: Parameter space not found for model {name}, skipping.\")\n",
    "        grid_searches[name] = None\n",
    "        continue\n",
    "\n",
    "    search_space = params_mapping[name]\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=clone(estimator), # Clone the estimator to ensure each tuning is independent\n",
    "        search_spaces=search_space,\n",
    "        scoring='neg_mean_squared_error', # Using negative MSE (for optimizing RMSE, as BayesSearchCV maximizes the score)\n",
    "        n_iter=N_ITER_BAYESIAN,\n",
    "        cv=cross_Valid, # Use LeavePOut (LOOCV)\n",
    "        n_jobs=-1, # Typically -1 to use all available cores for parallel processing\n",
    "        random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "        verbose=1 # Keep verbose=1 to output detailed information for each iteration\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        bayes_search.fit(X, Y) # Use the selected single target Y\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        grid_searches[name] = bayes_search\n",
    "\n",
    "        print(f\"--- {name} (Target: {target_column_name}) Tuning Complete ---\")\n",
    "        # Note: bayes_search.best_score_ is negative MSE, so take the negative and square root to get RMSE\n",
    "        print(f\"Best Score (Leave-{P_VALUE_FOR_BASE_TUNING}-Out CV RMSE): {np.sqrt(-bayes_search.best_score_):.4f}\")\n",
    "        print(f\"Best Parameters: {dict(bayes_search.best_params_)}\")\n",
    "        print(f\"Total Tuning Duration: {duration:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\n!!! Error occurred during tuning {name} (Target: {target_column_name}): {e}\")\n",
    "        print(f\"Tuning attempt duration: {duration:.2f} seconds\")\n",
    "        grid_searches[name] = None\n",
    "\n",
    "print(\"\\nAll model hyperparameter tuning complete.\")\n",
    "\n",
    "# --- 6. Final Results Overview ---\n",
    "print(\"\\n--- Final Optimization Results Overview ---\")\n",
    "print(f\"\\nTarget: {target_column_name}\")\n",
    "for name, result in grid_searches.items():\n",
    "    if result is not None:\n",
    "        print(f\"  Model: {name}\")\n",
    "        # Note: result.best_score_ is negative MSE, so take the negative and square root to get RMSE\n",
    "        print(f\"    Best Leave-{P_VALUE_FOR_BASE_TUNING}-Out CV RMSE: {np.sqrt(-result.best_score_):.4f}\")\n",
    "        print(f\"    Best Parameters: {result.best_params_}\")\n",
    "    else:\n",
    "        print(f\"  Model: {name} (Tuning Failed or Skipped)\")\n",
    "\n",
    "# X, Y, and grid_searches remain in memory for further use in the current session.\n",
    "print(\"\\nTraining data X, Y, and sub-model optimization results (grid_searches) are retained in the current session's memory.\")\n",
    "print(\"Please continue to run stacking evaluation and prediction scripts in the same session.\")\n",
    "\n",
    "print(\"\\nScript execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4156ff-118b-48fc-9573-76bb04eddf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_stacking_model.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import itertools # Used for generating combinations for LeavePOut\n",
    "\n",
    "# --- SHAP library import ---\n",
    "import shap\n",
    "# ------------------------\n",
    "\n",
    "# Model imports\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# Import StackingRegressor and meta-learner\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import CatBoost and LightGBM (if available)\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: CatBoost is not installed. Skipping CBR model.\")\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: LightGBM is not installed. Skipping LGBM model.\")\n",
    "    lightgbm_available = False\n",
    "\n",
    "# Scikit-learn Utilities\n",
    "from sklearn.model_selection import LeaveOneOut, LeavePOut, KFold # KFold added for meta-learner internal CV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.base import clone\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "### --- Adjustable Configuration Parameters --- ###\n",
    "\n",
    "# --- 1. Data Loading Settings (Note: Data X, Y, and grid_searches are expected to be available in the environment,\n",
    "# typically by running 'optimize_base_learners.py' first.) ---\n",
    "\n",
    "# --- 2. Bayesian Optimization Iterations ---\n",
    "# META_LEARNER_N_ITER_BAYESIAN controls the number of Bayesian optimization iterations for the meta-learner.\n",
    "META_LEARNER_N_ITER_BAYESIAN =  50  # Number of Bayesian optimization iterations for the meta-learner.\n",
    "                                 # Higher values lead to a more thorough search but increase computation time.\n",
    "\n",
    "# --- 3. Default Random State (for all model initializations and Bayesian optimization itself) ---\n",
    "# Note: For LeavePOut and LeaveOneOut (shuffle=False), the splits are deterministic.\n",
    "# The random_state primarily affects the internal randomness of the models.\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 # Ensures reproducibility of results across runs.\n",
    "\n",
    "# --- 4. External Cross-Validation and Overall Evaluation Parameters ---\n",
    "# Adjust OUTER_CV_P_VALUE here to select the external evaluation method:\n",
    "# P=1: Leave-One-Out (LOO) Cross-Validation. Each sample is a test set once.\n",
    "# P=2: Leave-Two-Out (LTO) Cross-Validation. Each pair of samples is a test set once.\n",
    "# P=3: Leave-Three-Out (L3O) Cross-Validation. Each triplet of samples is a test set once.\n",
    "#\n",
    "# Implications for dataset partitioning:\n",
    "# - A higher P value means smaller training sets (N-P samples) and a larger number of total folds (combinations).\n",
    "# - For N=7 samples:\n",
    "#   - P=1 (LOO) results in C(7,1) = 7 combinations (folds).\n",
    "#   - P=2 (LTO) results in C(7,2) = 21 combinations (folds).\n",
    "#   - P=3 (L3O) results in C(7,3) = 35 combinations (folds).\n",
    "# Choose P based on your dataset size and computational resources. For very small N, P=1 or P=2 is common.\n",
    "OUTER_CV_P_VALUE = 3        # <-- P-value for external evaluation.\n",
    "N_SEEDS_FOR_EVALUATION = None # <-- This value will be dynamically calculated based on OUTER_CV_P_VALUE.\n",
    "\n",
    "# --- 5. Noise Intensity Control Interface ---\n",
    "NOISE_SCALE_FACTOR = 0     # Factor for data augmentation (Gaussian noise). Set to 0 to disable.\n",
    "                           # Adding noise to the training data can help regularize the model and\n",
    "                           # prevent overfitting, especially beneficial for small datasets.\n",
    "                           # By tuning this factor, one can potentially mitigate overfitting.\n",
    "                           # In this specific work, noise augmentation is not utilized (factor is 0).\n",
    "\n",
    "# --- 6. SHAP Feature Importance Calculation Parameters ---\n",
    "# For KernelExplainer (used for non-tree models), the background data for SHAP calculation\n",
    "# will directly use the current training set. No separate background sample limit is needed.\n",
    "\n",
    "# --- 7. Plotting Parameters ---\n",
    "N_FEATURES_TO_PLOT = 20 # Number of features to display in the feature importance bar plot.\n",
    "PLOT_SHAP_SWARM_PLOT = True # Whether to generate the SHAP swarm plot.\n",
    "SHAP_SWARM_SAMPLES_LIMIT = 1000 # Maximum number of samples to use for the SHAP swarm plot to prevent memory issues.\n",
    "                                # Set to None to use all available samples.\n",
    "\n",
    "# --- 8. Sub-model Selection Interface for Stacking ---\n",
    "# List the names of the sub-models you wish to include in the Stacking model.\n",
    "# If a model's name is not present in this list, it will be excluded from the Stacking ensemble.\n",
    "# You can easily disable a model by commenting out its entry in this list.\n",
    "# Available model names typically include: 'XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM'\n",
    "ENABLED_SUBMODELS_LIST = [\n",
    "    'XGBR',\n",
    "    'RF',\n",
    "    'GBRT',\n",
    "    'ETR',\n",
    "    'HGBR',\n",
    "    'CBR',\n",
    "    'LGBM'\n",
    "]\n",
    "# Ensure models in ENABLED_SUBMODELS_LIST are actually available\n",
    "if 'CBR' in ENABLED_SUBMODELS_LIST and not catboost_available:\n",
    "    print(\"Warning: CatBoost is not installed. 'CBR' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('CBR')\n",
    "if 'LGBM' in ENABLED_SUBMODELS_LIST and not lightgbm_available:\n",
    "    print(\"Warning: LightGBM is not installed. 'LGBM' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('LGBM')\n",
    "\n",
    "### --- End Configuration Parameters --- ###\n",
    "\n",
    "# --- IMPORTANT: X, Y, and grid_searches variables must be defined in the environment before running this script. ---\n",
    "# For example, by first executing the 'optimize_base_learners.py' script to provide these variables.\n",
    "# If these variables are not defined, this script will fail with a NameError.\n",
    "\n",
    "try:\n",
    "    # Check if X is a DataFrame, which is important for getting indices\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        print(\"Warning: Input X is not a Pandas DataFrame. Sample indices will use RangeIndex.\")\n",
    "        X_index = pd.RangeIndex(start=0, stop=len(X), step=1)\n",
    "    else:\n",
    "        X_index = X.index\n",
    "\n",
    "    # Check Y's type and length\n",
    "    if len(Y) != len(X):\n",
    "         raise ValueError(\"Length of X and Y do not match!\")\n",
    "    if isinstance(Y, pd.DataFrame):\n",
    "        if Y.shape[1] != 1:\n",
    "            raise ValueError(\"If Y is a DataFrame, it should contain only one target column.\")\n",
    "        if not Y.index.equals(X_index):\n",
    "            print(\"Warning: Y's index does not match X's index, which might cause issues. It is recommended to align indices beforehand.\")\n",
    "        Y = Y.iloc[:, 0] # Ensure Y is a Series\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}. Variables 'X', 'Y', or 'grid_searches' are not defined in the current environment. Please ensure 'optimize_base_learners.py' is run first or load the required variables manually.\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Data Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Robust NaN Filling Check ---\n",
    "print(\"\\nForcing data conversion to numeric types and handling non-numeric entries...\")\n",
    "initial_X_nan_count = X.isnull().sum().sum()\n",
    "initial_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "Y = Y.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "new_X_nan_count = X.isnull().sum().sum()\n",
    "new_Y_nan_count = Y.isnull().sum().sum()\n",
    "\n",
    "if new_X_nan_count > initial_X_nan_count:\n",
    "    print(f\"Warning: {new_X_nan_count - initial_X_nan_count} non-numeric entries in feature data X were converted to NaN.\")\n",
    "if new_Y_nan_count > initial_Y_nan_count:\n",
    "    print(f\"Warning: {new_Y_nan_count - initial_Y_nan_count} non-numeric entries in target data Y were converted to NaN.\")\n",
    "\n",
    "print(\"\\nChecking and filling NaN values in data...\")\n",
    "if X.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in feature data X. Filling with median.\")\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"NaN values in X filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in feature data X.\")\n",
    "\n",
    "if Y.isnull().any().any():\n",
    "    warnings.warn(\"Warning: NaN values detected in target data Y. Filling with median.\")\n",
    "    y_median_val = Y.median()\n",
    "    if pd.isna(y_median_val): # If the median itself is NaN, it means all Y values are NaN\n",
    "        print(\"Error: All values in target data Y column are NaN. Cannot perform effective filling or model training. Please check the raw data.\")\n",
    "        exit() # Exit the script\n",
    "    Y = Y.fillna(y_median_val)\n",
    "    print(\"NaN values in Y filled with median.\")\n",
    "else:\n",
    "    print(\"No NaN values detected in target data Y.\")\n",
    "# --- End Robust NaN Filling Check ---\n",
    "\n",
    "\n",
    "# Check if the P-value for LeavePOut is valid\n",
    "if OUTER_CV_P_VALUE >= X.shape[0]:\n",
    "    print(f\"Error: The P-value for Leave-P-Out ({OUTER_CV_P_VALUE}) must be less than the total number of samples ({X.shape[0]}). Please adjust OUTER_CV_P_VALUE.\")\n",
    "    exit()\n",
    "if OUTER_CV_P_VALUE < 1:\n",
    "    print(f\"Error: The P-value for Leave-P-Out ({OUTER_CV_P_VALUE}) must be greater than or equal to 1. Please adjust OUTER_CV_P_VALUE.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Dataset sample size N = {X.shape[0]}\")\n",
    "print(f\"External evaluation will use Leave-{OUTER_CV_P_VALUE}-Out Cross-Validation.\")\n",
    "# Generate all possible test set combinations (e.g., N=7, P=2 will generate C(7,2) = 21 combinations)\n",
    "all_sample_indices = list(range(len(X)))\n",
    "lpo_combinations = list(itertools.combinations(all_sample_indices, OUTER_CV_P_VALUE))\n",
    "N_SEEDS_FOR_EVALUATION = len(lpo_combinations)\n",
    "print(f\"Total combinations for C({X.shape[0]}, {OUTER_CV_P_VALUE}) = {N_SEEDS_FOR_EVALUATION} folds.\")\n",
    "\n",
    "\n",
    "# --- Noise Intensity Control Interface ---\n",
    "if NOISE_SCALE_FACTOR > 0:\n",
    "    print(f\"Gaussian noise augmentation is enabled with intensity factor: {NOISE_SCALE_FACTOR}\")\n",
    "else:\n",
    "    print(\"Noise augmentation is disabled.\")\n",
    "np.random.seed(DEFAULT_MODEL_RANDOM_STATE) # Seed for noise generation\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def initialize_best_estimators(grid_searches_dict, enabled_models_list):\n",
    "    \"\"\"\n",
    "    Initializes best base learner instances from BayesSearchCV results, filtered by the enabled_models_list.\n",
    "    This function is used to retrieve the optimized base models for stacking.\n",
    "\n",
    "    Args:\n",
    "        grid_searches_dict (dict): A dictionary containing BayesSearchCV results for each model.\n",
    "        enabled_models_list (list): A list of model names that are enabled for stacking.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of initialized best estimator instances.\n",
    "    \"\"\"\n",
    "    estimators_init = {}\n",
    "    # Define all possible models and their default parameters\n",
    "    all_possible_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "\n",
    "    if catboost_available:\n",
    "        all_possible_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available:\n",
    "        all_possible_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing models...\")\n",
    "    for name, (model_class, fixed_params) in all_possible_models.items():\n",
    "        if name not in enabled_models_list: # If the model is not in the enabled list, skip it\n",
    "            print(f\"  Model {name} is not in ENABLED_SUBMODELS_LIST, skipping initialization.\")\n",
    "            continue\n",
    "\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} with optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "        else:\n",
    "            print(f\"  No optimized results found for {name}. Attempting to initialize with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} with default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (with default parameters): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v is not None}\n",
    "    if not initialized_estimators:\n",
    "         print(\"Warning: No models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "\n",
    "# --- Define Meta-Learner Hyperparameter Search Space ---\n",
    "meta_learner_params = {\n",
    "    'elasticnet__alpha': Real(1e-5, 10.0, prior='log-uniform', name='elasticnet__alpha'),\n",
    "    'elasticnet__l1_ratio': Real(0.0, 1.0, prior='uniform', name='elasticnet__l1_ratio')\n",
    "}\n",
    "\n",
    "# Global variables to store the best meta-learner instance (with fixed hyperparameters and coefficients)\n",
    "# after a one-time pre-tuning.\n",
    "global_fixed_meta_learner = None\n",
    "global_fixed_meta_coeffs_dict = None # Stores the final fixed coefficients for printing and inspection.\n",
    "\n",
    "# Define function to evaluate model performance and feature importance\n",
    "def evaluate_models_manual_cv(run_idx, X_full, Y_full, X_full_index, grid_searches_dict, noise_factor,\n",
    "                              fixed_meta_learner_instance, current_test_indices):\n",
    "    \"\"\"\n",
    "    Performs a manual Leave-P-Out CV iteration to generate Out-Of-Fold (OOF) predictions,\n",
    "    evaluate the Stacking model's performance, and calculate feature importances using SHAP values.\n",
    "\n",
    "    Args:\n",
    "        run_idx (int): Current run index for logging.\n",
    "        X_full (pd.DataFrame or np.array): Full feature dataset.\n",
    "        Y_full (pd.Series or np.array): Full target dataset.\n",
    "        X_full_index (pd.Index or np.array): Original indices of X_full.\n",
    "        grid_searches_dict (dict): Dictionary of pre-tuned base model results.\n",
    "        noise_factor (float): Factor for Gaussian noise augmentation.\n",
    "        fixed_meta_learner_instance (sklearn.pipeline.Pipeline): The pre-tuned and fixed meta-learner.\n",
    "        current_test_indices (list): Indices of samples for the current test fold.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics, feature importances, plot data,\n",
    "              meta-learner coefficients, and raw SHAP values for the current run.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Run {run_idx} ---\")\n",
    "  \n",
    "    # Initialize base models, filtered by ENABLED_SUBMODELS_LIST\n",
    "    best_estimators_dict = initialize_best_estimators(grid_searches_dict, ENABLED_SUBMODELS_LIST)\n",
    "\n",
    "    if not best_estimators_dict:\n",
    "        print(f\"Run {run_idx}: No base models available for evaluation.\")\n",
    "        return {\n",
    "            'submodel_r2_means': {}, 'submodel_rmse_means': {}, 'submodel_mae_means': {},\n",
    "            'stacking_regressor_rmse_mean': np.nan, 'stacking_regressor_mae_mean': np.nan,\n",
    "            'submodel_feature_importances': {}, 'weighted_feature_importances': {},\n",
    "            'plot_data': [], 'meta_learner_coefficients': {},\n",
    "            'raw_shap_values': {}, 'shap_expected_values': {}, 'X_val_data_for_shap': [],\n",
    "            'all_scores_data_for_seed_fold': []\n",
    "        }\n",
    "\n",
    "    submodels = list(best_estimators_dict.items())\n",
    "    submodel_names = [name for name, _ in submodels]\n",
    "\n",
    "    # --- Initialize storage for Fold results (now for a single run's results) ---\n",
    "    fold_r2_scores = {name: [] for name in submodel_names}\n",
    "    fold_rmse_scores = {name: [] for name in submodel_names}\n",
    "    fold_mae_scores = {name: [] for name in submodel_names}\n",
    "    fold_rmse_scores['Stacking'] = []\n",
    "    fold_mae_scores['Stacking'] = []\n",
    "    all_plot_data_for_seed = []\n",
    "    all_scores_data_for_seed_fold = []\n",
    "\n",
    "    # --- Initialize accumulators for feature importances (sub-models) ---\n",
    "    n_features = X_full.shape[1]\n",
    "    fold_feature_importances_sums = {name: np.zeros(n_features) for name in submodel_names}\n",
    "    fold_feature_importances_counts = {name: 0 for name in submodel_names}\n",
    "\n",
    "    # --- New: Lists to collect raw SHAP values ---\n",
    "    all_raw_shap_values_per_model = {name: [] for name in submodel_names}\n",
    "    all_shap_expected_values_per_model = {name: [] for name in submodel_names}\n",
    "    all_X_val_data_for_shap_folds = [] # Stores X_val_fold (DataFrame) for the current run\n",
    "\n",
    "    print(f\"  Run {run_idx}: Starting Leave-{OUTER_CV_P_VALUE}-Out evaluation...\")\n",
    "    is_X_dataframe = isinstance(X_full, pd.DataFrame)\n",
    "    original_columns = X_full.columns if is_X_dataframe else [f'Feature_{i}' for i in range(n_features)]\n",
    "    feature_names = list(original_columns)\n",
    "\n",
    "    # --- External Leave-P-Out loop (now a single iteration, with train/val indices provided by the main loop) ---\n",
    "    train_loc_idx = np.array(list(set(range(len(X_full))) - set(current_test_indices)))\n",
    "    val_loc_idx = np.array(current_test_indices)\n",
    "\n",
    "    train_original_indices = X_full_index[train_loc_idx]\n",
    "    val_original_indices = X_full_index[val_loc_idx]\n",
    "\n",
    "    if is_X_dataframe:\n",
    "        X_train_fold, X_val_fold = X_full.loc[train_original_indices], X_full.loc[val_original_indices]\n",
    "        if isinstance(Y_full, (pd.Series, pd.DataFrame)):\n",
    "            Y_train_fold, Y_val_fold = Y_full.loc[train_original_indices], Y_full.loc[val_original_indices]\n",
    "        else:\n",
    "            Y_train_fold, Y_val_fold = Y_full[train_loc_idx], Y_full[val_loc_idx]\n",
    "    else:\n",
    "        X_train_fold, X_val_fold = X_full[train_loc_idx], X_full[val_loc_idx]\n",
    "        Y_train_fold, Y_val_fold = Y_full[train_loc_idx], Y_full[val_loc_idx]\n",
    "\n",
    "    # --- Crucial: Add the current X_val_fold to the list for SHAP aggregation ---\n",
    "    all_X_val_data_for_shap_folds.append(X_val_fold)\n",
    "\n",
    "    y_val_true_np = Y_val_fold.values if isinstance(Y_val_fold, (pd.Series, pd.DataFrame)) else np.array(Y_val_fold)\n",
    "    y_train_true_np = Y_train_fold.values if isinstance(Y_train_fold, (pd.Series, pd.DataFrame)) else np.array(Y_train_fold)\n",
    "\n",
    "    X_train_augmented = X_train_fold\n",
    "    if noise_factor > 0:\n",
    "        # Apply Gaussian noise to the training features for data augmentation.\n",
    "        # This helps in regularizing the model and improving its generalization\n",
    "        # by making it robust to small perturbations in the input data.\n",
    "        X_train_fold_np = X_train_fold.values if is_X_dataframe else X_train_fold\n",
    "        feature_std_devs = np.std(X_train_fold_np, axis=0)\n",
    "        noise_std_devs = feature_std_devs * noise_factor + 1e-9\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_std_devs, size=X_train_fold_np.shape)\n",
    "        if is_X_dataframe:\n",
    "            X_train_augmented = pd.DataFrame(X_train_fold_np + noise, index=train_original_indices, columns=original_columns)\n",
    "        else:\n",
    "            X_train_augmented = X_train_fold_np + noise\n",
    "\n",
    "    # --- Store current run's OOF predictions (from base learners) ---\n",
    "    # Strict OOF prediction generation for meta-learner training on current external training set\n",
    "    oof_preds_train = np.zeros((len(Y_train_fold), len(submodels)))\n",
    "    oof_preds_val = np.zeros((len(Y_val_fold), len(submodels)))\n",
    "\n",
    "    # Use LeaveOneOut on the current external training set to generate OOF predictions for meta-learner training.\n",
    "    # For example, if external evaluation is Leave-Two-Out (P=2), the training set size is N-2 = 5 samples.\n",
    "    # LeaveOneOut will generate 5 folds for this internal OOF generation.\n",
    "    kf_for_oof_train = LeaveOneOut() # Base learners always use LeaveOneOut for OOF prediction generation for the meta-learner.\n",
    "  \n",
    "    for i, (name, estimator_template) in enumerate(submodels):\n",
    "        current_model_oof_preds = np.zeros(len(Y_train_fold))\n",
    "      \n",
    "        try:\n",
    "            # Generate OOF predictions for the current external training fold\n",
    "            for inner_train_idx, inner_val_idx in kf_for_oof_train.split(X_train_augmented, Y_train_fold):\n",
    "                X_inner_train = X_train_augmented.iloc[inner_train_idx] if is_X_dataframe else X_train_augmented[inner_train_idx]\n",
    "                Y_inner_train = Y_train_fold.iloc[inner_train_idx] if isinstance(Y_train_fold, pd.Series) else Y_train_fold[inner_train_idx]\n",
    "                X_inner_val = X_train_augmented.iloc[inner_val_idx] if is_X_dataframe else X_train_augmented[inner_val_idx]\n",
    "\n",
    "                estimator_clone = clone(estimator_template)\n",
    "                # Apply noise to X_inner_train if noise_factor > 0\n",
    "                X_inner_train_augmented_for_oof = X_inner_train\n",
    "                if noise_factor > 0:\n",
    "                    # Apply Gaussian noise to the training features for data augmentation.\n",
    "                    # This helps in regularizing the model and improving its generalization\n",
    "                    # by making it robust to small perturbations in the input data.\n",
    "                    X_inner_train_np_for_oof = X_inner_train.values if isinstance(X_inner_train, pd.DataFrame) else X_inner_train\n",
    "                    feature_std_devs_for_oof = np.std(X_inner_train_np_for_oof, axis=0)\n",
    "                    noise_std_devs_for_oof = feature_std_devs_for_oof * noise_factor + 1e-9\n",
    "                    noise_for_oof = np.random.normal(loc=0.0, scale=noise_std_devs_for_oof, size=X_inner_train_np_for_oof.shape)\n",
    "                    if isinstance(X_inner_train, pd.DataFrame):\n",
    "                        X_inner_train_augmented_for_oof = pd.DataFrame(X_inner_train_np_for_oof + noise_for_oof, index=X_inner_train.index, columns=X_inner_train.columns)\n",
    "                    else:\n",
    "                        X_inner_train_augmented_for_oof = X_inner_train_np_for_oof + noise_for_oof\n",
    "\n",
    "                estimator_clone.fit(X_inner_train_augmented_for_oof, Y_inner_train) # Use augmented data for training\n",
    "                current_model_oof_preds[inner_val_idx] = estimator_clone.predict(X_inner_val) # Predict on original (non-augmented) inner_val_set\n",
    "          \n",
    "            oof_preds_train[:, i] = current_model_oof_preds\n",
    "          \n",
    "            # Train a final model on the full X_train_augmented for prediction on X_val_fold\n",
    "            final_estimator_for_val_pred = clone(estimator_template)\n",
    "            final_estimator_for_val_pred.fit(X_train_augmented, Y_train_fold)\n",
    "            oof_preds_val[:, i] = final_estimator_for_val_pred.predict(X_val_fold)\n",
    "\n",
    "\n",
    "        except Exception as fit_e:\n",
    "             print(f\"\\nWarning: Run {run_idx}, Model {name} training or OOF generation failed: {fit_e}\")\n",
    "             oof_preds_train[:, i] = 0\n",
    "             oof_preds_val[:, i] = 0\n",
    "             # Record NaN scores for failed models\n",
    "             all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'MAE', 'Value': np.nan})\n",
    "             all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'RMSE', 'Value': np.nan})\n",
    "             continue\n",
    "\n",
    "        # Calculate metrics for base models on the current validation fold\n",
    "        r2 = r2_score(Y_val_fold, oof_preds_val[:, i])\n",
    "        rmse = np.sqrt(mean_squared_error(Y_val_fold, oof_preds_val[:, i]))\n",
    "        mae = mean_absolute_error(Y_val_fold, oof_preds_val[:, i])\n",
    "\n",
    "        fold_r2_scores[name].append(r2)\n",
    "        fold_rmse_scores[name].append(rmse)\n",
    "        fold_mae_scores[name].append(mae)\n",
    "      \n",
    "        # --- New: Record MAE and RMSE for each sub-model in the current run ---\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'MAE', 'Value': mae})\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': name, 'Metric': 'RMSE', 'Value': rmse})\n",
    "\n",
    "        for idx, (actual, pred) in enumerate(zip(y_val_true_np, oof_preds_val[:, i])):\n",
    "            original_index = val_original_indices[idx]\n",
    "            all_plot_data_for_seed.append({\n",
    "                'Seed': run_idx, 'Fold': 0, 'Model': name, # Fold unified to 0 for external loop\n",
    "                'Set': 'Validation',\n",
    "                'Actual': actual, 'Predicted': pred,\n",
    "                'Sample_Index': original_index\n",
    "            })\n",
    "        for idx, (actual, pred) in enumerate(zip(y_train_true_np, oof_preds_train[:, i])):\n",
    "            original_index = train_original_indices[idx]\n",
    "            all_plot_data_for_seed.append({\n",
    "                'Seed': run_idx, 'Fold': 0, 'Model': name, # Fold unified to 0 for external loop\n",
    "                'Set': 'Train',\n",
    "                'Actual': actual, 'Predicted': pred,\n",
    "                'Sample_Index': original_index\n",
    "            })\n",
    "\n",
    "        # --- SHAP Feature Importance Calculation ---\n",
    "        try:\n",
    "            # Convert to NumPy array for SHAP compatibility\n",
    "            X_val_fold_np = X_val_fold.values if isinstance(X_val_fold, pd.DataFrame) else X_val_fold\n",
    "            X_train_fold_np = X_train_fold.values if isinstance(X_train_fold, pd.DataFrame) else X_train_fold\n",
    "\n",
    "            # Ensure validation set is not empty, otherwise SHAP will error\n",
    "            if X_val_fold_np.shape[0] == 0:\n",
    "                print(f\"Warning: Run {run_idx}, Model {name}: Validation set is empty, skipping SHAP calculation.\")\n",
    "                continue\n",
    "\n",
    "            # Check model type to select appropriate Explainer\n",
    "            if isinstance(final_estimator_for_val_pred, (XGB.XGBRegressor, RandomForestRegressor,\n",
    "                                           GradientBoostingRegressor, CatBoostRegressor, LGBMRegressor,\n",
    "                                           ExtraTreesRegressor, HistGradientBoostingRegressor)):\n",
    "                explainer = shap.TreeExplainer(final_estimator_for_val_pred)\n",
    "                shap_values = explainer.shap_values(X_val_fold_np)\n",
    "                expected_value = explainer.expected_value\n",
    "            else: # For non-tree models, use KernelExplainer\n",
    "                # For N=7, P=2, training set has 5 samples, so background data is the training set itself.\n",
    "                # Ensure background data is not empty.\n",
    "                if X_train_fold_np.shape[0] == 0:\n",
    "                    print(f\"Warning: Run {run_idx}, Model {name}: Training set is empty, cannot generate background data for KernelExplainer, skipping SHAP calculation.\")\n",
    "                    continue\n",
    "\n",
    "                background_data = X_train_fold_np\n",
    "              \n",
    "                print(f\"  Run {run_idx}, Model {name}: Calculating SHAP using KernelExplainer (may be slow)...\")\n",
    "                explainer = shap.KernelExplainer(final_estimator_for_val_pred.predict, background_data)\n",
    "                shap_values = explainer.shap_values(X_val_fold_np)\n",
    "                expected_value = explainer.expected_value\n",
    "          \n",
    "            # --- Store raw SHAP values and expected values ---\n",
    "            all_raw_shap_values_per_model[name].append(shap_values)\n",
    "            all_shap_expected_values_per_model[name].append(expected_value)\n",
    "\n",
    "            # Global feature importance from SHAP is typically the mean absolute SHAP value (for bar plots)\n",
    "            importances = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "            if importances.shape[0] == n_features:\n",
    "                 fold_feature_importances_sums[name] += importances\n",
    "                 fold_feature_importances_counts[name] += 1\n",
    "            else:\n",
    "                print(f\"Warning: Run {run_idx}, Model {name} SHAP importance dimension mismatch.\")\n",
    "\n",
    "        except Exception as imp_e:\n",
    "             print(f\"\\nWarning: Run {run_idx}, Model {name} error getting SHAP importance: {imp_e}\")\n",
    "        # --- END SHAP Calculation ---\n",
    "\n",
    "    # --- Train and Evaluate Meta-Learner (ElasticNet) ---\n",
    "    if np.all(oof_preds_train == 0) or np.any(np.isnan(oof_preds_train)):\n",
    "        print(f\"Warning: Run {run_idx}: OOF predictions are all zeros or contain NaN, skipping meta-learner training.\")\n",
    "        fold_rmse_scores['Stacking'].append(np.nan)\n",
    "        fold_mae_scores['Stacking'].append(np.nan)\n",
    "        # Record NaN scores for failed models\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': np.nan})\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': np.nan})\n",
    "        return {\n",
    "            'submodel_r2_means': {}, 'submodel_rmse_means': {}, 'submodel_mae_means': {},\n",
    "            'stacking_regressor_rmse_mean': np.nan,\n",
    "            'stacking_regressor_mae_mean': np.nan,\n",
    "            'submodel_feature_importances': {}, 'weighted_feature_importances': {},\n",
    "            'plot_data': all_plot_data_for_seed, 'meta_learner_coefficients': {},\n",
    "            'raw_shap_values': {}, 'shap_expected_values': {}, 'X_val_data_for_shap': [],\n",
    "            'all_scores_data_for_seed_fold': []\n",
    "        }\n",
    "\n",
    "    # Use the globally fixed meta-learner instance for prediction, no re-training here.\n",
    "    try:\n",
    "        y_pred_val_stacking = fixed_meta_learner_instance.predict(oof_preds_val)\n",
    "        y_pred_train_stacking = fixed_meta_learner_instance.predict(oof_preds_train)\n",
    "    except Exception as meta_pred_e:\n",
    "        print(f\"\\nWarning: Run {run_idx}, prediction with fixed meta-learner failed: {meta_pred_e}\")\n",
    "        fold_rmse_scores['Stacking'].append(np.nan)\n",
    "        fold_mae_scores['Stacking'].append(np.nan)\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': np.nan})\n",
    "        all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': np.nan})\n",
    "        return {\n",
    "            'submodel_r2_means': {name: np.nan for name in submodel_names},\n",
    "            'submodel_rmse_means': {name: np.nan for name in submodel_names},\n",
    "            'submodel_mae_means': {name: np.nan for name in submodel_names},\n",
    "            'stacking_regressor_rmse_mean': np.nan,\n",
    "            'stacking_regressor_mae_mean': np.nan,\n",
    "            'submodel_feature_importances': {}, 'weighted_feature_importances': {},\n",
    "            'plot_data': all_plot_data_for_seed, 'meta_learner_coefficients': {},\n",
    "            'raw_shap_values': {},\n",
    "            'shap_expected_values': {},\n",
    "            'X_val_data_for_shap': [],\n",
    "            'all_scores_data_for_seed_fold': all_scores_data_for_seed_fold\n",
    "        }\n",
    "\n",
    "    rmse_stacking = np.sqrt(mean_squared_error(Y_val_fold, y_pred_val_stacking))\n",
    "    mae_stacking = mean_absolute_error(Y_val_fold, y_pred_val_stacking)\n",
    "\n",
    "    fold_rmse_scores['Stacking'].append(rmse_stacking)\n",
    "    fold_mae_scores['Stacking'].append(mae_stacking)\n",
    "  \n",
    "    # --- New: Record MAE and RMSE for the Stacking model in the current run ---\n",
    "    all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': mae_stacking})\n",
    "    all_scores_data_for_seed_fold.append({'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': rmse_stacking})\n",
    "\n",
    "    for idx, (actual, pred) in enumerate(zip(y_val_true_np, y_pred_val_stacking)):\n",
    "        original_index = val_original_indices[idx]\n",
    "        all_plot_data_for_seed.append({\n",
    "            'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking',\n",
    "            'Set': 'Validation',\n",
    "            'Actual': actual, 'Predicted': pred,\n",
    "            'Sample_Index': original_index\n",
    "        })\n",
    "    for idx, (actual, pred) in enumerate(zip(y_train_true_np, y_pred_train_stacking)):\n",
    "        original_index = train_original_indices[idx]\n",
    "        all_plot_data_for_seed.append({\n",
    "            'Seed': run_idx, 'Fold': 0, 'Model': 'Stacking',\n",
    "            'Set': 'Train',\n",
    "            'Actual': actual, 'Predicted': pred,\n",
    "            'Sample_Index': original_index\n",
    "        })\n",
    "\n",
    "\n",
    "    # --- End of Run ---\n",
    "    print(f\"\\n  Run {run_idx}: Leave-{OUTER_CV_P_VALUE}-Out evaluation complete.\")\n",
    "\n",
    "    # --- Calculate average scores (now just taking the values from the list for a single run) ---\n",
    "    submodel_r2_means = {name: scores[0] if scores else np.nan for name, scores in fold_r2_scores.items() if scores}\n",
    "    submodel_rmse_means = {name: scores[0] if scores else np.nan for name, scores in fold_rmse_scores.items() if scores}\n",
    "    submodel_mae_means = {name: scores[0] if scores else np.nan for name, scores in fold_mae_scores.items() if scores}\n",
    "    stacking_regressor_rmse_mean = fold_rmse_scores['Stacking'][0] if fold_rmse_scores['Stacking'] else np.nan\n",
    "    stacking_regressor_mae_mean = fold_mae_scores['Stacking'][0] if fold_mae_scores['Stacking'] else np.nan\n",
    "\n",
    "    # --- Calculate Feature Importances (Sub-models) ---\n",
    "    print(f\"  Run {run_idx}: Calculating feature importances based on current run (SHAP)...\")\n",
    "    submodel_avg_seed_importances = {}\n",
    "    feature_importances_weighted_sum = np.zeros(n_features)\n",
    "    total_exp_rmse_weighting = 0 # Sum of exp(-RMSE) for weighting\n",
    "    for name in submodel_names:\n",
    "        count = fold_feature_importances_counts[name]\n",
    "        if count > 0:\n",
    "            avg_importances = fold_feature_importances_sums[name] / count\n",
    "            sum_avg_importances = np.sum(avg_importances)\n",
    "            if sum_avg_importances > 1e-9:\n",
    "                importances_normalized = avg_importances / sum_avg_importances\n",
    "            else:\n",
    "                importances_normalized = np.zeros_like(avg_importances)\n",
    "            submodel_avg_seed_importances[name] = dict(zip(feature_names, importances_normalized))\n",
    "          \n",
    "            # Use exp(-RMSE) for weighting\n",
    "            model_rmse_val = submodel_rmse_means.get(name, np.nan)\n",
    "            if not np.isnan(model_rmse_val): # RMSE will not be negative, only check for NaN\n",
    "                 weight = np.exp(-model_rmse_val) # exp(-RMSE)\n",
    "                 feature_importances_weighted_sum += importances_normalized * weight\n",
    "                 total_exp_rmse_weighting += weight\n",
    "        else:\n",
    "            submodel_avg_seed_importances[name] = {}\n",
    "  \n",
    "    if total_exp_rmse_weighting > 1e-9:\n",
    "        weighted_feature_importances_vector = feature_importances_weighted_sum / total_exp_rmse_weighting\n",
    "    else:\n",
    "        weighted_feature_importances_vector = np.zeros(n_features)\n",
    "    weighted_feature_importances_dict = dict(zip(feature_names, weighted_feature_importances_vector))\n",
    "    print(f\"  Run {run_idx}: Feature importance calculation complete (SHAP).\")\n",
    "\n",
    "    # Return the globally fixed meta-learner coefficients\n",
    "    return {\n",
    "        'submodel_r2_means': submodel_r2_means,\n",
    "        'submodel_rmse_means': submodel_rmse_means,\n",
    "        'submodel_mae_means': submodel_mae_means,\n",
    "        'stacking_regressor_rmse_mean': stacking_regressor_rmse_mean,\n",
    "        'stacking_regressor_mae_mean': stacking_regressor_mae_mean,\n",
    "        'submodel_feature_importances': submodel_avg_seed_importances,\n",
    "        'weighted_feature_importances': weighted_feature_importances_dict,\n",
    "        'plot_data': all_plot_data_for_seed,\n",
    "        'meta_learner_coefficients': global_fixed_meta_coeffs_dict, # Always return the fixed global coefficients\n",
    "        'raw_shap_values': all_raw_shap_values_per_model, # New return\n",
    "        'shap_expected_values': all_shap_expected_values_per_model, # New return\n",
    "        'X_val_data_for_shap': all_X_val_data_for_shap_folds, # New return\n",
    "        'all_scores_data_for_seed_fold': all_scores_data_for_seed_fold # New return\n",
    "    }\n",
    "\n",
    "# === Main Execution Flow ===\n",
    "# Get X's index and feature names\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    X_index = X.index\n",
    "    feature_names_list = X.columns.tolist()\n",
    "else:\n",
    "    print(\"Warning: Input X is not a Pandas DataFrame. Feature importances will use default names 'Feature_i', and sample indices will use RangeIndex.\")\n",
    "    X_index = pd.RangeIndex(start=0, stop=len(X), step=1)\n",
    "    feature_names_list = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# Get available model names (filtered by ENABLED_SUBMODELS_LIST)\n",
    "try:\n",
    "    _initial_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "    model_names_available = list(_initial_estimators.keys())\n",
    "    if not model_names_available:\n",
    "        print(\"Error: No models successfully initialized, cannot proceed. Please check ENABLED_SUBMODELS_LIST.\")\n",
    "        exit()\n",
    "    del _initial_estimators\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while trying to initialize models to get names: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Meta-Learner Hyperparameter Pre-tuning Stage (One-time execution) ---\n",
    "# The meta-learner tuning uses Leave-One-Out CV for OOF generation and internal CV, then fixes coefficients.\n",
    "print(f\"\\n=== Starting Meta-Learner Hyperparameter Pre-tuning (One-time execution, using Leave-One-Out CV for OOF generation and internal CV) ===\")\n",
    "\n",
    "# Generate OOF predictions for ALL N samples for this meta-tuning iteration\n",
    "oof_preds_full_current_iter = np.zeros((len(X), len(model_names_available)))\n",
    "\n",
    "# Initialize base estimators, filtered by ENABLED_SUBMODELS_LIST\n",
    "temp_base_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "if not temp_base_estimators:\n",
    "    print(\"Warning: Failed to initialize base learners, skipping meta-learner tuning.\")\n",
    "    # Fallback to a default ElasticNet if base learners fail\n",
    "    global_fixed_meta_learner = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "    ])\n",
    "    # Also set default coefficients if no tuning happens\n",
    "    global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "else:\n",
    "    # Use LeaveOneOut to generate OOF predictions on the entire dataset (e.g., N=7)\n",
    "    loo_for_full_oof = LeaveOneOut() # Meta-learner OOF generation always uses LeaveOneOut.\n",
    "  \n",
    "    # This check is mostly for general robustness; for N=7, LOOCV is fine.\n",
    "    if loo_for_full_oof.get_n_splits(X) < 2:\n",
    "        print(f\"Warning: LeaveOneOut cannot generate at least 2 folds with {len(X)} samples. Skipping meta-learner tuning.\")\n",
    "        global_fixed_meta_learner = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "        ])\n",
    "        global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "    else:\n",
    "        for i, (name, estimator_template) in enumerate(temp_base_estimators.items()):\n",
    "            print(f\"  Generating OOF predictions for model {name} (for meta-learner tuning)...\")\n",
    "            fold_preds = np.zeros(len(X))\n",
    "          \n",
    "            # Generate OOF predictions using LeaveOneOut\n",
    "            for train_idx, val_idx in loo_for_full_oof.split(X, Y):\n",
    "                X_inner_train = X.iloc[train_idx] if isinstance(X, pd.DataFrame) else X[train_idx]\n",
    "                Y_inner_train = Y.iloc[train_idx] if isinstance(Y, pd.Series) else Y[train_idx]\n",
    "                X_inner_val = X.iloc[val_idx] if isinstance(X, pd.DataFrame) else X[val_idx]\n",
    "\n",
    "                estimator_clone = clone(estimator_template)\n",
    "                # Apply noise to X_inner_train if NOISE_SCALE_FACTOR > 0\n",
    "                X_inner_train_augmented_for_oof_meta_tune = X_inner_train\n",
    "                if NOISE_SCALE_FACTOR > 0:\n",
    "                    # Apply Gaussian noise to the training features for data augmentation.\n",
    "                    # This helps in regularizing the model and improving its generalization\n",
    "                    # by making it robust to small perturbations in the input data.\n",
    "                    X_inner_train_np_for_oof_meta_tune = X_inner_train.values if isinstance(X_inner_train, pd.DataFrame) else X_inner_train\n",
    "                    feature_std_devs_for_oof_meta_tune = np.std(X_inner_train_np_for_oof_meta_tune, axis=0)\n",
    "                    noise_std_devs_for_oof_meta_tune = feature_std_devs_for_oof_meta_tune * NOISE_SCALE_FACTOR + 1e-9\n",
    "                    noise_for_oof_meta_tune = np.random.normal(loc=0.0, scale=noise_std_devs_for_oof_meta_tune, size=X_inner_train_np_for_oof_meta_tune.shape)\n",
    "                    if isinstance(X_inner_train, pd.DataFrame):\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = pd.DataFrame(X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune, index=X_inner_train.index, columns=X_inner_train.columns)\n",
    "                    else:\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune\n",
    "\n",
    "                estimator_clone.fit(X_inner_train_augmented_for_oof_meta_tune, Y_inner_train) # Use augmented data for training\n",
    "                fold_preds[val_idx] = estimator_clone.predict(X_inner_val) # Predict on original (non-augmented) inner_val_set\n",
    "            oof_preds_full_current_iter[:, i] = fold_preds\n",
    "\n",
    "        if np.all(oof_preds_full_current_iter == 0) or np.any(np.isnan(oof_preds_full_current_iter)):\n",
    "            print(\"Warning: Generated OOF predictions are all zeros or contain NaN, skipping meta-learner tuning.\")\n",
    "            global_fixed_meta_learner = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "            ])\n",
    "            global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "        else:\n",
    "            print(\"  Starting meta-learner Bayesian optimization...\")\n",
    "            meta_bayes_search = BayesSearchCV(\n",
    "                estimator=Pipeline([('scaler', StandardScaler()), ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))]),\n",
    "                search_spaces=meta_learner_params,\n",
    "                n_iter=META_LEARNER_N_ITER_BAYESIAN,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                cv=LeaveOneOut(), # Meta-learner internal CV always uses LeaveOneOut.\n",
    "                n_jobs=-1,\n",
    "                random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                verbose=0\n",
    "            )\n",
    "            try:\n",
    "                meta_bayes_search.fit(oof_preds_full_current_iter, Y) # Use full OOF and full Y\n",
    "                print(f\"  Meta-learner Best Score (RMSE): {np.sqrt(-meta_bayes_search.best_score_):.4f}\")\n",
    "                print(f\"  Meta-learner Best Parameters: {dict(meta_bayes_search.best_params_)}\")\n",
    "              \n",
    "                # Initialize and train the final meta-learner with optimized parameters, then fix its coefficients.\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(\n",
    "                        alpha=meta_bayes_search.best_params_.get('elasticnet__alpha', 1.0),\n",
    "                        l1_ratio=meta_bayes_search.best_params_.get('elasticnet__l1_ratio', 0.5),\n",
    "                        random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                        max_iter=2000\n",
    "                    ))\n",
    "                ])\n",
    "                # Train once on the entire OOF dataset to fix coefficients\n",
    "                global_fixed_meta_learner.fit(oof_preds_full_current_iter, Y)\n",
    "              \n",
    "                # Extract and store fixed coefficients\n",
    "                elastic_net_model = global_fixed_meta_learner.named_steps['elasticnet']\n",
    "                global_fixed_meta_coeffs_dict = dict(zip(model_names_available, elastic_net_model.coef_))\n",
    "\n",
    "                print(\"Global best meta-learner initialized with optimized parameters and fixed coefficients.\")\n",
    "                print(f\"Fixed Meta-Learner Coefficients: {global_fixed_meta_coeffs_dict}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Meta-learner tuning or final training failed: {e}\")\n",
    "                print(\"Using default meta-learner parameters, and coefficients cannot be fixed.\")\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "                ])\n",
    "                global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available} # Indicate failure for coeffs\n",
    "\n",
    "print(\"\\n--- Meta-Learner Hyperparameter Tuning Complete ---\")\n",
    "\n",
    "\n",
    "# Define storage for results\n",
    "all_results = []\n",
    "all_plot_data_accumulated = []\n",
    "all_scores_accumulated = [] # New: Stores MAE/RMSE data for all runs\n",
    "\n",
    "# Initialize accumulators for cross-run averages\n",
    "submodel_r2_sums = {name: 0.0 for name in model_names_available}\n",
    "submodel_rmse_sums = {name: 0.0 for name in model_names_available}\n",
    "submodel_mae_sums = {name: 0.0 for name in model_names_available}\n",
    "stacking_rmse_sum = 0.0\n",
    "stacking_mae_sum = 0.0\n",
    "weighted_feature_importances_sums = pd.Series(0.0, index=feature_names_list)\n",
    "# meta_coefficients_sums_across_seeds is redundant as coefficients are now fixed globally\n",
    "valid_runs_count = 0\n",
    "\n",
    "# --- New: List to aggregate all raw SHAP values ---\n",
    "all_aggregated_shap_values_for_swarm_raw_data = [] # Stores raw SHAP data for each model in each run\n",
    "\n",
    "print(f\"\\n=== Starting Repeated Evaluation on {N_SEEDS_FOR_EVALUATION} Leave-{OUTER_CV_P_VALUE}-Out Combinations ===\")\n",
    "for run_idx, test_indices_tuple in enumerate(lpo_combinations): # Iterate through all combinations\n",
    "    test_indices = list(test_indices_tuple) # Convert tuple to list\n",
    "  \n",
    "    # Call the evaluation function, passing the globally fixed meta-learner\n",
    "    result = evaluate_models_manual_cv(run_idx, X, Y, X_index, grid_searches, NOISE_SCALE_FACTOR,\n",
    "                                       fixed_meta_learner_instance=global_fixed_meta_learner, current_test_indices=test_indices)\n",
    "\n",
    "    all_results.append(result)\n",
    "    plot_data = result.get('plot_data', [])\n",
    "    if plot_data:\n",
    "        all_plot_data_accumulated.extend(plot_data)\n",
    "  \n",
    "    # --- New: Accumulate MAE/RMSE data for each run ---\n",
    "    scores_data_for_current_seed = result.get('all_scores_data_for_seed_fold', [])\n",
    "    if scores_data_for_current_seed:\n",
    "        all_scores_accumulated.extend(scores_data_for_current_seed)\n",
    "\n",
    "    # --- Accumulate raw SHAP values and related data (for subsequent swarm plot aggregation) ---\n",
    "    if PLOT_SHAP_SWARM_PLOT and result.get('raw_shap_values') and result.get('X_val_data_for_shap'):\n",
    "        current_seed_raw_shaps = result['raw_shap_values']\n",
    "        current_seed_expected_vals = result['shap_expected_values']\n",
    "        # Note: X_val_data_for_shap is a list of X_val_fold DataFrames for each inner fold within this run\n",
    "        current_X_val_data_folds = result['X_val_data_for_shap']\n",
    "\n",
    "        for model_name in model_names_available:\n",
    "            if model_name in current_seed_raw_shaps:\n",
    "                for fold_idx_inner in range(len(current_seed_raw_shaps[model_name])):\n",
    "                    # Ensure we have corresponding X_val_data for the SHAP values\n",
    "                    if fold_idx_inner < len(current_X_val_data_folds):\n",
    "                        all_aggregated_shap_values_for_swarm_raw_data.append({\n",
    "                            'seed': run_idx,\n",
    "                            'fold_idx': fold_idx_inner, # This fold_idx_inner is always 0 in this script's current structure for outer loop\n",
    "                            'model_name': model_name,\n",
    "                            'shap_values': current_seed_raw_shaps[model_name][fold_idx_inner],\n",
    "                            'expected_value': current_seed_expected_vals[model_name][fold_idx_inner],\n",
    "                            'X_val_data': current_X_val_data_folds[fold_idx_inner]\n",
    "                        })\n",
    "\n",
    "\n",
    "    # --- Accumulate overall results (for final average printing) ---\n",
    "    current_stacking_rmse = result.get('stacking_regressor_rmse_mean', np.nan)\n",
    "    current_stacking_mae = result.get('stacking_regressor_mae_mean', np.nan)\n",
    "  \n",
    "    # Check if all key metrics are valid to decide whether to accumulate\n",
    "    if not np.isnan(current_stacking_rmse) and not np.isnan(current_stacking_mae):\n",
    "        stacking_rmse_sum += current_stacking_rmse\n",
    "        stacking_mae_sum += current_stacking_mae\n",
    "      \n",
    "        for name in model_names_available:\n",
    "             submodel_r2_sums[name] += result.get('submodel_r2_means', {}).get(name, np.nan)\n",
    "             submodel_rmse_sums[name] += result.get('submodel_rmse_means', {}).get(name, np.nan)\n",
    "             submodel_mae_sums[name] += result.get('submodel_mae_means', {}).get(name, np.nan)\n",
    "      \n",
    "        current_weighted_dict = result.get('weighted_feature_importances', {})\n",
    "        if current_weighted_dict:\n",
    "             current_weighted_series = pd.Series(current_weighted_dict)\n",
    "             aligned_importances = current_weighted_series.reindex(feature_names_list).fillna(0.0)\n",
    "             weighted_feature_importances_sums += aligned_importances\n",
    "\n",
    "        valid_runs_count += 1\n",
    "    else:\n",
    "         print(f\"Warning: Stacking Regressor results for Run {run_idx} are NaN, skipping accumulation.\")\n",
    "\n",
    "\n",
    "    # --- Print results for the current run (only MAE) ---\n",
    "    print(f\"\\n--- Run {run_idx} Results Summary ---\")\n",
    "    sr_mae_str = f\"{current_stacking_mae:.4f}\" if not np.isnan(current_stacking_mae) else \"N/A\"\n",
    "    print(f\"  Stacking Regressor MAE Mean: {sr_mae_str}\")\n",
    "    print(\"  Submodel MAE Means:\")\n",
    "    current_sub_mae = result.get('submodel_mae_means', {})\n",
    "    for name in model_names_available:\n",
    "        mae_val = current_sub_mae.get(name, np.nan)\n",
    "        mae_str = f\"{mae_val:.4f}\" if not np.isnan(mae_val) else \"N/A\"\n",
    "        print(f\"    {name}: MAE Mean = {mae_str}\")\n",
    "\n",
    "    # Print fixed meta-learner coefficients\n",
    "    if global_fixed_meta_coeffs_dict:\n",
    "        print(\"  Meta-Learner Coefficients (Fixed Global):\")\n",
    "        for name, coeff in global_fixed_meta_coeffs_dict.items():\n",
    "            print(f\"    {name}: {coeff:.4f}\")\n",
    "\n",
    "\n",
    "# --- Calculate Average Results Across All Runs ---\n",
    "if valid_runs_count > 0:\n",
    "    # avg_stacking_regressor_rmse_mean = stacking_rmse_sum / valid_runs_count # Removed\n",
    "    avg_stacking_regressor_mae_mean = stacking_mae_sum / valid_runs_count\n",
    "    # avg_submodel_r2_means = {name: total / valid_runs_count for name, total in submodel_r2_sums.items()} # Removed\n",
    "    # avg_submodel_rmse_means = {name: total / valid_runs_count for name, total in submodel_rmse_sums.items()} # Removed\n",
    "    avg_submodel_mae_means = {name: total / valid_runs_count for name, total in submodel_mae_sums.items()}\n",
    "    avg_weighted_feature_importances_series = weighted_feature_importances_sums / valid_runs_count\n",
    "  \n",
    "    # Average meta-learner coefficients are now the fixed global coefficients\n",
    "    avg_meta_coefficients_final = global_fixed_meta_coeffs_dict\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Average Results Across All {valid_runs_count} Valid Runs ===\")\n",
    "    # print(f\"Average Stacking Regressor RMSE Mean: {avg_stacking_regressor_rmse_mean:.4f}\") # Removed\n",
    "    print(f\"Average Stacking Regressor MAE Mean: {avg_stacking_regressor_mae_mean:.4f}\")\n",
    "    print(\"\\nAverage Submodel MAE Means:\") # Modified to only mention MAE\n",
    "    for name in model_names_available:\n",
    "        # r2_avg = avg_submodel_r2_means.get(name, np.nan) # Removed\n",
    "        # rmse_avg = avg_submodel_rmse_means.get(name, np.nan) # Removed\n",
    "        mae_avg = avg_submodel_mae_means.get(name, np.nan)\n",
    "        # r2_avg_str = f\"{r2_avg:.4f}\" if not np.isnan(r2_avg) else \"N/A\" # Removed\n",
    "        # rmse_avg_str = f\"{rmse_avg:.4f}\" if not np.isnan(rmse_avg) else \"N/A\" # Removed\n",
    "        mae_avg_str = f\"{mae_avg:.4f}\" if not np.isnan(mae_avg) else \"N/A\"\n",
    "        print(f\"  {name}: MAE Mean = {mae_avg_str}\") # Modified to only print MAE\n",
    "\n",
    "    print(\"\\nFixed Global Meta-Learner Coefficients:\")\n",
    "    if avg_meta_coefficients_final:\n",
    "        for name, coeff in avg_meta_coefficients_final.items():\n",
    "            print(f\"  {name}: {coeff:.4f}\")\n",
    "    else:\n",
    "        print(\"  (Meta-learner coefficients not successfully fixed)\")\n",
    "\n",
    "    # --- Process and Output Feature Importances (Bar Plot) ---\n",
    "    avg_weighted_feature_importances_dict = avg_weighted_feature_importances_series.to_dict()\n",
    "    sorted_feature_importances = sorted(avg_weighted_feature_importances_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(\"\\nAverage Weighted Feature Importances (Sorted by SHAP):\")\n",
    "    for feature, importance in sorted_feature_importances:\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "    try:\n",
    "        features_to_plot = [item[0] for item in sorted_feature_importances[:N_FEATURES_TO_PLOT]]\n",
    "        importances_to_plot = [item[1] for item in sorted_feature_importances[:N_FEATURES_TO_PLOT]]\n",
    "        plt.figure(figsize=(10, max(6, N_FEATURES_TO_PLOT * 0.4)))\n",
    "        sns.barplot(x=importances_to_plot, y=features_to_plot, palette=\"viridis\")\n",
    "        plt.xlabel('Average Weighted Feature Importance (exp(-RMSE) Weight, From CV Runs, SHAP)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Top {N_FEATURES_TO_PLOT} Feature Importances Averaged Over {valid_runs_count} Runs (SHAP)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError plotting feature importance: {e}\")\n",
    "\n",
    "    # --- Plot Weighted Average Combined SHAP Swarm Plot ---\n",
    "    if PLOT_SHAP_SWARM_PLOT:\n",
    "        print(\"\\n--- Aggregating raw SHAP data and plotting weighted average combined SHAP swarm plot (exp(-RMSE) weighted) ---\")\n",
    "      \n",
    "        # 1. Calculate exp(-RMSE) weights\n",
    "        shap_exp_rmse_weights = {}\n",
    "        total_exp_rmse_sum_for_shap_weights = 0.0\n",
    "        for name in model_names_available:\n",
    "            rmse_val = avg_submodel_rmse_means.get(name, np.nan)\n",
    "            if not np.isnan(rmse_val):\n",
    "                weight = np.exp(-rmse_val)\n",
    "                shap_exp_rmse_weights[name] = weight\n",
    "                total_exp_rmse_sum_for_shap_weights += weight\n",
    "            else:\n",
    "                shap_exp_rmse_weights[name] = 0.0\n",
    "\n",
    "        # Normalize weights\n",
    "        if total_exp_rmse_sum_for_shap_weights > 1e-9:\n",
    "            for name in shap_exp_rmse_weights:\n",
    "                shap_exp_rmse_weights[name] /= total_exp_rmse_sum_for_shap_weights\n",
    "            print(f\"  exp(-RMSE) weighting coefficients (normalized): {shap_exp_rmse_weights}\")\n",
    "        else:\n",
    "            print(\"Warning: Average RMSE for all submodels resulted in invalid exp(-RMSE) weights. Skipping swarm plot.\")\n",
    "            PLOT_SHAP_SWARM_PLOT = False # Disable swarm plot\n",
    "          \n",
    "        if PLOT_SHAP_SWARM_PLOT: # Re-check if disabled\n",
    "            # 2. Aggregate weighted SHAP values and X data from all runs\n",
    "            final_shap_values_for_plot_list = []\n",
    "            final_X_data_for_plot_list = []\n",
    "            final_expected_values_for_plot_list = []\n",
    "\n",
    "            # Iterate through all collected raw SHAP data\n",
    "            # all_aggregated_shap_values_for_swarm_raw_data contains raw SHAP data for each model in each run.\n",
    "            # We need to group by (run_idx, fold_idx) and then weight models within each group.\n",
    "          \n",
    "            # First, group by (run_idx, fold_idx)\n",
    "            grouped_shap_data = {}\n",
    "            for item in all_aggregated_shap_values_for_swarm_raw_data:\n",
    "                key = (item['seed'], item['fold_idx'])\n",
    "                if key not in grouped_shap_data:\n",
    "                    grouped_shap_data[key] = {}\n",
    "                grouped_shap_data[key][item['model_name']] = {\n",
    "                    'shap_values': item['shap_values'],\n",
    "                    'expected_value': item['expected_value'],\n",
    "                    'X_val_data': item['X_val_data']\n",
    "                }\n",
    "\n",
    "            for (run_key, fold_key), models_data in grouped_shap_data.items():\n",
    "                # Ensure at least one model successfully calculated SHAP values, otherwise skip\n",
    "                first_valid_model_shap_found = False\n",
    "                for name in model_names_available:\n",
    "                    if name in models_data and models_data[name]['shap_values'].shape[0] > 0:\n",
    "                        weighted_shap_for_fold = np.zeros_like(models_data[name]['shap_values'])\n",
    "                        weighted_expected_value_for_fold = 0.0\n",
    "                        current_X_val_fold = models_data[name]['X_val_data']\n",
    "                        first_valid_model_shap_found = True\n",
    "                        break\n",
    "                if not first_valid_model_shap_found:\n",
    "                    print(f\"  Warning: Run/Fold {run_key}/{fold_key+1} has no valid SHAP data, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                valid_model_shaps_in_fold = 0\n",
    "                for name in model_names_available:\n",
    "                    if name in models_data:\n",
    "                        weight = shap_exp_rmse_weights.get(name, 0.0)\n",
    "                        shap_vals = models_data[name]['shap_values']\n",
    "                        exp_val = models_data[name]['expected_value']\n",
    "\n",
    "                        # Ensure shap_vals and weighted_shap_for_fold dimensions match\n",
    "                        if shap_vals.shape == weighted_shap_for_fold.shape:\n",
    "                            weighted_shap_for_fold += weight * shap_vals\n",
    "                            weighted_expected_value_for_fold += weight * exp_val\n",
    "                            valid_model_shaps_in_fold += 1\n",
    "                        else:\n",
    "                            print(f\"  Warning: Run/Fold {run_key}/{fold_key+1}, Model {name}'s SHAP value dimension mismatch, skipping weighting.\")\n",
    "              \n",
    "                if valid_model_shaps_in_fold > 0:\n",
    "                    final_shap_values_for_plot_list.append(weighted_shap_for_fold)\n",
    "                    final_X_data_for_plot_list.append(current_X_val_fold)\n",
    "                    # Each sample corresponds to this weighted expected value\n",
    "                    final_expected_values_for_plot_list.extend([weighted_expected_value_for_fold] * current_X_val_fold.shape[0])\n",
    "          \n",
    "            if not final_shap_values_for_plot_list:\n",
    "                print(\"Warning: No valid weighted SHAP data collected, cannot plot swarm plot.\")\n",
    "            else:\n",
    "                try:\n",
    "                    final_shap_values_for_plot = np.vstack(final_shap_values_for_plot_list)\n",
    "                    final_X_data_for_plot = pd.concat(final_X_data_for_plot_list)\n",
    "                  \n",
    "                    # Limit sample count to prevent memory issues or slow plotting\n",
    "                    if SHAP_SWARM_SAMPLES_LIMIT is not None and final_shap_values_for_plot.shape[0] > SHAP_SWARM_SAMPLES_LIMIT:\n",
    "                        print(f\"  Warning: Too many samples for SHAP swarm plot ({final_shap_values_for_plot.shape[0]}), randomly sampling {SHAP_SWARM_SAMPLES_LIMIT} samples.\")\n",
    "                        rng = np.random.default_rng(DEFAULT_MODEL_RANDOM_STATE)\n",
    "                        sample_indices = rng.choice(final_shap_values_for_plot.shape[0], SHAP_SWARM_SAMPLES_LIMIT, replace=False)\n",
    "                        final_shap_values_for_plot = final_shap_values_for_plot[sample_indices]\n",
    "                        final_X_data_for_plot = final_X_data_for_plot.iloc[sample_indices]\n",
    "                        final_expected_values_for_plot_sampled = np.array(final_expected_values_for_plot_list)[sample_indices]\n",
    "                        final_base_value = np.mean(final_expected_values_for_plot_sampled)\n",
    "                    else:\n",
    "                        final_base_value = np.mean(final_expected_values_for_plot_list)\n",
    "\n",
    "                    # --- New: Export aggregated swarm plot data (fourth Excel) ---\n",
    "                    print(\"\\n--- Exporting aggregated SHAP swarm plot data to Excel/CSV ---\")\n",
    "                    try:\n",
    "                        # Create DataFrame for SHAP values\n",
    "                        shap_values_df = pd.DataFrame(final_shap_values_for_plot, columns=[f'SHAP_{col}' for col in feature_names_list])\n",
    "\n",
    "                        # Combine original feature data and SHAP values\n",
    "                        # Ensure indices are aligned if X_data was a DataFrame\n",
    "                        combined_shap_data_df = pd.concat([final_X_data_for_plot.reset_index(drop=True), shap_values_df], axis=1)\n",
    "\n",
    "                        # Add the base value as a constant column\n",
    "                        combined_shap_data_df['SHAP_Base_Value'] = final_base_value\n",
    "\n",
    "                        # --- START MODIFICATION FOR SHAP EXCEL/CSV SORTING ---\n",
    "                        # Calculate mean absolute value for each SHAP feature\n",
    "                        shap_feature_columns = [col for col in combined_shap_data_df.columns if col.startswith('SHAP_')]\n",
    "                        if shap_feature_columns: # Ensure there are SHAP columns to sort\n",
    "                            mean_abs_shap_values = combined_shap_data_df[shap_feature_columns].abs().mean().sort_values(ascending=False)\n",
    "                            # Get sorted SHAP feature column names\n",
    "                            sorted_shap_columns = mean_abs_shap_values.index.tolist()\n",
    "\n",
    "                            # Get non-SHAP feature column names (e.g., Seed, Fold, Sample_Index, Actual, SHAP_Base_Value etc.)\n",
    "                            non_shap_columns = [col for col in combined_shap_data_df.columns if not col.startswith('SHAP_')]\n",
    "                            # Ensure SHAP_Base_Value is at the end, or any desired position\n",
    "                            if 'SHAP_Base_Value' in non_shap_columns:\n",
    "                                non_shap_columns.remove('SHAP_Base_Value')\n",
    "                                non_shap_columns.append('SHAP_Base_Value')\n",
    "\n",
    "                            # Construct final column order: non-SHAP columns + sorted SHAP columns\n",
    "                            final_column_order = non_shap_columns + sorted_shap_columns\n",
    "                            # Reorder DataFrame columns\n",
    "                            combined_shap_data_df = combined_shap_data_df[final_column_order]\n",
    "                        # --- END MODIFICATION FOR SHAP EXCEL/CSV SORTING ---\n",
    "\n",
    "                        excel_filename_shap_data = f'shap_swarm_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "                        combined_shap_data_df.to_excel(excel_filename_shap_data, index=False, engine='openpyxl')\n",
    "                        print(f\"Successfully exported aggregated SHAP swarm plot data to: {excel_filename_shap_data}\")\n",
    "\n",
    "                    except ImportError:\n",
    "                        print(\"!!! Exporting to Excel requires 'openpyxl' library. Attempting to export as CSV file...\")\n",
    "                        try:\n",
    "                            # Re-create combined_shap_data_df if needed for CSV fallback\n",
    "                            # This block is executed if openpyxl is not found, so combined_shap_data_df might not be fully processed yet\n",
    "                            if 'combined_shap_data_df' not in locals() or combined_shap_data_df.empty:\n",
    "                                shap_values_df = pd.DataFrame(final_shap_values_for_plot, columns=[f'SHAP_{col}' for col in feature_names_list])\n",
    "                                combined_shap_data_df = pd.concat([final_X_data_for_plot.reset_index(drop=True), shap_values_df], axis=1)\n",
    "                                combined_shap_data_df['SHAP_Base_Value'] = final_base_value\n",
    "\n",
    "                            # --- START MODIFICATION FOR SHAP CSV SORTING (DUPLICATE LOGIC) ---\n",
    "                            shap_feature_columns = [col for col in combined_shap_data_df.columns if col.startswith('SHAP_')]\n",
    "                            if shap_feature_columns: # Ensure there are SHAP columns to sort\n",
    "                                mean_abs_shap_values = combined_shap_data_df[shap_feature_columns].abs().mean().sort_values(ascending=False)\n",
    "                                sorted_shap_columns = mean_abs_shap_values.index.tolist()\n",
    "                                non_shap_columns = [col for col in combined_shap_data_df.columns if not col.startswith('SHAP_')]\n",
    "                                if 'SHAP_Base_Value' in non_shap_columns:\n",
    "                                    non_shap_columns.remove('SHAP_Base_Value')\n",
    "                                    non_shap_columns.append('SHAP_Base_Value')\n",
    "                                final_column_order = non_shap_columns + sorted_shap_columns\n",
    "                                combined_shap_data_df = combined_shap_data_df[final_column_order]\n",
    "                            # --- END MODIFICATION FOR SHAP CSV SORTING ---\n",
    "\n",
    "                            csv_filename_shap_data = f'shap_swarm_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "                            combined_shap_data_df.to_csv(csv_filename_shap_data, index=False)\n",
    "                            print(f\"Successfully exported SHAP swarm plot data as CSV: {csv_filename_shap_data}\")\n",
    "                        except Exception as e_csv:\n",
    "                            print(f\"!!! Error exporting SHAP swarm plot data to CSV: {e_csv}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"!!! Other error occurred while exporting aggregated SHAP swarm plot data to Excel: {e}\")\n",
    "                        print(\"Attempting to export SHAP swarm plot data as CSV file...\")\n",
    "                        try:\n",
    "                            if 'combined_shap_data_df' not in locals() or combined_shap_data_df.empty:\n",
    "                                shap_values_df = pd.DataFrame(final_shap_values_for_plot, columns=[f'SHAP_{col}' for col in feature_names_list])\n",
    "                                combined_shap_data_df = pd.concat([final_X_data_for_plot.reset_index(drop=True), shap_values_df], axis=1)\n",
    "                                combined_shap_data_df['SHAP_Base_Value'] = final_base_value\n",
    "\n",
    "                            # --- START MODIFICATION FOR SHAP CSV SORTING (DUPLICATE LOGIC) ---\n",
    "                            shap_feature_columns = [col for col in combined_shap_data_df.columns if col.startswith('SHAP_')]\n",
    "                            if shap_feature_columns: # Ensure there are SHAP columns to sort\n",
    "                                mean_abs_shap_values = combined_shap_data_df[shap_feature_columns].abs().mean().sort_values(ascending=False)\n",
    "                                sorted_shap_columns = mean_abs_shap_values.index.tolist()\n",
    "                                non_shap_columns = [col for col in combined_shap_data_df.columns if not col.startswith('SHAP_')]\n",
    "                                if 'SHAP_Base_Value' in non_shap_columns:\n",
    "                                    non_shap_columns.remove('SHAP_Base_Value')\n",
    "                                    non_shap_columns.append('SHAP_Base_Value')\n",
    "                                final_column_order = non_shap_columns + sorted_shap_columns\n",
    "                                combined_shap_data_df = combined_shap_data_df[final_column_order]\n",
    "                            # --- END MODIFICATION FOR SHAP CSV SORTING ---\n",
    "\n",
    "                            csv_filename_shap_data = f'shap_swarm_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "                            combined_shap_data_df.to_csv(csv_filename_shap_data, index=False)\n",
    "                            print(f\"Successfully exported SHAP swarm plot data as CSV: {csv_filename_shap_data}\")\n",
    "                        except Exception as e_csv:\n",
    "                            print(f\"!!! Error exporting SHAP swarm plot data to CSV: {e_csv}\")\n",
    "                    # --- End Export ---\n",
    "\n",
    "                    # Create shap.Explanation object\n",
    "                    shap_explanation = shap.Explanation(\n",
    "                        values=final_shap_values_for_plot,\n",
    "                        base_values=final_base_value,\n",
    "                        data=final_X_data_for_plot.values, # shap.Explanation expects numpy array\n",
    "                        feature_names=feature_names_list\n",
    "                    )\n",
    "\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    shap.summary_plot(shap_explanation, final_X_data_for_plot, plot_type=\"dot\", show=False) # plot_type=\"dot\" is the swarm plot\n",
    "                    plt.title(f'exp(-RMSE) Weighted Average SHAP Values for Stacking Model (Aggregated over {valid_runs_count} Runs)')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError plotting SHAP swarm plot: {e}\")\n",
    "        else:\n",
    "            print(\"\\nWarning: Swarm plot not generated due to invalid exp(-RMSE) weights.\")\n",
    "    else:\n",
    "        print(\"\\nWarning: PLOT_SHAP_SWARM_PLOT is set to False, skipping swarm plot generation.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nError: No runs completed successfully, cannot calculate average results or generate plots.\")\n",
    "\n",
    "\n",
    "# --- Export predictions in wide format (first Excel) ---\n",
    "print(\"\\n--- Exporting prediction results data to Excel/CSV (pivoted by model) ---\")\n",
    "if all_plot_data_accumulated:\n",
    "    try:\n",
    "        plot_data_df_for_pivot = pd.DataFrame(all_plot_data_accumulated)\n",
    "\n",
    "        if 'Sample_Index' not in plot_data_df_for_pivot.columns:\n",
    "            print(\"Error: 'Sample_Index' column is missing from plot data, cannot perform pivoted export. Please check evaluate_models_manual_cv function.\")\n",
    "            raise KeyError(\"'Sample_Index' not found in plot data\")\n",
    "\n",
    "        print(\"    Pivoting data to have models as columns...\")\n",
    "        # Note: The 'Fold' column is now unified to 0, so pivot only by Seed, Sample_Index, Actual\n",
    "        export_df_pivoted = pd.pivot_table(\n",
    "            plot_data_df_for_pivot,\n",
    "            index=['Seed', 'Sample_Index', 'Actual'], # <-- Removed Fold\n",
    "            columns=['Model', 'Set'],\n",
    "            values='Predicted'\n",
    "        )\n",
    "\n",
    "        export_df_pivoted = export_df_pivoted.reset_index()\n",
    "\n",
    "        new_columns = []\n",
    "        for col in export_df_pivoted.columns.values:\n",
    "            if isinstance(col, tuple):\n",
    "                flat_name = '_'.join(filter(None, col)).strip('_')\n",
    "                new_columns.append(flat_name + '_Pred')\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "        export_df_pivoted.columns = new_columns\n",
    "        print(\"    Data pivoting and column name cleaning complete.\")\n",
    "\n",
    "\n",
    "        excel_filename = f'cv_predictions_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "        export_df_pivoted.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Successfully exported pivoted data by model to: {excel_filename}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"!!! Exporting to Excel requires 'openpyxl' library. Please run 'pip install openpyxl'.\")\n",
    "        print(\"Attempting to export pivoted data by model as CSV file...\")\n",
    "        try:\n",
    "            if 'export_df_pivoted' not in locals():\n",
    "                 if 'plot_data_df_for_pivot' not in locals(): plot_data_df_for_pivot = pd.DataFrame(all_plot_data_accumulated)\n",
    "                 if 'Sample_Index' not in plot_data_df_for_pivot.columns: raise KeyError(\"Sample_Index missing\")\n",
    "                 export_df_pivoted = pd.pivot_table(plot_data_df_for_pivot, index=['Seed', 'Sample_Index', 'Actual'], columns=['Model', 'Set'], values='Predicted') # <-- Removed Fold\n",
    "                 export_df_pivoted = export_df_pivoted.reset_index()\n",
    "                 new_columns = ['_'.join(filter(None, col)).strip('_')+'_Pred' if isinstance(col, tuple) else col for col in export_df_pivoted.columns.values]\n",
    "                 export_df_pivoted.columns = new_columns\n",
    "\n",
    "            csv_filename = f'cv_predictions_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            export_df_pivoted.to_csv(csv_filename, index=False)\n",
    "            print(f\"Successfully exported data as CSV: {csv_filename}\")\n",
    "        except KeyError:\n",
    "             print(\"!!! Pivoting and CSV export failed due to missing Sample_Index.\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "\n",
    "    except KeyError as e_key:\n",
    "        print(f\"!!! Data pivoting failed due to missing required column: {e_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Other error occurred while exporting pivoted data by model to Excel: {e}\")\n",
    "        print(\"Attempting to export pivoted data by model as CSV file...\")\n",
    "        try:\n",
    "            if 'export_df_pivoted' not in locals():\n",
    "                 if 'plot_data_df_for_pivot' not in locals(): plot_data_df_for_pivot = pd.DataFrame(all_plot_data_accumulated)\n",
    "                 if 'Sample_Index' not in plot_data_df_for_pivot.columns: raise KeyError(\"Sample_Index missing\")\n",
    "                 export_df_pivoted = pd.pivot_table(plot_data_df_for_pivot, index=['Seed', 'Sample_Index', 'Actual'], columns=['Model', 'Set'], values='Predicted') # <-- Removed Fold\n",
    "                 export_df_pivoted = export_df_pivoted.reset_index()\n",
    "                 new_columns = ['_'.join(filter(None, col)).strip('_')+'_Pred' if isinstance(col, tuple) else col for col in export_df_pivoted.columns.values]\n",
    "                 export_df_pivoted.columns = new_columns\n",
    "\n",
    "            csv_filename = f'cv_predictions_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            export_df_pivoted.to_csv(csv_filename, index=False)\n",
    "            print(f\"Successfully exported data as CSV: {csv_filename}\")\n",
    "        except KeyError:\n",
    "             print(\"!!! Pivoting and CSV export failed due to missing Sample_Index.\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "else:\n",
    "    print(\"No data collected for plotting or export.\")\n",
    "\n",
    "# --- Export MAE/RMSE data per run (second Excel) ---\n",
    "print(\"\\n--- Exporting MAE and RMSE data per run to Excel/CSV (pivoted by model) ---\")\n",
    "if all_scores_accumulated:\n",
    "    try:\n",
    "        scores_df = pd.DataFrame(all_scores_accumulated)\n",
    "      \n",
    "        # First pivot: Convert 'Metric' column to columns\n",
    "        scores_df_temp = scores_df.pivot_table(\n",
    "            index=['Seed', 'Model'], # <-- Removed Fold\n",
    "            columns='Metric',\n",
    "            values='Value'\n",
    "        ).reset_index()\n",
    "      \n",
    "        # Second pivot: Convert 'Model' column to columns, and create new column names for MAE and RMSE\n",
    "        scores_df_pivoted = scores_df_temp.pivot_table(\n",
    "            index=['Seed'], # <-- Removed Fold\n",
    "            columns='Model',\n",
    "            values=['MAE', 'RMSE'] # <-- Select only MAE and RMSE\n",
    "        )\n",
    "      \n",
    "        # Clean multi-level column names, e.g., ('MAE', 'XGBR') becomes 'XGBR_MAE'\n",
    "        scores_df_pivoted.columns = [f'{col[1]}_{col[0]}' for col in scores_df_pivoted.columns]\n",
    "        scores_df_pivoted = scores_df_pivoted.reset_index()\n",
    "      \n",
    "        excel_filename_scores = f'cv_model_scores_per_run_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "        scores_df_pivoted.to_excel(excel_filename_scores, index=False, engine='openpyxl')\n",
    "        print(f\"Successfully exported MAE and RMSE data per run to: {excel_filename_scores}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"!!! Exporting to Excel requires 'openpyxl' library. Please run 'pip install openpyxl'。\")\n",
    "        print(\"Attempting to export MAE and RMSE data as CSV file...\")\n",
    "        try:\n",
    "            if 'scores_df_pivoted' not in locals():\n",
    "                scores_df = pd.DataFrame(all_scores_accumulated)\n",
    "                scores_df_temp = scores_df.pivot_table(\n",
    "                    index=['Seed', 'Model'], # <-- Removed Fold\n",
    "                    columns='Metric',\n",
    "                    values='Value'\n",
    "                ).reset_index()\n",
    "                scores_df_pivoted = scores_df_temp.pivot_table(\n",
    "                    index=['Seed'], # <-- Removed Fold\n",
    "                    columns='Model',\n",
    "                    values=['MAE', 'RMSE'] # <-- Select only MAE and RMSE\n",
    "                )\n",
    "                scores_df_pivoted.columns = [f'{col[1]}_{col[0]}' for col in scores_df_pivoted.columns]\n",
    "                scores_df_pivoted = scores_df_pivoted.reset_index()\n",
    "\n",
    "            csv_filename_scores = f'cv_model_scores_per_run_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            scores_df_pivoted.to_csv(csv_filename_scores, index=False)\n",
    "            print(f\"Successfully exported MAE and RMSE data as CSV: {csv_filename_scores}\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Other error occurred while exporting MAE and RMSE data per run to Excel: {e}\")\n",
    "        print(\"Attempting to export MAE and RMSE data as CSV file...\")\n",
    "        try:\n",
    "            if 'scores_df_pivoted' not in locals():\n",
    "                scores_df = pd.DataFrame(all_scores_accumulated)\n",
    "                scores_df_temp = scores_df.pivot_table(\n",
    "                    index=['Seed', 'Model'], # <-- Removed Fold\n",
    "                    columns='Metric',\n",
    "                    values='Value'\n",
    "                ).reset_index()\n",
    "                scores_df_pivoted = scores_df_temp.pivot_table(\n",
    "                    index=['Seed'], # <-- Removed Fold\n",
    "                    columns='Model',\n",
    "                    values=['MAE', 'RMSE'] # <-- Select only MAE and RMSE\n",
    "                )\n",
    "                scores_df_pivoted.columns = [f'{col[1]}_{col[0]}' for col in scores_df_pivoted.columns]\n",
    "                scores_df_pivoted = scores_df_pivoted.reset_index()\n",
    "\n",
    "            csv_filename_scores = f'cv_model_scores_per_run_pivoted_{time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            scores_df_pivoted.to_csv(csv_filename_scores, index=False)\n",
    "            print(f\"Successfully exported MAE and RMSE data as CSV: {csv_filename_scores}\")\n",
    "        except Exception as e_csv:\n",
    "            print(f\"!!! Error exporting to CSV: {e_csv}\")\n",
    "else:\n",
    "    print(\"No data collected for MAE and RMSE export.\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d89dd1-a04d-4225-b914-8b38daa03d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_unknown_data.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys # Used for sys.exit()\n",
    "import warnings\n",
    "\n",
    "# Model imports\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# Import StackingRegressor and meta-learner components\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import CatBoost and LightGBM (if available)\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: CatBoost is not installed. Skipping CBR model.\")\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    print(\"Info: LightGBM is not installed. Skipping LGBM model.\")\n",
    "    lightgbm_available = False\n",
    "\n",
    "# Scikit-learn Utilities\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import LeaveOneOut # Used for meta-learner internal CV during re-training\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "### --- Adjustable Configuration Parameters --- ###\n",
    "\n",
    "# --- 1. Unknown Data File Path ---\n",
    "UNKNOWN_DATA_FILE = 'Band alignment-prediction.xlsx' # Path to your Excel file containing unknown data for prediction.\n",
    "\n",
    "# --- 2. Prediction Results Output File Path ---\n",
    "OUTPUT_PREDICTIONS_FILE = f'predictions_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx' # Output path for the prediction results file.\n",
    "\n",
    "# --- 3. Default Random State (for model initialization, ensuring reproducibility) ---\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 # Ensures reproducibility of results across runs.\n",
    "\n",
    "# --- 4. Sub-model Selection Interface ---\n",
    "# List the names of the sub-models you wish to include in the Stacking model for prediction.\n",
    "# These models must have been successfully tuned in the previous optimization script.\n",
    "# Available model names typically include: 'XGBR', 'RF', 'GBRT', 'ETR', 'HGBR', 'CBR', 'LGBM'\n",
    "# You can easily exclude a model from the stacking prediction by commenting out its entry in this list.\n",
    "ENABLED_SUBMODELS_LIST = [\n",
    "    'XGBR',\n",
    "    'RF',\n",
    "    'GBRT',\n",
    "    'ETR',\n",
    "    'HGBR',\n",
    "    'CBR',\n",
    "    'LGBM'\n",
    "]\n",
    "# Ensure models in ENABLED_SUBMODELS_LIST are actually available\n",
    "if 'CBR' in ENABLED_SUBMODELS_LIST and not catboost_available:\n",
    "    print(\"Warning: CatBoost is not installed. 'CBR' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('CBR')\n",
    "if 'LGBM' in ENABLED_SUBMODELS_LIST and not lightgbm_available:\n",
    "    print(\"Warning: LightGBM is not installed. 'LGBM' removed from ENABLED_SUBMODELS_LIST.\")\n",
    "    ENABLED_SUBMODELS_LIST.remove('LGBM')\n",
    "\n",
    "# --- 5. Meta-Learner Bayesian Optimization Iterations (if re-training is needed) ---\n",
    "META_LEARNER_N_ITER_BAYESIAN = 50 # Number of Bayesian optimization iterations for the meta-learner.\n",
    "                                 # This is only used if the meta-learner is not found in memory and needs re-training.\n",
    "\n",
    "# --- 6. Noise Intensity Control Interface (if re-training is needed) ---\n",
    "NOISE_SCALE_FACTOR = 0     # Factor for data augmentation (Gaussian noise). Set to 0 to disable.\n",
    "                           # This parameter is used if the meta-learner needs to be re-trained within this script.\n",
    "                           # Adding noise to the training data can help regularize the model and\n",
    "                           # prevent overfitting, especially beneficial for small datasets.\n",
    "                           # By tuning this factor, one can potentially mitigate overfitting.\n",
    "                           # In this specific work, noise augmentation is not utilized (factor is 0).\n",
    "\n",
    "### --- End Configuration Parameters --- ###\n",
    "\n",
    "print(\"\\n--- Starting Prediction on Unknown Data ---\")\n",
    "\n",
    "# --- 0. Mandatory Check: Ensure training data X, Y and base model optimization results grid_searches are in memory ---\n",
    "# These variables are expected to be generated by the previous model tuning script in the same Python session.\n",
    "\n",
    "# Check if X (training features) exists\n",
    "if 'X' not in globals():\n",
    "    print(\"\\nFatal Error: Variable 'X' (training feature set) is not defined in the current environment.\")\n",
    "    print(\"Please ensure you have run the model tuning script in the same Python session.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    X = globals()['X'] # Retrieve X from the global scope\n",
    "\n",
    "# Check if Y (training target) exists\n",
    "if 'Y' not in globals():\n",
    "    print(\"\\nFatal Error: Variable 'Y' (training target set) is not defined in the current environment.\")\n",
    "    print(\"Please ensure you have run the model tuning script in the same Python session.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    Y = globals()['Y'] # Retrieve Y from the global scope\n",
    "\n",
    "# Check if grid_searches (base model optimization results) exists\n",
    "if 'grid_searches' not in globals():\n",
    "    print(\"\\nFatal Error: Variable 'grid_searches' (base model optimization results) is not defined in the current environment.\")\n",
    "    print(\"Please ensure you have run the model tuning script in the same Python session.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    grid_searches = globals()['grid_searches'] # Retrieve grid_searches from the global scope\n",
    "    if not grid_searches: # Check if grid_searches is an empty dictionary\n",
    "        print(\"\\nFatal Error: 'grid_searches' is empty. No base models were successfully tuned.\")\n",
    "        print(\"Please check the execution results of the model tuning script.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"All required training data and base model optimization results loaded successfully.\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def initialize_best_estimators(grid_searches_dict, enabled_models_list):\n",
    "    \"\"\"\n",
    "    Initializes best base learner instances from BayesSearchCV results, filtered by the enabled_models_list.\n",
    "    This function is used to retrieve the optimized base models for stacking.\n",
    "\n",
    "    Args:\n",
    "        grid_searches_dict (dict): A dictionary containing BayesSearchCV results for each model.\n",
    "        enabled_models_list (list): A list of model names that are enabled for stacking.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of initialized best estimator instances.\n",
    "    \"\"\"\n",
    "    estimators_init = {}\n",
    "    # Define all possible models and their default parameters\n",
    "    all_possible_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "\n",
    "    if catboost_available:\n",
    "        all_possible_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available:\n",
    "        all_possible_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing base models...\")\n",
    "    for name, (model_class, fixed_params) in all_possible_models.items():\n",
    "        if name not in enabled_models_list: # If the model is not in the enabled list, skip it\n",
    "            print(f\"  Model {name} is not in ENABLED_SUBMODELS_LIST, skipping initialization.\")\n",
    "            continue\n",
    "\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} with optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "        else:\n",
    "            print(f\"  No optimized results found for {name}. Attempting to initialize with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} with default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (with default parameters): {e}. This model will be skipped.\")\n",
    "                estimators_init[name] = None\n",
    "\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v is not None}\n",
    "    if not initialized_estimators:\n",
    "         print(\"Warning: No base models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "# --- Define Meta-Learner Hyperparameter Search Space (copied from evaluate_stacking_model.py) ---\n",
    "meta_learner_params = {\n",
    "    'elasticnet__alpha': Real(1e-5, 10.0, prior='log-uniform', name='elasticnet__alpha'),\n",
    "    'elasticnet__l1_ratio': Real(0.0, 1.0, prior='uniform', name='elasticnet__l1_ratio')\n",
    "}\n",
    "\n",
    "# --- Check and Retrieve or Train global_fixed_meta_learner ---\n",
    "global_fixed_meta_learner = None\n",
    "global_fixed_meta_coeffs_dict = None # Stores the final fixed coefficients for printing and inspection.\n",
    "\n",
    "# Attempt to load the pre-trained meta-learner from memory (if it was passed from the evaluation script)\n",
    "if 'global_fixed_meta_learner' in globals() and isinstance(globals()['global_fixed_meta_learner'], Pipeline):\n",
    "    global_fixed_meta_learner = globals()['global_fixed_meta_learner']\n",
    "    print(\"\\n--- Successfully loaded pre-trained meta-learner from memory (from evaluation script). ---\")\n",
    "    # Attempt to retrieve coefficients, if passed by the evaluation script\n",
    "    if 'global_fixed_meta_coeffs_dict' in globals():\n",
    "        global_fixed_meta_coeffs_dict = globals()['global_fixed_meta_coeffs_dict']\n",
    "        print(f\"  Loaded meta-learner coefficients: {global_fixed_meta_coeffs_dict}\")\n",
    "    else:\n",
    "        print(\"  Meta-learner coefficients not found, but the meta-learner itself was loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Pre-trained meta-learner not found, starting meta-learner hyperparameter tuning and training. ---\")\n",
    "    # Get available model names (filtered by ENABLED_SUBMODELS_LIST)\n",
    "    try:\n",
    "        temp_base_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "        model_names_available = list(temp_base_estimators.keys())\n",
    "        if not model_names_available:\n",
    "            print(\"Error: No models successfully initialized, cannot proceed with meta-learner training. Please check ENABLED_SUBMODELS_LIST.\")\n",
    "            sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred while trying to initialize models to get names: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Meta-Learner Hyperparameter Pre-tuning Stage (copied from evaluate_stacking_model.py) ---\n",
    "    print(f\"\\n=== Starting Meta-Learner Hyperparameter Tuning (using Leave-One-Out CV for OOF generation and internal CV) ===\")\n",
    "\n",
    "    # Generate OOF predictions for ALL N samples for this meta-tuning iteration\n",
    "    oof_preds_full_current_iter = np.zeros((len(X), len(model_names_available)))\n",
    "\n",
    "    # Use LeaveOneOut to generate OOF predictions on the entire dataset\n",
    "    loo_for_full_oof = LeaveOneOut()\n",
    "\n",
    "    is_X_dataframe = isinstance(X, pd.DataFrame)\n",
    "    original_columns = X.columns if is_X_dataframe else [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    if loo_for_full_oof.get_n_splits(X) < 2:\n",
    "        print(f\"Warning: LeaveOneOut cannot generate at least 2 folds with {len(X)} samples. Skipping meta-learner tuning.\")\n",
    "        global_fixed_meta_learner = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "        ])\n",
    "        global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "    else:\n",
    "        for i, (name, estimator_template) in enumerate(temp_base_estimators.items()):\n",
    "            print(f\"  Generating OOF predictions for model {name} (for meta-learner tuning)...\")\n",
    "            fold_preds = np.zeros(len(X))\n",
    "\n",
    "            # Generate OOF predictions using LeaveOneOut\n",
    "            for train_idx, val_idx in loo_for_full_oof.split(X, Y):\n",
    "                X_inner_train = X.iloc[train_idx] if is_X_dataframe else X[train_idx]\n",
    "                Y_inner_train = Y.iloc[train_idx] if isinstance(Y, pd.Series) else Y[train_idx]\n",
    "                X_inner_val = X.iloc[val_idx] if is_X_dataframe else X[val_idx]\n",
    "\n",
    "                estimator_clone = clone(estimator_template)\n",
    "                # Apply noise to X_inner_train if NOISE_SCALE_FACTOR > 0\n",
    "                X_inner_train_augmented_for_oof_meta_tune = X_inner_train\n",
    "                if NOISE_SCALE_FACTOR > 0:\n",
    "                    # Apply Gaussian noise to the training features for data augmentation.\n",
    "                    # This helps in regularizing the model and improving its generalization\n",
    "                    # by making it robust to small perturbations in the input data.\n",
    "                    X_inner_train_np_for_oof_meta_tune = X_inner_train.values if isinstance(X_inner_train, pd.DataFrame) else X_inner_train\n",
    "                    feature_std_devs_for_oof_meta_tune = np.std(X_inner_train_np_for_oof_meta_tune, axis=0)\n",
    "                    noise_std_devs_for_oof_meta_tune = feature_std_devs_for_oof_meta_tune * NOISE_SCALE_FACTOR + 1e-9\n",
    "                    noise_for_oof_meta_tune = np.random.normal(loc=0.0, scale=noise_std_devs_for_oof_meta_tune, size=X_inner_train_np_for_oof_meta_tune.shape)\n",
    "                    if isinstance(X_inner_train, pd.DataFrame):\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = pd.DataFrame(X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune, index=X_inner_train.index, columns=X_inner_train.columns)\n",
    "                    else:\n",
    "                        X_inner_train_augmented_for_oof_meta_tune = X_inner_train_np_for_oof_meta_tune + noise_for_oof_meta_tune\n",
    "\n",
    "                estimator_clone.fit(X_inner_train_augmented_for_oof_meta_tune, Y_inner_train) # Use augmented data for training\n",
    "                fold_preds[val_idx] = estimator_clone.predict(X_inner_val) # Predict on original (non-augmented) inner_val_set\n",
    "            oof_preds_full_current_iter[:, i] = fold_preds\n",
    "\n",
    "        if np.all(oof_preds_full_current_iter == 0) or np.any(np.isnan(oof_preds_full_current_iter)):\n",
    "            print(\"Warning: Generated OOF predictions are all zeros or contain NaN, skipping meta-learner tuning.\")\n",
    "            global_fixed_meta_learner = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "            ])\n",
    "            global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available}\n",
    "        else:\n",
    "            print(\"  Starting meta-learner Bayesian optimization...\")\n",
    "            meta_bayes_search = BayesSearchCV(\n",
    "                estimator=Pipeline([('scaler', StandardScaler()), ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))]),\n",
    "                search_spaces=meta_learner_params,\n",
    "                n_iter=META_LEARNER_N_ITER_BAYESIAN,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                cv=LeaveOneOut(), # Meta-learner internal CV always uses LeaveOneOut.\n",
    "                n_jobs=-1,\n",
    "                random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                verbose=0\n",
    "            )\n",
    "            try:\n",
    "                meta_bayes_search.fit(oof_preds_full_current_iter, Y) # Use full OOF and full Y\n",
    "                print(f\"  Meta-learner Best Score (RMSE): {np.sqrt(-meta_bayes_search.best_score_):.4f}\")\n",
    "                print(f\"  Meta-learner Best Parameters: {dict(meta_bayes_search.best_params_)}\")\n",
    "\n",
    "                # Initialize and train the final meta-learner with optimized parameters, then fix its coefficients.\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(\n",
    "                        alpha=meta_bayes_search.best_params_.get('elasticnet__alpha', 1.0),\n",
    "                        l1_ratio=meta_bayes_search.best_params_.get('elasticnet__l1_ratio', 0.5),\n",
    "                        random_state=DEFAULT_MODEL_RANDOM_STATE,\n",
    "                        max_iter=2000\n",
    "                    ))\n",
    "                ])\n",
    "                # Train once on the entire OOF dataset to fix coefficients\n",
    "                global_fixed_meta_learner.fit(oof_preds_full_current_iter, Y)\n",
    "\n",
    "                # Extract and store fixed coefficients\n",
    "                elastic_net_model = global_fixed_meta_learner.named_steps['elasticnet']\n",
    "                global_fixed_meta_coeffs_dict = dict(zip(model_names_available, elastic_net_model.coef_))\n",
    "\n",
    "                print(\"Global best meta-learner initialized with optimized parameters and fixed coefficients.\")\n",
    "                print(f\"Fixed Meta-Learner Coefficients: {global_fixed_meta_coeffs_dict}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Meta-learner tuning or final training failed: {e}\")\n",
    "                print(\"Using default meta-learner parameters, and coefficients cannot be fixed.\")\n",
    "                global_fixed_meta_learner = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))\n",
    "                ])\n",
    "                global_fixed_meta_coeffs_dict = {name: np.nan for name in model_names_available} # Indicate failure for coeffs\n",
    "\n",
    "print(\"\\n--- Meta-learner preparation complete ---\")\n",
    "\n",
    "\n",
    "# --- Main Prediction Flow ---\n",
    "print(f\"\\n--- Loading unknown data file: {UNKNOWN_DATA_FILE} ---\")\n",
    "try:\n",
    "    # Load the entire Excel file\n",
    "    full_unknown_data = pd.read_excel(UNKNOWN_DATA_FILE)\n",
    "\n",
    "    # Assume the first column contains sample names and save it\n",
    "    # Check column count to ensure a first column exists\n",
    "    if full_unknown_data.shape[1] > 0:\n",
    "        sample_names = full_unknown_data.iloc[:, 0]\n",
    "        # Feature data for model prediction, starting from the second column\n",
    "        unknown_data_features = full_unknown_data.iloc[:, 1:]\n",
    "        print(f\"Identified sample name column: '{full_unknown_data.columns[0]}'\")\n",
    "    else:\n",
    "        print(\"Warning: Unknown data file appears to have no columns. Sample names cannot be extracted.\")\n",
    "        sample_names = pd.Series(range(len(full_unknown_data)), name='Sample_Index')\n",
    "        unknown_data_features = full_unknown_data.copy() # If no columns, use an empty DataFrame directly\n",
    "\n",
    "    # Ensure all feature columns are numeric, coercing non-numeric to NaN\n",
    "    unknown_data_features = unknown_data_features.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill NaN values using the median from the training set (if training set had NaN filling)\n",
    "    # This assumes training set X has already been NaN-filled and its column medians are available.\n",
    "    if isinstance(X, pd.DataFrame) and X.isnull().any().any(): # Check if training set X has NaN\n",
    "        for col in X.columns:\n",
    "            if col in unknown_data_features.columns and unknown_data_features[col].isnull().any().any():\n",
    "                median_val = X[col].median()\n",
    "                if pd.isna(median_val): # If the median for that training feature is also NaN, fill with 0\n",
    "                    warnings.warn(f\"Warning: Median for training feature '{col}' is NaN, NaN values in unknown data will be filled with 0.\")\n",
    "                    unknown_data_features[col] = unknown_data_features[col].fillna(0)\n",
    "                else:\n",
    "                    unknown_data_features[col] = unknown_data_features[col].fillna(median_val)\n",
    "    print(f\"Successfully loaded {unknown_data_features.shape[0]} unknown data samples.\")\n",
    "    print(\"NaN values in unknown data filled using the median of corresponding training features (or 0).\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fatal Error: Unknown data file '{UNKNOWN_DATA_FILE}' not found. Please check the file path.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error: An error occurred while loading or processing unknown data: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Ensure unknown data feature columns are consistent with training data\n",
    "# Prioritizing training data column order and names\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    training_features = X.columns.tolist()\n",
    "    # Check if unknown data contains all training features\n",
    "    missing_features = [f for f in training_features if f not in unknown_data_features.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: Unknown data is missing the following features from the training set: {missing_features}. They will be filled with 0.\")\n",
    "        for mf in missing_features:\n",
    "            unknown_data_features[mf] = 0 # Fill missing features with 0\n",
    "\n",
    "    # Ensure unknown data column order matches training data\n",
    "    unknown_data_aligned = unknown_data_features[training_features]\n",
    "else:\n",
    "    # If X is not a DataFrame, assume unknown data column order is identical.\n",
    "    print(\"Warning: Training features X is not a Pandas DataFrame, cannot align unknown data columns by name. Assuming column order is identical.\")\n",
    "    unknown_data_aligned = unknown_data_features.values # Convert to numpy array\n",
    "\n",
    "# Get available model names (filtered by ENABLED_SUBMODELS_LIST)\n",
    "try:\n",
    "    base_estimators = initialize_best_estimators(grid_searches, ENABLED_SUBMODELS_LIST)\n",
    "    model_names_available_for_prediction = list(base_estimators.keys()) # Using a different variable name to avoid confusion\n",
    "    if not model_names_available_for_prediction:\n",
    "        print(\"Error: No base models successfully initialized, cannot proceed with prediction. Please check ENABLED_SUBMODELS_LIST.\")\n",
    "        sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error: An error occurred while trying to initialize base models: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Generate Base Model Predictions ---\n",
    "print(\"\\n--- Generating Base Model Predictions ---\")\n",
    "base_model_predictions = pd.DataFrame(index=unknown_data_aligned.index)\n",
    "\n",
    "for name, estimator in base_estimators.items():\n",
    "    print(f\"  Using model {name} for prediction...\")\n",
    "    try:\n",
    "        # For default models that haven't been trained, training is required here.\n",
    "        # A simple check if already trained (e.g., has 'n_features_in_' or 'coef_')\n",
    "        if not hasattr(estimator, 'n_features_in_') and not hasattr(estimator, 'coef_'):\n",
    "            print(f\"  Warning: Model {name} appears untrained, training with training data X, Y...\")\n",
    "            try:\n",
    "                estimator.fit(X, Y) # Train with the original training data\n",
    "                print(f\"  Model {name} training complete.\")\n",
    "            except Exception as fit_e:\n",
    "                print(f\"  Error: Model {name} training failed: {fit_e}. This model will be skipped.\")\n",
    "                base_model_predictions[name] = np.nan\n",
    "                continue\n",
    "\n",
    "        preds = estimator.predict(unknown_data_aligned)\n",
    "        base_model_predictions[name] = preds\n",
    "        print(f\"  Model {name} prediction complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: An error occurred during prediction with model {name}: {e}. Predictions for this model will be NaN.\")\n",
    "        base_model_predictions[name] = np.nan\n",
    "\n",
    "if base_model_predictions.isnull().all().all():\n",
    "    print(\"Fatal Error: All base model predictions failed or are empty. Cannot proceed with Stacking prediction.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Generate Stacking Model Predictions ---\n",
    "print(\"\\n--- Generating Stacking Model Predictions ---\")\n",
    "\n",
    "# Ensure global_fixed_meta_learner exists and is valid\n",
    "if global_fixed_meta_learner is None:\n",
    "    print(\"Fatal Error: Meta-learner not successfully loaded or trained. Cannot proceed with Stacking prediction.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    stacking_predictions = global_fixed_meta_learner.predict(base_model_predictions.values)\n",
    "    base_model_predictions['Stacking_Prediction'] = stacking_predictions\n",
    "    print(\"Stacking model prediction complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error: An error occurred during Stacking model prediction: {e}.\")\n",
    "    print(\"Please check if global_fixed_meta_learner has been correctly trained and passed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Output Prediction Results ---\n",
    "print(f\"\\n--- Saving prediction results to file: {OUTPUT_PREDICTIONS_FILE} ---\")\n",
    "try:\n",
    "    # Merge original unknown data (including sample names) with prediction results\n",
    "    # Using full_unknown_data instead of unknown_data_features, as it contains the original first column.\n",
    "    final_output_df = full_unknown_data.copy()\n",
    "    final_output_df['Stacking_Prediction'] = base_model_predictions['Stacking_Prediction']\n",
    "    # Base model predictions can also be added.\n",
    "    for col in base_model_predictions.columns:\n",
    "        if col != 'Stacking_Prediction':\n",
    "            final_output_df[f'{col}_Prediction'] = base_model_predictions[col]\n",
    "\n",
    "    final_output_df.to_excel(OUTPUT_PREDICTIONS_FILE, index=False, engine='openpyxl')\n",
    "    print(f\"Prediction results successfully saved to: {OUTPUT_PREDICTIONS_FILE}\")\n",
    "except ImportError:\n",
    "    print(\"!!! Exporting to Excel requires 'openpyxl' library. Please run 'pip install openpyxl'.\")\n",
    "    print(\"Attempting to export as CSV file...\")\n",
    "    try:\n",
    "        final_output_df.to_csv(OUTPUT_PREDICTIONS_FILE.replace('.xlsx', '.csv'), index=False)\n",
    "        print(f\"Successfully exported prediction results as CSV: {OUTPUT_PREDICTIONS_FILE.replace('.xlsx', '.csv')}\")\n",
    "    except Exception as e_csv:\n",
    "        print(f\"!!! Error exporting to CSV as well: {e_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! Other error occurred while exporting prediction results to file: {e}\")\n",
    "\n",
    "print(\"\\nPrediction script execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a477a-01a9-4883-be43-de3c78adc6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matsci-ai)",
   "language": "python",
   "name": "matsci-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
