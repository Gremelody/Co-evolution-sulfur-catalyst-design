{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec9fecc-9bc7-4a98-b9f0-b3921f6d034f",
   "metadata": {},
   "source": [
    "导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b600fd92-5a86-4706-a77e-cb4f847e62ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'Original features.xlsx'...\n",
      "Extracting features from column 'Number' to 'S-NM (Å)' and target 'Shift range (eV)'.\n",
      "Extracted features X, shape: (7, 25)\n",
      "Extracted target Y 'Shift range (eV)', shape: (7,)\n",
      "Data loading complete. X shape: (7, 25), y shape: (7,)\n",
      "\n",
      "--- Outputting Original Feature Correlation Data ---\n",
      "Saved: Original Feature Correlation Data -> feature_engineering_output\\original_features_correlation_data_20250710_224045.xlsx\n",
      "\n",
      "--- Stage 1: Filtering Highly Correlated Features ---\n",
      "The following features are removed based on Pearson correlation (threshold>0.8):\n",
      "Kept Feature                  Dropped Features\n",
      "----------------------------- ----------------\n",
      "Nucleophilicity index (eV)    'Chemical potential (eV) ',\n",
      "                              'Electronegativity',\n",
      "                              'Electrophilicity index (eV) ',\n",
      "                              'Ionization Energy (eV) ',\n",
      "                              'S-NM (Å)',\n",
      "                              'Vertical EA (eV) ',\n",
      "                              'Vertical IP (eV)'\n",
      "main group number             'MendeleevNumber', 'pUnfilled', 'pValence'\n",
      "Hardness (eV)                 'Overall surface area (Angstrom^2)',\n",
      "                              'Volume (Angstrom^3)'\n",
      "CovalentRadius (pm)           'AtomicWeight', 'Number', 'Ti-NM (Å)',\n",
      "                              'period number'\n",
      "Number of features after filtering: 9\n",
      "\n",
      "--- Outputting Stage 1 Filtered Feature Correlation Data ---\n",
      "Saved: Stage 1 Filtered Feature Correlation Data -> feature_engineering_output\\step1_filtered_features_correlation_data_20250710_224045.xlsx\n",
      "\n",
      "--- Stage 2: SHAP Coarse Feature Selection ---\n",
      "Calculating SHAP values (using Leave-One-Out Cross-Validation, 7 folds)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHAP Coarse Selection Progress:  71%|███████▏  | 5/7 [00:08<00:03,  1.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 620\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo features remaining after Stage 1 filtering, cannot calculate correlation matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Execute Stage 2: SHAP Coarse Feature Selection\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m X_shap_coarse, dropped_shap \u001b[38;5;241m=\u001b[39m \u001b[43mstep2_embed_shap_coarse_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m# Execute Stage 3: Iterative Feature Refinement\u001b[39;00m\n\u001b[0;32m    622\u001b[0m final_features, performance_history \u001b[38;5;241m=\u001b[39m step3_wrapper_shap_iterative_selection(X_shap_coarse, y_full, config)\n",
      "Cell \u001b[1;32mIn[3], line 317\u001b[0m, in \u001b[0;36mstep2_embed_shap_coarse_selection\u001b[1;34m(X_filtered, y, config)\u001b[0m\n\u001b[0;32m    314\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mRF_N_ESTIMATORS, max_features\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mRF_MAX_FEATURES, min_samples_leaf\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mRF_MIN_SAMPLES_LEAF, min_samples_split\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mRF_MIN_SAMPLES_SPLIT, random_state\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mRANDOM_STATE, n_jobs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mRF_N_JOBS)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fold_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# Use TreeExplainer to calculate SHAP values\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mTreeExplainer(rf_model)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\matsci-ai\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class for all adjustable parameters.\n",
    "    \"\"\"\n",
    "    # File paths configuration\n",
    "    INPUT_FILE = 'Original features.xlsx'\n",
    "    OUTPUT_DIR = 'feature_engineering_output'\n",
    "\n",
    "    # Core algorithm parameters\n",
    "    RANDOM_STATE = 0\n",
    "    PEARSON_CORR_THRESHOLD = 0.80\n",
    "\n",
    "    # Random Forest Regressor hyperparameters\n",
    "    RF_N_ESTIMATORS = 2000\n",
    "    RF_MIN_SAMPLES_LEAF = 1\n",
    "    RF_MIN_SAMPLES_SPLIT = 2\n",
    "    RF_MAX_FEATURES = 'sqrt'\n",
    "    RF_MAX_DEPTH = None\n",
    "    RF_N_JOBS = -1\n",
    "\n",
    "    # SHAP value calculation and Cross-validation parameters\n",
    "    PERFORMANCE_METRIC = 'mae' # Options: 'mae' (lower is better) or 'r2' (higher is better)\n",
    "    SHAP_COARSE_SELECTION_PERCENT = 1 # Percentage of features to keep during SHAP coarse selection (between 0 and 1)\n",
    "\n",
    "def load_data_from_excel(config: Config):\n",
    "    \"\"\"\n",
    "    Loads data from an Excel file, extracts features (X) and target (Y) by column position,\n",
    "    and performs data cleaning.\n",
    "\n",
    "    Args:\n",
    "        config (Config): Configuration object containing input file path and other settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X_cleaned (pd.DataFrame): Cleaned feature data.\n",
    "            - y_cleaned (pd.Series): Cleaned target data.\n",
    "            - df_original (pd.DataFrame): A copy of the original DataFrame, with rows containing missing values removed.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from '{config.INPUT_FILE}'...\")\n",
    "    if not os.path.exists(config.INPUT_FILE):\n",
    "        print(f\"Error: File '{config.INPUT_FILE}' does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_excel(config.INPUT_FILE)\n",
    "    df_original = df.copy() # Keep a copy of the original DataFrame for final output\n",
    "\n",
    "    try:\n",
    "        all_col_names = df.columns\n",
    "        # Feature columns: from the second column (index 1) to the third-to-last column (index -2)\n",
    "        feature_col_names = all_col_names[1:-2]\n",
    "        \n",
    "        # Target column: the second-to-last column (index -2)\n",
    "        # Note: Adjust this index based on your specific target column.\n",
    "        # For example:\n",
    "        # If 'band alignment' is the second-to-last column: target_col_name = all_col_names[-2]\n",
    "        # If 'shift range' is the very last column: target_col_name = all_col_names[-1]\n",
    "        target_col_name = all_col_names[-1]\n",
    "        \n",
    "        # Dynamically describe the extracted feature column range\n",
    "        if len(feature_col_names) > 1:\n",
    "            feature_range_str = f\"from column '{feature_col_names[0]}' to '{feature_col_names[-1]}'\"\n",
    "        elif len(feature_col_names) == 1:\n",
    "            feature_range_str = f\"from column '{feature_col_names[0]}'\"\n",
    "        else:\n",
    "            feature_range_str = \"with no features\" # Case where feature_col_names is empty\n",
    "\n",
    "        print(f\"Extracting features {feature_range_str} and target '{target_col_name}'.\")\n",
    "\n",
    "        if target_col_name in feature_col_names:\n",
    "            print(f\"Critical Error: Target column '{target_col_name}' is also identified as a feature column. Please check Excel column order or slicing rules.\")\n",
    "            exit()\n",
    "\n",
    "        X = df[feature_col_names]\n",
    "        Y = df[target_col_name]\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"Error: Column index out of bounds. Ensure the Excel file has at least 3 columns (e.g., SampleID, Feature..., Target, Ignored).\")\n",
    "        exit()\n",
    "    except Exception as e: # Catch other potential errors during column access, e.g., empty feature_col_names\n",
    "        print(f\"Error during column extraction: {e}. Please check your Excel file structure and the slicing logic.\")\n",
    "        exit()\n",
    "    \n",
    "    print(f\"Extracted features X, shape: {X.shape}\")\n",
    "    print(f\"Extracted target Y '{Y.name}', shape: {Y.shape}\")\n",
    "    \n",
    "    # Combine X and Y for missing value handling\n",
    "    combined_df = pd.concat([X, Y], axis=1)\n",
    "    # Attempt to convert all columns to numeric, coercing errors to NaN\n",
    "    for col in combined_df.columns:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "    \n",
    "    original_rows = len(combined_df)\n",
    "    # Drop rows containing any NaN values\n",
    "    combined_df_cleaned = combined_df.dropna()\n",
    "    \n",
    "    if len(combined_df_cleaned) < original_rows:\n",
    "        print(f\"Warning: {original_rows - len(combined_df_cleaned)} rows with missing values have been removed.\")\n",
    "        # Update the original DataFrame copy to match the cleaned data rows\n",
    "        df_original = df_original.loc[combined_df_cleaned.index]\n",
    "\n",
    "    # Separate X and Y again from the cleaned combined data\n",
    "    X_cleaned = combined_df_cleaned.drop(columns=[Y.name])\n",
    "    y_cleaned = combined_df_cleaned[Y.name]\n",
    "\n",
    "    print(f\"Data loading complete. X shape: {X_cleaned.shape}, y shape: {y_cleaned.shape}\")\n",
    "    return X_cleaned, y_cleaned, df_original\n",
    "\n",
    "def step1_filter_high_correlated_features(X, y, config: Config):\n",
    "    \"\"\"\n",
    "    Stage 1: Filter method to remove highly correlated features based on Pearson correlation coefficient.\n",
    "    For any pair of features, if their Pearson correlation exceeds the threshold, the feature with\n",
    "    higher correlation to the target variable is kept, and the other is removed.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Input feature data.\n",
    "        y (pd.Series): Target variable data.\n",
    "        config (Config): Configuration object containing correlation threshold and other settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X_filtered (pd.DataFrame): Filtered feature data.\n",
    "            - to_drop_list (list): List of names of features that were removed.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Stage 1: Filtering Highly Correlated Features ---\")\n",
    "    \n",
    "    if X.shape[1] <= 1:\n",
    "        print(\"Insufficient number of features (<=1) for Pearson correlation filtering.\")\n",
    "        return X.copy(), []\n",
    "    if X.shape[0] <= 1:\n",
    "        print(\"Insufficient number of samples (<=1) for Pearson correlation filtering.\")\n",
    "        return X.copy(), []\n",
    "\n",
    "    # Calculate the absolute Pearson correlation matrix between features\n",
    "    corr_matrix = X.corr(method='pearson').abs()\n",
    "    \n",
    "    # Calculate the absolute Pearson correlation coefficient for each feature with the target variable\n",
    "    feature_y_corr = {}\n",
    "    for col in X.columns:\n",
    "        corr_val = X[col].corr(y)\n",
    "        feature_y_corr[col] = abs(corr_val) if not pd.isna(corr_val) else -1.0 # NaN values are set to -1 to ensure they are sorted last\n",
    "\n",
    "    # Sort features in descending order based on their correlation with the target variable\n",
    "    sorted_features = sorted(X.columns, key=lambda col: feature_y_corr[col], reverse=True)\n",
    "\n",
    "    kept_features_final = [] # List of features finally kept\n",
    "    all_dropped_features_set = set() # Set of all features marked for dropping\n",
    "    retained_to_dropped_map = defaultdict(list) # Maps retained features to features they caused to be dropped\n",
    "\n",
    "    # Iterate through sorted features to perform high correlation feature removal\n",
    "    for current_feature in sorted_features:\n",
    "        # If the current feature has already been marked for dropping, skip it\n",
    "        if current_feature in all_dropped_features_set:\n",
    "            continue\n",
    "\n",
    "        # Add the current feature to the kept list\n",
    "        kept_features_final.append(current_feature)\n",
    "\n",
    "        # Iterate through other features to check correlation with the current feature\n",
    "        for other_feature in sorted_features:\n",
    "            # Skip self-comparison or features already marked for dropping\n",
    "            if other_feature == current_feature or other_feature in all_dropped_features_set:\n",
    "                continue\n",
    "\n",
    "            # Get the correlation coefficient between the feature pair\n",
    "            pair_corr = corr_matrix.loc[current_feature, other_feature]\n",
    "            # If the correlation coefficient exceeds the threshold, mark the 'other feature' for dropping\n",
    "            if pair_corr > config.PEARSON_CORR_THRESHOLD:\n",
    "                all_dropped_features_set.add(other_feature)\n",
    "                retained_to_dropped_map[current_feature].append(other_feature)\n",
    "    \n",
    "    to_drop_list = list(all_dropped_features_set)\n",
    "\n",
    "    if to_drop_list:\n",
    "        print(f\"The following features are removed based on Pearson correlation (threshold>{config.PEARSON_CORR_THRESHOLD}):\")\n",
    "        \n",
    "        # Prepare structured data for printing\n",
    "        output_data = []\n",
    "        for kept_feat in kept_features_final:\n",
    "            if kept_feat in retained_to_dropped_map:\n",
    "                dropped_feats_for_this_kept = retained_to_dropped_map[kept_feat]\n",
    "                output_data.append({\n",
    "                    'Kept Feature': kept_feat,\n",
    "                    'Dropped Features_list': dropped_feats_for_this_kept\n",
    "                })\n",
    "        \n",
    "        if output_data:\n",
    "            # Calculate the maximum width for the first column (kept features), including the header\n",
    "            max_kept_len = max(len(row['Kept Feature']) for row in output_data)\n",
    "            col1_header = 'Kept Feature'\n",
    "            col1_width = max(max_kept_len, len(col1_header)) + 2 # Add 2 extra spaces for column padding\n",
    "\n",
    "            col2_header = 'Dropped Features'\n",
    "            terminal_width = 100 # Terminal display width, adjustable as needed\n",
    "\n",
    "            # Calculate the absolute starting position for the second column's content (characters from line start)\n",
    "            target_col2_start_pos = col1_width + 1 # First column width + 1 space for padding\n",
    "            \n",
    "            # textwrap.fill will add a prefix indent string to all lines\n",
    "            indent_str_for_textwrap = ' ' * target_col2_start_pos\n",
    "\n",
    "            # Print table header\n",
    "            print(f\"{col1_header:<{col1_width}} {col2_header:<}\")\n",
    "            print(f\"{'-' * col1_width} {'-' * len(col2_header)}\")\n",
    "\n",
    "            # Print each row of data\n",
    "            for row in output_data:\n",
    "                kept_feat = row['Kept Feature']\n",
    "                original_dropped_feats_list = sorted(row['Dropped Features_list'])\n",
    "\n",
    "                # Step 1: Replace spaces within feature names with a placeholder and add quotes for textwrap to handle correctly\n",
    "                safe_dropped_feats_list = []\n",
    "                for d in original_dropped_feats_list:\n",
    "                    safe_d = d.replace(' ', '___FEAT_SPACE___') # Use a unique placeholder\n",
    "                    safe_dropped_feats_list.append(f\"'{safe_d}'\")\n",
    "                \n",
    "                # Step 2: Join the processed feature name list with commas and spaces\n",
    "                dropped_feats_str_temp = \", \".join(safe_dropped_feats_list)\n",
    "\n",
    "                # Step 3: Use textwrap.fill to wrap the text, enabling automatic line breaks and indentation\n",
    "                wrapped_lines_with_indent = textwrap.fill(\n",
    "                    dropped_feats_str_temp, \n",
    "                    width=terminal_width, \n",
    "                    initial_indent=indent_str_for_textwrap, \n",
    "                    subsequent_indent=indent_str_for_textwrap, \n",
    "                    break_long_words=False # Try not to break words\n",
    "                ).split('\\n')\n",
    "\n",
    "                # Step 4: Replace placeholders back with spaces to restore original feature names\n",
    "                wrapped_dropped_lines_clean = [line.replace('___FEAT_SPACE___', ' ') for line in wrapped_lines_with_indent]\n",
    "\n",
    "                # Print the first line: Kept Feature + first line of dropped features content\n",
    "                # The dropped features content needs to remove the prefix indent added by textwrap,\n",
    "                # as the f-string already handles the first column's width and spacing.\n",
    "                print(f\"{kept_feat:<{col1_width}} {wrapped_dropped_lines_clean[0][len(indent_str_for_textwrap):]}\")\n",
    "\n",
    "                # Print subsequent lines (if dropped features content spans multiple lines)\n",
    "                for i in range(1, len(wrapped_dropped_lines_clean)):\n",
    "                    # For subsequent lines, the first column is empty but needs to be filled to the same width\n",
    "                    # as the first column to maintain alignment.\n",
    "                    # The dropped features content needs to remove the prefix indent added by textwrap\n",
    "                    # and ensure printing starts from the correct position.\n",
    "                    print(f\"{'':<{col1_width}}{wrapped_dropped_lines_clean[i][col1_width:]}\") \n",
    "            \n",
    "        else:\n",
    "            print(\"  (No feature pairs exceeded the Pearson correlation threshold, no features removed.)\")\n",
    "        \n",
    "        X_filtered = X.drop(columns=to_drop_list)\n",
    "    else:\n",
    "        print(\"No feature pairs exceeded the Pearson correlation threshold, no features removed.\")\n",
    "        X_filtered = X.copy()\n",
    "    \n",
    "    print(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "    return X_filtered, to_drop_list\n",
    "\n",
    "def step2_embed_shap_coarse_selection(X_filtered, y, config: Config):\n",
    "    \"\"\"\n",
    "    Stage 2: Embedded method for coarse feature selection based on SHAP values.\n",
    "    Uses Leave-One-Out Cross-Validation (LOOCV) to calculate the average absolute SHAP value for each feature,\n",
    "    keeping the top N% most important features.\n",
    "\n",
    "    Args:\n",
    "        X_filtered (pd.DataFrame): Feature data after Stage 1 filtering.\n",
    "        y (pd.Series): Target variable data.\n",
    "        config (Config): Configuration object containing Random Forest parameters and SHAP coarse selection percentage.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X_shap_coarse (pd.DataFrame): Feature data after SHAP coarse selection.\n",
    "            - features_to_drop (list): List of names of features that were removed.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Stage 2: SHAP Coarse Feature Selection ---\")\n",
    "    \n",
    "    if X_filtered.shape[1] == 0:\n",
    "        print(\"No features available for SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "    if X_filtered.shape[0] <= 1:\n",
    "        print(\"Insufficient number of samples (<=1) for SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    avg_abs_shap_values_per_fold = [] # Stores average absolute SHAP values for each LOOCV fold\n",
    "    \n",
    "    n_splits = X_filtered.shape[0]\n",
    "    print(f\"Calculating SHAP values (using Leave-One-Out Cross-Validation, {n_splits} folds)...\")\n",
    "    \n",
    "    # Check if the training set sample size meets the minimum requirements for RandomForest\n",
    "    min_samples_for_rf = max(config.RF_MIN_SAMPLES_LEAF, config.RF_MIN_SAMPLES_SPLIT)\n",
    "    if (n_splits - 1) < min_samples_for_rf:\n",
    "        print(f\"Warning: Training set sample size ({n_splits-1}) is less than the minimum required samples ({min_samples_for_rf}) for RandomForestRegressor. SHAP coarse selection will be skipped.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    # Iterate through LOOCV folds, train the model, and calculate SHAP values\n",
    "    for train_idx, val_idx in tqdm(loo.split(X_filtered), total=n_splits, desc=\"SHAP Coarse Selection Progress\"):\n",
    "        X_fold_train, y_fold_train = X_filtered.iloc[train_idx], y.iloc[train_idx]\n",
    "        \n",
    "        # Standardize the training data\n",
    "        scaler = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train_scaled = scaler.transform(X_fold_train)\n",
    "        \n",
    "        # Initialize and train the RandomForestRegressor\n",
    "        rf_model = RandomForestRegressor(n_estimators=config.RF_N_ESTIMATORS, max_features=config.RF_MAX_FEATURES, min_samples_leaf=config.RF_MIN_SAMPLES_LEAF, min_samples_split=config.RF_MIN_SAMPLES_SPLIT, random_state=config.RANDOM_STATE, n_jobs=config.RF_N_JOBS)\n",
    "        \n",
    "        try:\n",
    "            rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            # Use TreeExplainer to calculate SHAP values\n",
    "            explainer = shap.TreeExplainer(rf_model)\n",
    "            shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "            \n",
    "            # Handle SHAP value dimensions (typically 1D for regression, multi-dimensional for classification)\n",
    "            if shap_values_fold.ndim == 1:\n",
    "                avg_abs_shap_values_per_fold.append(np.abs(shap_values_fold))\n",
    "            else:\n",
    "                avg_abs_shap_values_per_fold.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: RandomForestRegressor training or SHAP calculation failed during SHAP coarse selection (possibly due to data issues or improper parameters): {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not avg_abs_shap_values_per_fold:\n",
    "        print(\"Warning: Failed to calculate any SHAP values, possibly due to small sample size or model training failure. Skipping SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    # Calculate the average absolute SHAP values across all folds and sort by importance in descending order\n",
    "    feature_importances_shap = pd.Series(np.mean(avg_abs_shap_values_per_fold, axis=0), index=X_filtered.columns).sort_values(ascending=False)\n",
    "    print(\"\\nFeature Importance (based on average absolute SHAP values from LOOCV):\")\n",
    "    print(feature_importances_shap)\n",
    "\n",
    "    # Determine the number of features to keep based on the configured percentage\n",
    "    num_features_to_keep = int(len(feature_importances_shap) * config.SHAP_COARSE_SELECTION_PERCENT)\n",
    "    # Ensure at least one feature is kept (if available)\n",
    "    if num_features_to_keep == 0 and len(feature_importances_shap) > 0: num_features_to_keep = 1\n",
    "    # Ensure the number of features to keep does not exceed the total number of features\n",
    "    if num_features_to_keep > len(feature_importances_shap): num_features_to_keep = len(feature_importances_shap)\n",
    "        \n",
    "    # Get the list of features to keep and features to drop\n",
    "    features_to_keep = feature_importances_shap.index[:num_features_to_keep].tolist()\n",
    "    features_to_drop = list(set(X_filtered.columns) - set(features_to_keep))\n",
    "\n",
    "    if features_to_drop:\n",
    "        print(f\"\\nBased on SHAP coarse selection (keeping top {config.SHAP_COARSE_SELECTION_PERCENT*100:.1f}%), the following features are removed: {features_to_drop}\")\n",
    "        X_shap_coarse = X_filtered[features_to_keep]\n",
    "    else:\n",
    "        print(\"\\nNo features were removed after SHAP coarse selection.\")\n",
    "        X_shap_coarse = X_filtered.copy()\n",
    "    \n",
    "    print(f\"Number of features after SHAP coarse selection: {X_shap_coarse.shape[1]}\")\n",
    "    return X_shap_coarse, features_to_drop\n",
    "\n",
    "def step3_wrapper_shap_iterative_selection(X_shap_coarse, y, config: Config):\n",
    "    \"\"\"\n",
    "    Stage 3: Wrapper method for iterative feature refinement.\n",
    "    In each iteration, LOOCV is used to evaluate model performance and SHAP feature importance.\n",
    "    The feature with the lowest SHAP value in the current set is removed until performance no longer\n",
    "    improves or only one feature remains.\n",
    "\n",
    "    Args:\n",
    "        X_shap_coarse (pd.DataFrame): Feature data after Stage 2 SHAP coarse selection.\n",
    "        y (pd.Series): Target variable data.\n",
    "        config (Config): Configuration object containing Random Forest parameters, performance metric, etc.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - best_features (list): List of names of the finally selected best features.\n",
    "            - performance_history (list): List of dictionaries, each containing feature count and performance metric for an iteration.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Stage 3: Iterative Feature Refinement (Leave-One-Out Cross-Validation) ---\")\n",
    "    \n",
    "    if X_shap_coarse.shape[1] <= 1:\n",
    "        print(\"Insufficient number of features (<=1) for iterative refinement.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "    if X_shap_coarse.shape[0] <= 1:\n",
    "        print(\"Insufficient number of samples (<=1) for iterative refinement.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    current_features = list(X_shap_coarse.columns) # List of features used in the current iteration\n",
    "    best_features = list(current_features) # Records the best feature set found so far\n",
    "    \n",
    "    # Set initial best score and comparison function based on the performance metric\n",
    "    if config.PERFORMANCE_METRIC == 'r2':\n",
    "        best_score = -np.inf # Higher R2 is better\n",
    "        is_better = lambda current, best: current > best\n",
    "    elif config.PERFORMANCE_METRIC == 'mae':\n",
    "        best_score = np.inf # Lower MAE is better\n",
    "        is_better = lambda current, best: current < best\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported performance metric. Please choose 'r2' or 'mae'.\")\n",
    "\n",
    "    performance_history = [] # Records performance history for each iteration\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # Check if the initial training set sample size meets the minimum requirements for RandomForest\n",
    "    min_samples_for_rf = max(config.RF_MIN_SAMPLES_LEAF, config.RF_MIN_SAMPLES_SPLIT)\n",
    "    if (X_shap_coarse.shape[0] - 1) < min_samples_for_rf:\n",
    "        print(f\"Warning: Initial training set sample size ({X_shap_coarse.shape[0]-1}) is less than the minimum required samples ({min_samples_for_rf}) for RandomForestRegressor. Cannot calculate baseline performance, iterative refinement will be skipped.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    print(\"Calculating baseline performance for the initial feature set...\")\n",
    "    initial_fold_scores = [] # Stores performance scores for the initial feature set across LOOCV folds\n",
    "    initial_shap_values = [] # Stores SHAP values for the initial feature set across LOOCV folds (primarily for debugging or future expansion)\n",
    "\n",
    "    # Iterate through LOOCV folds to evaluate the performance of the initial feature set\n",
    "    for train_idx, val_idx in tqdm(loo.split(X_shap_coarse), total=X_shap_coarse.shape[0], desc=\"Baseline Performance Calculation Progress\"):\n",
    "        X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "        X_fold_val, y_fold_val = X_shap_coarse.iloc[val_idx][current_features], y.iloc[val_idx]\n",
    "        \n",
    "        # Standardize the training and validation data\n",
    "        scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "        X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
    "\n",
    "        # Initialize and train the RandomForestRegressor\n",
    "        rf_model = RandomForestRegressor(n_estimators=config.RF_N_ESTIMATORS, max_features=config.RF_MAX_FEATURES, min_samples_leaf=config.RF_MIN_SAMPLES_LEAF, min_samples_split=config.RF_MIN_SAMPLES_SPLIT, random_state=config.RANDOM_STATE, n_jobs=config.RF_N_JOBS)\n",
    "        \n",
    "        try:\n",
    "            rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            y_pred = rf_model.predict(X_fold_val_scaled)\n",
    "            \n",
    "            # Calculate performance based on the configured metric\n",
    "            if config.PERFORMANCE_METRIC == 'r2':\n",
    "                initial_fold_scores.append(r2_score(y_fold_val, y_pred))\n",
    "            elif config.PERFORMANCE_METRIC == 'mae':\n",
    "                initial_fold_scores.append(mean_absolute_error(y_fold_val, y_pred))\n",
    "\n",
    "            # Calculate SHAP values (though not directly used for baseline performance decision, maintains consistency)\n",
    "            explainer = shap.TreeExplainer(rf_model)\n",
    "            shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "            if shap_values_fold.ndim == 1:\n",
    "                initial_shap_values.append(np.abs(shap_values_fold))\n",
    "            else:\n",
    "                initial_shap_values.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: RandomForestRegressor training or SHAP calculation failed during baseline performance calculation: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not initial_fold_scores:\n",
    "        print(\"Warning: The initial feature set failed to yield any valid scores, iterative refinement cannot proceed.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    # Update best score and performance history\n",
    "    best_score = np.mean(initial_fold_scores)\n",
    "    performance_history.append({'num_features': len(current_features), config.PERFORMANCE_METRIC: best_score})\n",
    "    print(f\"  Initial feature set ({len(current_features)} features), Average {config.PERFORMANCE_METRIC.upper()}: {best_score:.4f}\")\n",
    "    \n",
    "    print(\"Starting iterative feature removal...\")\n",
    "    iteration_count = 0\n",
    "    # Loop until only one feature remains or performance no longer improves\n",
    "    while len(current_features) > 1:\n",
    "        iteration_count += 1\n",
    "        \n",
    "        current_iteration_shap_values_for_drop = [] # Stores SHAP values for each fold in the current iteration\n",
    "        n_splits_current_loop = X_shap_coarse.shape[0]\n",
    "        \n",
    "        # Re-check if the training set sample size meets the requirements\n",
    "        if (n_splits_current_loop - 1) < min_samples_for_rf:\n",
    "            print(f\"  Warning: Iteration {iteration_count} training set sample size ({n_splits_current_loop-1}) is less than the minimum required samples ({min_samples_for_rf}) for RandomForestRegressor. Cannot calculate SHAP values, stopping iteration.\")\n",
    "            break\n",
    "\n",
    "        # Iterate through LOOCV folds to calculate SHAP values for the current feature set\n",
    "        for train_idx, val_idx in loo.split(X_shap_coarse):\n",
    "            X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "            \n",
    "            scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "            X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "            rf_model = RandomForestRegressor(n_estimators=config.RF_N_ESTIMATORS, max_features=config.RF_MAX_FEATURES, min_samples_leaf=config.RF_MIN_SAMPLES_LEAF, min_samples_split=config.RF_MIN_SAMPLES_SPLIT, random_state=config.RANDOM_STATE, n_jobs=config.RF_N_JOBS)\n",
    "            try:\n",
    "                rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                explainer = shap.TreeExplainer(rf_model)\n",
    "                shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "                if shap_values_fold.ndim == 1:\n",
    "                    current_iteration_shap_values_for_drop.append(np.abs(shap_values_fold))\n",
    "                else:\n",
    "                    current_iteration_shap_values_for_drop.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Iteration {iteration_count} SHAP calculation failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not current_iteration_shap_values_for_drop:\n",
    "            print(f\"  Warning: Iteration {iteration_count} failed to calculate SHAP values, cannot remove features. Stopping iteration.\")\n",
    "            break\n",
    "        \n",
    "        # Calculate average SHAP values and identify the least important feature\n",
    "        avg_iteration_shap = np.mean(current_iteration_shap_values_for_drop, axis=0)\n",
    "        feature_importances_current = pd.Series(avg_iteration_shap, index=current_features).sort_values(ascending=True)\n",
    "        \n",
    "        feature_to_drop = feature_importances_current.index[0] # Feature with the lowest SHAP value\n",
    "        current_features.remove(feature_to_drop) # Remove this feature from the current feature set\n",
    "        print(f\"  Removing least important feature: {feature_to_drop}\")\n",
    "\n",
    "        fold_scores_after_drop = [] # Stores performance scores after feature removal across LOOCV folds\n",
    "        n_splits_after_drop = X_shap_coarse.shape[0]\n",
    "\n",
    "        # Re-check if the training set sample size meets the requirements\n",
    "        if (n_splits_after_drop - 1) < min_samples_for_rf:\n",
    "            print(f\"  Warning: Iteration {iteration_count} training set sample size after feature removal ({n_splits_after_drop-1}) is less than the minimum required samples ({min_samples_for_rf}) for RandomForestRegressor. Cannot evaluate performance, stopping iteration.\")\n",
    "            break\n",
    "\n",
    "        # Iterate through LOOCV folds to evaluate model performance after feature removal\n",
    "        for train_idx, val_idx in tqdm(loo.split(X_shap_coarse), total=n_splits_after_drop, desc=f\"Evaluating Performance After Removal\"):\n",
    "            X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "            X_fold_val, y_fold_val = X_shap_coarse.iloc[val_idx][current_features], y.iloc[val_idx]\n",
    "            \n",
    "            scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "            X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "            X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
    "\n",
    "            rf_model = RandomForestRegressor(n_estimators=config.RF_N_ESTIMATORS, max_features=config.RF_MAX_FEATURES, min_samples_leaf=config.RF_MIN_SAMPLES_LEAF, min_samples_split=config.RF_MIN_SAMPLES_SPLIT, random_state=config.RANDOM_STATE, n_jobs=config.RF_N_JOBS)\n",
    "            try:\n",
    "                rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                y_pred = rf_model.predict(X_fold_val_scaled)\n",
    "                if config.PERFORMANCE_METRIC == 'r2':\n",
    "                    fold_scores_after_drop.append(r2_score(y_fold_val, y_pred))\n",
    "                elif config.PERFORMANCE_METRIC == 'mae':\n",
    "                    fold_scores_after_drop.append(mean_absolute_error(y_fold_val, y_pred))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nWarning: Iteration {iteration_count} failed to evaluate performance after removal: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not fold_scores_after_drop:\n",
    "            print(f\"  Warning: Iteration {iteration_count} failed to calculate any valid scores after feature removal, stopping iteration.\")\n",
    "            break\n",
    "\n",
    "        current_avg_score = np.mean(fold_scores_after_drop)\n",
    "        performance_history.append({'num_features': len(current_features), config.PERFORMANCE_METRIC: current_avg_score})\n",
    "        print(f\"  Number of features: {len(current_features)}, Average {config.PERFORMANCE_METRIC.upper()}: {current_avg_score:.4f}\")\n",
    "        \n",
    "        # If current performance is better than historical best, update the best feature set\n",
    "        if is_better(current_avg_score, best_score):\n",
    "            best_score, best_features = current_avg_score, list(current_features)\n",
    "            print(f\"    -> Performance improved, current best feature set size: {len(best_features)}\")\n",
    "        else:\n",
    "            print(\"    -> Performance did not improve, stopping iteration.\")\n",
    "            break\n",
    "        \n",
    "    print(f\"\\nIterative refinement complete. Best feature set ({len(best_features)} features): {best_features}\")\n",
    "    return best_features, performance_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # If the input file does not exist, generate dummy data for demonstration\n",
    "    if not os.path.exists(config.INPUT_FILE):\n",
    "        print(f\"'{config.INPUT_FILE}' not found, creating dummy data...\")\n",
    "        num_samples, feature_cols = 7, 25 \n",
    "        df_list = [pd.DataFrame([f'Sample_{i+1}' for i in range(num_samples)], columns=['SampleID'])]\n",
    "        \n",
    "        # Define some dummy feature names, including names with spaces\n",
    "        feature_names_from_image = [\n",
    "            'Chemical potential', 'main group number', 'Overall surface area', 'period number',\n",
    "            'Vertical IP', 'Electronegativity', 'Electrophilicity index', 'Ionization Energy',\n",
    "            'Nucleophilicity index', 'S-NM (Å)', 'MendeleevNumber', 'Vertical EA', 'pUnfilled',\n",
    "            'pValence', 'CovalentRadius', 'Ti-NM (Å)', 'Volume', 'AtomicWeight', 'Number', 'density'\n",
    "        ]\n",
    "        # Add generic features to reach the total feature count\n",
    "        generic_features = [f'F{i+1}' for i in range(feature_cols - len(feature_names_from_image))]\n",
    "        all_feature_names = generic_features + feature_names_from_image\n",
    "        \n",
    "        # Generate dummy feature data\n",
    "        feature_data = np.random.rand(num_samples, len(all_feature_names))\n",
    "        df_features = pd.DataFrame(feature_data, columns=all_feature_names)\n",
    "        df_list.append(df_features)\n",
    "        \n",
    "        # Add a dummy column that will be ignored\n",
    "        df_list.append(pd.DataFrame(np.random.rand(num_samples), columns=['Ignored_G']))\n",
    "        df_dummy = pd.concat(df_list, axis=1)\n",
    "        \n",
    "        # Generate dummy target data and name it 'Shift range (eV)'\n",
    "        y_values = 5 * df_dummy[all_feature_names[0]] + 3 * df_dummy[all_feature_names[1]]**2 + np.random.randn(num_samples) * 0.5\n",
    "        df_dummy['Shift range (eV)'] = y_values\n",
    "\n",
    "        # Ensure column order matches the expectation of load_data_from_excel function (SampleID, Feature..., Ignored_G, Target_Y)\n",
    "        all_cols_ordered = ['SampleID'] + all_feature_names + ['Ignored_G', 'Shift range (eV)']\n",
    "        df_dummy = df_dummy[all_cols_ordered]\n",
    "\n",
    "        df_dummy.to_excel(config.INPUT_FILE, index=False)\n",
    "        print(f\"Dummy data '{config.INPUT_FILE}' created.\")\n",
    "\n",
    "    # Load data\n",
    "    X_full, y_full, df_original = load_data_from_excel(config)\n",
    "    \n",
    "    print(\"\\n--- Outputting Original Feature Correlation Data ---\")\n",
    "    if not X_full.empty:\n",
    "        # Calculate and save the correlation matrix of original features\n",
    "        original_corr_matrix = X_full.corr(method='pearson')\n",
    "        original_corr_path = os.path.join(config.OUTPUT_DIR, f'original_features_correlation_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        original_corr_matrix.to_excel(original_corr_path, index=True)\n",
    "        print(f\"Saved: Original Feature Correlation Data -> {original_corr_path}\")\n",
    "    else:\n",
    "        print(\"No features loaded, cannot calculate original correlation matrix.\")\n",
    "\n",
    "    # Execute Stage 1: Filter highly correlated features\n",
    "    X_filtered, dropped_pearson_list = step1_filter_high_correlated_features(X_full, y_full, config)\n",
    "\n",
    "    print(\"\\n--- Outputting Stage 1 Filtered Feature Correlation Data ---\")\n",
    "    if not X_filtered.empty and X_filtered.shape[1] > 1: # Ensure there are at least two features to calculate correlation matrix\n",
    "        # Calculate and save the correlation matrix of Stage 1 filtered features\n",
    "        filtered_corr_matrix = X_filtered.corr(method='pearson')\n",
    "        filtered_corr_path = os.path.join(config.OUTPUT_DIR, f'step1_filtered_features_correlation_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        filtered_corr_matrix.to_excel(filtered_corr_path, index=True)\n",
    "        print(f\"Saved: Stage 1 Filtered Feature Correlation Data -> {filtered_corr_path}\")\n",
    "    elif X_filtered.shape[1] <= 1:\n",
    "        print(\"Insufficient features after Stage 1 filtering (<=1), cannot calculate correlation matrix.\")\n",
    "    else: # X_filtered is empty\n",
    "        print(\"No features remaining after Stage 1 filtering, cannot calculate correlation matrix.\")\n",
    "\n",
    "    # Execute Stage 2: SHAP Coarse Feature Selection\n",
    "    X_shap_coarse, dropped_shap = step2_embed_shap_coarse_selection(X_filtered, y_full, config)\n",
    "    # Execute Stage 3: Iterative Feature Refinement\n",
    "    final_features, performance_history = step3_wrapper_shap_iterative_selection(X_shap_coarse, y_full, config)\n",
    "\n",
    "    print(f\"\\n--- Outputting Final Results to '{config.OUTPUT_DIR}' Directory ---\")\n",
    "\n",
    "    # Create and save feature engineering summary\n",
    "    num_dropped_pearson = len(dropped_pearson_list)\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Stage': ['Original', 'Stage 1: Pearson Filtering', 'Stage 2: SHAP Coarse Selection', 'Stage 3: SHAP Iterative Selection'],\n",
    "        'Num_Features': [X_full.shape[1], X_filtered.shape[1], X_shap_coarse.shape[1], len(final_features)],\n",
    "        'Dropped_Features_Count': [0, num_dropped_pearson, len(dropped_shap), X_shap_coarse.shape[1] - len(final_features)]\n",
    "    })\n",
    "    summary_path = os.path.join(config.OUTPUT_DIR, f'feature_engineering_summary_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "    summary_df.to_excel(summary_path, index=False)\n",
    "    print(f\"Saved: Feature Engineering Summary -> {summary_path}\")\n",
    "\n",
    "    # Save performance history\n",
    "    history_path = os.path.join(config.OUTPUT_DIR, f'performance_history_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "    pd.DataFrame(performance_history).to_excel(history_path, index=False)\n",
    "    print(f\"Saved: Performance History -> {history_path}\")\n",
    "\n",
    "    # If the number of final features is greater than 1, calculate and save the correlation matrix of final features\n",
    "    if len(final_features) > 1:\n",
    "        final_features_df = df_original[final_features]\n",
    "        final_corr_matrix = final_features_df.corr(method='pearson')\n",
    "        final_corr_path = os.path.join(config.OUTPUT_DIR, f'final_features_correlation_data_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        final_corr_matrix.to_excel(final_corr_path, index=True)\n",
    "        print(f\"Saved: Final Features Correlation Data -> {final_corr_path}\")\n",
    "    elif len(final_features) == 1:\n",
    "        print(f\"Only one feature remains: {final_features[0]}, cannot calculate correlation matrix.\")\n",
    "    else:\n",
    "        print(\"No features remaining, cannot calculate correlation matrix.\")\n",
    "    \n",
    "    # If final features exist, save the final dataset containing selected features and the target variable\n",
    "    if final_features:\n",
    "        cols_to_export = []\n",
    "        # If the original data contains a 'SampleID' column, export it as well\n",
    "        if 'SampleID' in df_original.columns:\n",
    "            cols_to_export.append('SampleID')\n",
    "        # Add the finally selected feature columns\n",
    "        cols_to_export.extend(final_features)\n",
    "        # Add the target variable column\n",
    "        if y_full.name in df_original.columns:\n",
    "            cols_to_export.append(y_full.name)\n",
    "        \n",
    "        dataset_path = os.path.join(config.OUTPUT_DIR, f'final_engineered_dataset_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        df_original[cols_to_export].to_excel(dataset_path, index=False)\n",
    "        print(f\"Saved: Final Engineered Dataset -> {dataset_path}\")\n",
    "    else:\n",
    "        print(\"No final features were selected, final dataset not saved.\")\n",
    "\n",
    "    print(\"\\nAll processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ab25e-3c58-42ea-94a0-2ac03b1fea1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d633f1-5723-424a-8968-369a1ce55fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matsci-ai)",
   "language": "python",
   "name": "matsci-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
